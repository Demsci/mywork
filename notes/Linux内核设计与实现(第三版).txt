

*****************************************************************************************

									第3章 进程管理
								
*****************************************************************************************




		本章引入进程的概念，进程是Unix操作系统抽象概念中最基本的一种。其中涉及进程的定
	义以及相关的概念，比如线程；然后讨论Linux内核如何管理每个进程：它们在内核中如何被列
	举，如何创建，最终又如何消亡。我们拥有操作系统就是为了运行用户程序，因此，进程管理就
	是所有操作系统的心脏所在，Linux也不例外。
	
3.1 进程

		进程就是处于执行期的程序（目标码存放在某种存储介质上）。但进程并不仅仅局限于一段
	可执行程序代码（Unix称其为代码段，text section）。通常进程还要包含其他资源，像打开的文
	件，挂起的信号，内核内部数据，处理器状态，一个或多个具有内存映射的内存地址空间及一个
	或多个执行线程（thread of execution），当然还包括用来存放全局变量的数据段等。实际上，进
	程就是正在执行的程序代码的实时结果。内核需要有效而又透明地管理所有细节。
	
		执行线程，简称线程（thread），是在进程中活动的对象。每个线程都拥有一个独立的程序
	计数器、进程栈和一组进程寄存器。内核调度的对象是线程，而不是进程。在传统地Unix系统
	中，一个进程只包含一个线程，但现在的系统中，包含多个线程的多线程程序司空见惯。稍后你
	会看到，Linux系统的线程实现非常特别：它对线程和进程并不特别区分。对Linux而言，线程
	只不过是一种特殊的进程罢了。
	
		在现代操作系统中，进程提供两种虚拟机制：虚拟处理器和虚拟内存。虽然实际上可能是
	许多进程正在分享一个处理器，但虚拟处理器给进程一种假象，让这些进程觉得自己在独享处理
	器。第4章将详细描述这种机制。而虚拟内存让进程在分配和管理内存时觉得自己拥有整个
	系统的所有内存资源。第12章将描述虚拟内存机制。有趣的是，注意在线程之间可以共享虚拟
	内存，但每个都拥有各自的虚拟处理器。
	
		程序本身并不是进程，进程是处于执行期的程序以及相关的资源的总称。实际上，完全可能
	存在两个或多个不同的进程执行的是同一个程序。并且两个或两个以上并存的进程还可以共享许
	多诸如打开的文件、地址空间之类的资源。
	
		无疑，进程在创建它的时刻开始存活。在Linux系统中，这通常是调用fork()系统的结果，
	该系统调用通过复制一个现有进程来创建一个全新的进程。调用fork()的进程称为父进程，新产
	生的进程称为子进程。在该调用结束时，在返回点这个相同位置上，父进程恢复执行，子进程开
	始执行。fork()系统调用从内核返回两次：一次回到父进程，另一次回到新产生的子进程。
	
		通常，创建新的进程都是为了立即执行新的、不同的程序，而接着调用exec()这组函数
	就可以创建新的地址空间，并把新的程序载入其中。在现代Linux内核中，fork()实际上是由
	clone()系统调用实现的，后者将在后面讨论。
	
		最终，程序通过exit()系统调用退出执行。这个函数会终结进程并将其占用的资源释放掉。
	父进程可以通过wait4()系统调用查询子进程是否终结，边其实使得进程拥有了等待特定进程
	执行完毕的能力。进程退出执行后被设置为僵死状态，直到它的父进程调用wait()或waitpid()
	为止。
	
		注意，进程的另一个名字是任务（task）。Linux内核通常把进程也叫做任务。本书会交替
	使用这两个术语，不过我所说的任务通常指的是从内核观点所看到的进程。
	
	
3.2 进程描述符及任务结构

		内核把进程的列表存放在叫做任务队列（task list）的双向循环链表中。链表中的每一
	项都是类型为task_struct、称为进程描述符（process descriptor）的结构，该结构定义在<linux/
	sched.h>文件中。进程描述符中包含一个具体进程的所有信息。
	
		task_struct相对较大，在32位机器上，它大约有1.7KB。但如果考虑到该结构内包含了内
	核管理一个进程所需的所有信息，那么它的大小也算相当小了。进程描述符中包含的数据能完整
	地描述一个正在执行的程序：它打开的文件，进程的地址空间，挂起的信号，进程的状态，还有
	其他更多信息（见图3-1）。
	

3.2.1 分配进程描述符

		Linux通过slab分配器分配task_struct结构，这样能达到对象复用和缓存着色（cache
	coloring）（参见第12章）的目的。在2.6以前的内核中，各个进程的task_struct存放在它们
	内核栈的尾端。这样做是为了让那些像x86那样寄存器较少的硬件体系结构只要通过栈指针就
	能计算出它的位置，而避免使用额外的寄存器专门记录。由于现在用slab分配器动态生成task_
	struct，所以只需在栈底（对于向下增长的栈来说）或栈顶（对于向上增长的栈来说）创建一个
	新的结构struct thread_info（见图3-2）。
	
		在x86上，struct thread_info在文件<asm/thread_info.h>中定义如下：
		
		struct thread_info {
				struct task_struct			*task;
				struct exec_domain			*exec_domain;
				__u32						flags;
				__u32						status;
				__u32						cpu;
				int							preempt_count;
				mm_segment_t				addr_limit;
				struct restart_block		restart_block;
				void						*sysenter_return;
				int							uaccess_err;
		};
	
		每个任务的thread_info结构在它的内核栈的尾端分配。结构中task域中存放的是指向该任
	务实际task_struct的指针。
	
	
3.2.2 进程描述符的存放

		内核通过一个唯一的进程标识值（process identification value）或PID来标识每个进程。PID是
	一个数，表示为pid_t隐含类型，实际上就是一个int类型。为了与老版本的Unix和Linux兼容，
	PID的最大值默认设置为32768（short int短整型的最大值），尽管这个值也可以增加到高达400万
	（这受<linux/threads.h>中所定义PID最大值的限制）。内核把每个进程的PID存放在它们各自的进
	程描述符中。
	
		这个最大值很重要，因为它实际上就是系统中允许同时存在的进程的最大数目。尽管32768
	对于一般的桌面系统足够用了，但是大型服务器可能需要更多进程。这个值越小，转一圈就越快，
	本来数值大的进程比数值小的进程迟运行，但这样一来就破坏了这一原则。如果确实需要的话，
	可以不考虑与老式系统兼容，由系统管理员通过修改/proc/sys/kernel/pid_max来提高上限。
	
		在内核中，访问任务通常需要获得指向其task_struct的指针。实际上，内核中大部分处理进
	程的代码都是直接通过task_struct进行的。因此，通过current宏查找到当前正在运行进程的进
	程描述符的速度就显得尤为重要。硬件体系结构不同，该宏的实现也不同，它必须针对专门的硬
	件体系结构做处理。有的硬件体系结构可以拿出一个专门寄存器来存放指向当前进程task_struct
	的指针，用于加快访问速度。而有些像x86这样的体系结构（其寄存器并不富余），就只能在内
	核栈的尾端创建thread_info结构，通过计算偏移间接地查找task_struct结构。
	
		在x86系统上，current把栈指针的后13个有效位屏蔽掉，用来计算了thread_info的偏移。
	该操作是通过current_thread_info()函数来完成的。汇编代码如下：
		
		mov  $-8192,  %eax
		andl %esp,  %eax
		
		这里假定栈的大小为8KB。当4KB的栈启用时，就要用4096，而不是8192。
		
		最后，current再从thread_info的task域中提取并返回task_struct的地址：
		
		current_thread_info()-task;
		
		对比一下这部分在PowerPC上的实现（IBM基于RISC的现代微处理器），我们可以发现
	PPC当前的task_struct是保存在一个寄存器中的。也就是说，在PPC上，current宏只需把r2寄
	存器中的值返回就行了。与x86不一样，PPC有足够多的寄存器，所以它的实现有这样选择的余
	地。而访问进程描述符是一个重要的频繁操作，所以PPC的内核开发者觉得完全有必要为此使
	用一个专门的寄存器。
	
	
3.2.3 进程状态

		进程描述符中的state域描述了进程的当前状态（见图3-3）。系统中的每个进程都必然处于
	五种进程状态中的一种。该域的值也必为下列五种状态标志之一：
	
		* TASK_RUNNING（运行）————进程是可执行的；它或者正在执行，或者在运行队列中等
		  待执行（运行队列将会在第4章中讨论）。这是进程在用户空间中执行的唯一可能的状态；
		  这种状态也可以应用到内核空间中正在执行的进程。
		  
		* TASK_INTERRUPTIBLE（可中断）————进程正在睡眠（也就是说它被阻塞），等待某些条
		  件的达成。一旦这些条件达成，内核就会把进程状态设置为运行。处于此状态的进程也会
		  因为接收到信号而提前被唤醒并随时准备投入运行。
		  
		* TASK_UNINTERRUPTIBLE（不可中断）————除了就算是接收到信号也不会被唤醒或准备
		  投入运行外，这个状态与可打断状态相同。这个状态通常在进程必须在等待时不受干扰或
		  等待事件很快就会发生时出现。由于处于此状态的任务对信号不做响应，所以较之可中断
		  状态，使用得较少。
		  
		* __TASK_TRACED————被其他进程跟踪的进程，例如通过ptrace对调试程序进行跟踪。
		
		* __TASK_STOPPED（停止）————进程停止执行；进程没有投入运行也不能投入运行。通常
		  这种状态发生在接收到SIGSTOP、SIGTSTP、SIGTTIN、SIGTTOU等信号的时候。此外，
		  在调试期间接收到任何信号，都会使进程进入这种状态。
		
	
3.2.4 设置当前进程状态

		内核经常需要调整某个进程的状态。这时最好使用set_task_state(task, state)函数：
		set_task_state(task, state);		/* 将任务task的状态设置为state */
	
		该函数将指定的进程设置为指定的状态。必要的时候，它会设置内存屏障来强制其他处理器
	作重新排序。（一般只有在SMP系统中有此必要。）否则，它等价于：
		task_state = state;
		
		set_current_state(state)和set_task_state(current, state)含义是等同的。参看<linux/sched.h>中
	对这些相关函数实现的说明。
	
	
3.2.5 进程上下文

		可执行程序代码是进程的重要组成部分。这些代码从一个可执行文件载入到进程的地址空间
	执行。一般程序在用户空间执行。当一个程序调执行了系统调用（参见第5章）或者触发了某个
	异常，它就陷入了内核空间。此时，我们称内核“代表进程执行”并处于进程上下文中。在此上
	下文中current宏是有效的。除非在此间隙有更高优先级的进程需要执行并由调度器做出了相
	应调整，否则在内核退出的时候，程序恢复在用户空间会继续执行。
	
		系统调用和异常处理程序是对内核明确定义的接口。进程只有通过这些接口才能陷入内核
	执行————对内核的所有访问都必须通过这些接口。
	
	
3.2.6 进程家族树

		Unix系统的进程之间存在一个明显的继承关系，在Linux系统中也是如此。所有的进程都
	是PID为1的init进程的后代。内核在系统启动的最后阶段启动init进程。该进程读取系统的初
	始化脚本(initscript)并执行其他相关程序，最终完成系统启动的整个过程。
	
		系统中的每个进程必有一个父进程，相应的，每个进程也可以拥有零个或多个子进程。拥
	有同一个父进程的所有进程被称为兄弟。进程间的关系存放在进程描述符中。每个task_struct都
	包含一个指向其父进程task_struct、叫做parent的指针，还包含一个称为children的子进程链表。
	所以，对于当前进程，可以通过下面的代码获得其父进程的进程描述符：
	
		struct task_struct			*my_parent = current->parent;
		
		同样，也可以按以下方式依次访问子进程：
		
		struct task_struct *task;
		struct list_head *list;
		
		list_for_each(list, &current->children) {
			task = list_entry(list, struct task_struct, sibling);
			/* task现在指向当前的某个子进程 */
		}
		
		init进程的进程描述符是作为init_task静态分配的。下面的代码可以很好地演示所有进程之
	间的关系：
		
		struct task_struct *task;
		
		for (task = current; task != &init_task; task = task->parent)
		/* task 现在指向init */
	
		实际上，你可以通过这种继承体系从系统的任何一个进程出发查找到任意指定的其他进程。
	但大多数时候，只需要通过简单的重复方式就可以遍历系统中的所有进程。这非常容易做到，因
	为任务队列本来就是一个双向的循环链表。对于给定的进程，获取链表中的下一个进程：
	
		list_entry(task->tasks.next, struct task_struct, tasks)
		
		获取前一个进程的方法与之相同：
		
		list_entry(task->tasks.prev, struct task_struct, tasks)
		
		这两个例程分别通过next_task(task)宏和prev_task(task)宏实现。而实际上，for_each_
	process(task)宏提供了依次访问整个任务队列的能力。每次访问，任务指针都指向链表中的下一
	个元素：
		
		struct task_struct *task;
		
		for_each_process(task) {
			/* 它打印出每一个任务的名称和PID */
			printk("%s[%d]\n", task->comm, task->pid);
		}
		
		
3.3 进程创建

		Unix的进程创建很特别。许多其他的操作系统都提供了产生（spawn）进程的机制，首先在
	新的地址空间里创建进程，读入可执行文件，最后开始执行。Unix采用了与众不同的实现方式，
	它把上述步骤分解到两个单独的函数中去执行：fork()和exec()。首先，fork()通过拷贝当前进
	程创建一个子进程。子进程与父进程的区别仅仅在于PID（每个进程唯一）、PPID（父进程的进
	程号，子进程将其设置为被拷贝进程的PID）和某些资源和统计量（例如，挂起的信号，它没有
	必要被继承）。exec()函数负责读取可执行文件并将其载入地址空间开始运行。把这两个函数组
	合起来使用的效果跟其他系统使用的单一函数的效果相似。
	
	
3.3.1 写时拷贝

		传统的fork()系统调用直接把所有的资源复制给新创建的进程。这种实现过于简单并且效率
	低下，因为它拷贝的数据也许并不共享，更糟的情况是，如果新进程打算立即执行一个新的映
	像，那么所有的拷贝都将前功尽弃。Linux的fork()使用写时拷贝（copy-on-write）页实现。写
	时拷贝是一种可以推迟甚至免除拷贝数据的技术。内核此时并不复制整个进程地址空间，而是让
	父进程和子进程共享同一个拷贝。
	
		只有在需要写入的时候，数据才会被复制，从而使各个进程拥有各自的拷贝。也就是说，资
	源的复制只有在需要写入的时候才进行，在此之前，只是以只读方式共享。这种技术使地址空间
	上的页的拷贝被推迟到实际发生写入的时候才进行。在页根本不会被写入的情况下（举例来说，
	fork()后立即调用exec()）它们就无须复制了。
	
		fork()的实际开销就是复制父进程的页表以及给子进程创建唯一的进程描述符。在一般情况
	下，进程创建后都会马上运行一个可执行的文件，这种优化可以避免拷贝大量根本就不会被使用
	的数据（地址空间里常常包含数十兆的数据）。由于Unix强调进程快速执行的能力，所以这个优
	化是很重要的。
	
	
3.3.2 fork()

		Linux通过clone()系统调用实现fork()。这个调用通过一系列的参数标志来指明父、子进程
	需要共享的资源（关于这些标志更多的信息请参考本章后面3.4节）。fork()、vfork()和__clone()
	库函数都根据各自需要的参数标志去调用clone()，然后由clone()去调用do_fork()。
	
		do_fork完成了创建中的大部分工作，它的定义在kernel/fork.c文件中。该函数调用copy_
	process()函数，然后让进程开始运行。copy_process()函数完成的工作很有意思：
	
		1) 调用dup_task_struct()为新进程创建一个内核栈、thread_info结构和task_struct，这些值
		   与当前进程的值相同。此时，子进程和父进程的描述符是完全相同的。
		   
		2) 检查并确保新创建这个子进程后，当前用户所拥有的进程数目没有超出给它分配的资源
		   的限制。
	
		3) 子进程着手使自己与父进程区别开来。进程描述符内的许多成员都要被清0或设为初始
		   值。那些不是继承而来的进程描述符成员，主要是统计信息。task_struct中的大多数数据都依然
		   未被修改。
	
		4) 子进程的状态被设置为TASK_UNINTERRUPTIBLE，以保证它不会投入运行。
		
		5) copy_process()调用copy_flags()以更新task_struct的flags成员，表明进程是否拥有超级
		   用户权限的PF_SUPERPRIV标志被清0。表明进程还没有调用exec()函数的PF_FORKNOEXEC
		   标志被设置。
		   
		6) 调用alloc_pid()为新进程分配一个有效的PID。
		
		7) 根据传递给clone()的参数标志，copy_process()拷贝或共享打开的文件、文件系统信息、
		   信号处理函数、进程地址空间和命名空间等。在一般情况下，这些资源会被给定进程的所有线程
		   共享；否则，这些资源对每个进程是不同的，因此被拷贝到这里。
		   
		8) 最后，copy_process()做扫尾工作并返回一个指向子进程的指针。
		
		再回到do_fork()函数，如果copy_process()函数成功返回，新创建的子进程被唤醒并让其
	投入运行。内核有意选择子进程首先执行。因为一般子进程都会马上调用exec()函数，这样可
	以避免写时拷贝的额外开销，如果父进程首先执行的话，有可能会开始向地址空间写入。 
	
	
3.3 vfork()

		除了不拷贝父进程的页表项外，vfork()系统调用和fork()的功能相同。子进程作为父进程
	的一个单独的线程在它的地址空间里运行，父进程被阻塞，直到子进程退出或执行exec()。子进
	程不能向地址空间写入。在过去的3BSD时期，这个优化是很有意义的，那时并未使用写时拷贝
	页来实现fork()。现在由于在执行fork()时引入了写时拷贝页并且明确了子进程先执行，vfork()
	的好处就仅限于不拷贝父进程的页表项了。如果Linux将来fork()有了写时拷贝页表项，那么
	vfork()就彻底没用了。另外由于vfork()语意非常微妙（试想，如果exec()调用失败会发生什
	么），所以理想情况下，系统最好不要调用vfork()，内核也不用实现它。完全可以把vfork()实现
	成一个普普通通的fork()————实际上，Linux2.2以前都是这么做的。
	
		vfork()系统调用的实现是通过向clone()系统调用传递一个特殊标志来进行的。
		
		。。。。。。
		
		
3.4 线程在Linux中的实现

		线程机制是现代编程技术中常用的一种抽象概念。该机制提供了在同一程序内共享内存地址
	空间运行的一组线程。这些线程还可以共享打开的文件和其他资源。线程机制支持并发程序设计
	技术（concurrent programming），在多处理器系统上，它也能保证真正的并行处理（parallelism）。
	
		Linux实现线程的机制非常独特。从内核的角度来说，它并没有线程这个概念。Linux把所
	有的线程都当做进程来实现。内核并没有准备特别的调度算法或是定义特别的数据结构来表征线
	程。相反，线程仅仅被视为一个与其他进程共享某些资源的进程。每个线程都拥有唯一隶属于自
	己的task_struct，所以在内核中，它看起来就像是一个普通的进程（只是线程和其他一些进程共
	享某些资源，如地址空间）。
	
		上述线程机制的实现与Microsoft Windows或是Sun Solaris等操作系统的实现差异非常
	大。这些系统都在内核中提供了专门支持线程的机制（这些系统常常把线程称作轻量级进程
	(lightweight processes)）。“轻量级进程”这种叫法本身就概括了Linux在此处与其他系统的差
	异。在其他的系统中，相较于重量级的进程，线程被抽象成一种耗费较少资源，运行迅速的执行单
	元。而对于Linux来说，它只是一种进程间共享资源的手段（Linux的进程本身就够轻量级了）。
	举个例子来说，假如我们有一个包含四个线程的进程，在提供专门线程支持的系统中，通常会有
	一个包含指向四个不同线程的指针的进程描述符。该描述符负责描述像地址空间、打开的文件这
	样的共享资源。线程本身再去描述它独占的资源。相反，Linux仅仅创建四个进程并分配四个普
	通的task_struct结构。建立这四个进程时指定他们共享某些资源，这是相当高雅的做法。
	
	
3.4.1 创建线程

		线程的创建和普通进程的创建类似，只不过在调用clone()的时候需要传递一些参数标志来
	指明需要共享的资源：
	
		clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0);
		
		上面的代码产生的结果和调用fork()差不多，只是父子俩共享地址空间、文件系统资源、文
	件描述符和信号处理程序。换个说法就是，新建的进程和它的父进程就是流行的所谓线程。
	
		对比一下，一个普通的fork()实现是：
		
		clone(SIGCHLD, 0);
		
		而vfork()的实现是：
		
		clone(CLONE_VFORK | CLONE_VM | SIGCHLD, 0);
		
		传递给clone()的参数标志决定了新创建进程的行为方式和父子进程之间共享的资源种类。
	表3-1列举了这些clone()用到的参数标志以及它们的作用，这些是在<linux/sched.h>中定义的。
	
									表3-1 clone()参数标志
			参数标志									含义
		CLONE_FILES							父子进程共享打开的文件
		CLONE_FS							父子进程共享文件系统信息
		CLONE_IDLETASK						将PID设置为0（只供idle进程使用）
		CLONE_NEWNS							为子进程创建新的命名空间
		CLONE_PARENT						指定子进程与父进程拥有同一个父进程
		CLONE_PTRACE						继续调试子进程
		CLONE_SETTID						将TID回写至用户空间
		CLONE_SETTLS						为子进程创建新的TLS
		CLONE_SIGHAND						父子进程共享信号处理函数及被阻断的信号
		CLONE_SYSVSEM						父子进程共享System V SEM_UNDO语义
		CLONE_THREAD						父子进程放入相同的线程组
		CLONE_VFORK							调用vfork()，所以父进程准备睡眠等待子进程将其唤醒
		CLONE_UNTRACED						防止跟踪进程在子进程上强制执行CLONE_PTRACE
		CLONE_STOP							以TASK_STOPPED状态开始进程
		CLONE_SETTLS						为子进程创建新的TLS（thread-local storage）
		CLONE_CHILD_CLEARTID				清除子进程的TID
		CLONE_CHILD_SETTID					设置子进程的TID
		CLONE_PARENT_SETTID					设置父进程的TID
		CLONE_VM							父子进程共享地址空间
		
		
3.4.2 内核线程

		内核经常需要在后台执行一些操作。这种任务可以通过内核线程（kernel thread）完成————
	独立运行在内核空间的标准进程。内核线程和普通的进程间的区别在于内核线程没有独立的地址
	空间（实际上指向地址空间的mm指针被设置为NULL）。它们只在内核空间运行，从来不切换
	到用户空间去。内核进程和普通进程一样，可以被调度，也可以被抢占。
			
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	














































*****************************************************************************************

									第12章 内存管理
								
*****************************************************************************************



		在内核里分配内存不像在其他地方分配内存那么容易。造成这种局面的因素很多。从根本
	上讲，是因为内核本身不能像用户空间那样奢侈地使用内存。内核与用户空间不同，它不具备这
	种能力，它不支持简单便捷的内存分配方式。比如，内核一般不能睡眠。此外，处理内存分配错
	误对内核来说也绝非易事。正是由于这种限制，再加止内存分配机制不能太复杂，所以在内核中
	获取内存要比用户空间复杂得多。不过，从程序开发者角度来看，也不是说内核的内存分配就
	困难得不得了，只是和用户空间中的内存分配不太一样而已。
	
		本章讨论的是在内核之中获取内存的方法。在深入研究实际的分配接口之前，我们需要理解
	内核是如何管理内存的。
	
	
12.1 页

		内核把物理页作为内存管理的基本单位。尽管处理器的最小可寻址单位通常为字（甚至字
	节），但是，内存管理单元（MMU，管理内存并把虚拟地址转换为物理地址的硬件）通常以页为
	单位进行处理。正因为如此，MMU以页（page）大小为单位来管理系统中的页表（这也是页表
	名的来由）。从虚拟内存的角度来看，页就是最小单位。
	
		在第19章中我们将会看到，体系结构不同，支持的页大小也不尽相同，还有些体系结构甚
	至支持几种不同的页大小。大多数32位体系结构支持4KB的页，而64位体系结构一般会支持
	8KB的页。这就意味着，在支持4KB页大小并有1GB物理内存的机器上，物理内存会被划分为
	262144个页。
	
		内核用struct page结构表示系统中的每个物理页，该结构位于<linux/mm_types.h>中————我
	简化了定义，去除了两个容易混淆我们讨论主题的联合结构体：
		
		struct page {
			unsigned long			flags;
			atomic_t				_count;
			atomic_t				_mapcount;
			unsigned long			private;
			struct address_space	*mapping;
			pgoff_t					index;
			struct list_head		lru;
			void					*virtual;
		};

		让我们看一下其中比较重要的域。flag域用来存放页的状态。这些状态包括页是不是脏的，
	是不是被锁定在内存中等。flag的每一位单独表示一种状态，所以它至少可以同时表示出32种
	不同的状态。这些标志定义在<linux/page-flags.h>中。
	
		_count域存放页的引用计数————也就是这一页被引用了多少次。当计数值变为-1时，就
	说明当前内核并没有引用这一页，于是，在新的分配中就可以使用它。内核代码不应当直接检
	查该域，而是调用page_count()函数进行检查，该函数唯一的参数就是page结构。当页空闲时，
	尽管该结构内部的_count值是负的，但是对page_count()函数而言，返回0表示页空闲，返回
	一个正整数表示页在使用。一个页可以由页缓存使用（这时，mapping域指向和这个页关联的
	address_space对象），或者作为私有数据（由private指向），或者作为进程页表中的映射。
	
		virtual域是页的虚拟地址。通常情况下，它就是页在虚拟内存中的地址。有些内存（即所谓
	的高端内存）并不永久地映射到内核地址空间上。在这种情况下，这个域的值为NULL，需要的
	时候，必须动态地映射这些页。稍后我们将讨论高端内存。
	
		必须要理解的一点是page结构与物理页相关，而并非与虚拟页相关。因此，该结构对页的
	描述只是短暂的。即使页中所包含的数据继续存在，由于交换等原因，它们也可能并不再和同
	一个page结构相关联。内核仅仅用这个数据结构来描述当前时刻在相关的物理页中存放的东西。
	这种数据结构的目的在于描述物理内存本身，而不是描述包含在其中的数据。
	
		内核用这一结构来管理系统中所有的页，因为内核需要知道一个页是否空闲（也就是页有
	没有被分配）。如果页已经被分配，内核还需要知道谁拥有这个页。拥有者可能是用户空间进程、
	动态分配的内核数据、静态内核代码或页高速缓存等。
	
		系统中的每个物理页都要分配一个这样的结构体，开发者常常对此感到惊讶。他们会想
	“这得浪费多少内存呀”！让我们来算算对所有这些页都这么做，到底要消耗多少内存。就算
	struct page占40字节的内存吧，假定系统的物理页为8KB大小，系统有4GB物理内存。那么，
	系统中共有页面524288个，而描述这么多页面的page结构体消耗的内存只不过是20MB：也许
	绝对值不小，但是相对系统4GB内存而言，仅是很小的一部分罢了。因此，要管理系统中这么
	多物理页面，这个代价并不算太高。
	
	
12.2 区

		由于硬件的限制，内核不能对所有的页一视同仁。有些页位于内存中特定的物理地址上，
	所以不能将其用于特定的任务。由于存在这种限制，所以内核把页划分为不同的区(zone)。
	内核使用区对具有相似特性的页进行分组。Linux必须处理如下两种由于硬件存在缺陷而引起的
	内存寻址问题：
		* 一些硬件只能用某些特定的内存地址来执行DMA（直接内存访问）。
		* 一些体系结构的内存的物理寻址范围比虚拟寻址范围大得多。这样，就有一些内存不能永
		  久地映射到内核空间上。
		  
		因为存在这些制约条件，Linux主要使用了四种区：
		* ZONE_DMA————这个区包含的页能用来执行DMA操作。
		* ZONE_DMA32————和ZONE_DMA类似，该区包含的页面可用来执行DMA操作；而和
		  ZONE_DMA不同之处在于，这些页面只能被32位设备访问。在某些体系结构中，该区将
		  比ZONE_DMA更大。
		* ZONE_NORMAL————这个区包含的都是能正常映射的页。
		* ZONE_HIGHMEM————这个区包含“高端内存”，其中的页并不能永久地映射到内核地址空间。
		
		这些区（还有两种不大重要的）在<linux/mmzone.h>中定义。
		
		区的实际使用和分布是与体系结构相关的。例如，某些体系结构在内存的任何地址上执行
	DMA都没有问题。在这些体系结构中，ZONE_DMA为空，ZONE_NORMAL就可以直接用于分
	配。与此相反，在x86体系结构上，ISA设备就不能在整个32位的地址空间中执行DMA，因
	为ISA设备只能访问物理内存的前16MB。因此，ZONE_DMA在x86上包含的页都在0-16MB
	的内存范围里。
	
		ZONE_HIGHMEM的工作方式也差不多。能否直接映射取决于体系结构。在32位x86系统
	上，ZONE_HIGHMEM为高于896MB的所有物理内存。在其他体系结构上，由于所有内存都被
	直接映射，所以ZONE_HIGHMEM为空。ZONE_HIGHMEM所在的内存就是所谓的高端内存
	（high memory）。系统的其余内存就是所谓的低端内存（low memory）。

		前两个区各取所需之后，剩余的就由ZONE_NORMAL区独享了。在x86上，ZONE_
	NORMAL是从16MB到896MB的所有物理内存。在其他（更幸运）的体系结构上，ZONE_
	NORMAL是所有的可用物理内存。表12-1是每个区及其在x86-32上所占页的列表。
	
		Linux把系统的页划分为区，形成不同的内存池，这样就可以根据用途进行分配了。例如，
	ZONE_DMA内存池让内核有能力为DMA分配所需的内存。如果需要这样的内存，那么，内核
	就可以从ZONE_DMA中按照请求的数目取出页。注意，区的划分没有任何物理意义，这只不过
	是内核为了管理页而采取的一种逻辑上的分组。
	
		某些分配可能需要从特定的区中获取页，而另外一些分配则可以从多个区中获取页。比如，
	尽管用于DMA的内存必须从ZONE_DMA中进行分配，但是一般用途的内存却既能从ZONE_
	DMA分配，也能从ZONE_NORMAL分配，不过不可能同时从两个区分配，因为分配是不能跨
	区界限的。当然，内核更希望一般用途的内存从常规区分配，这样能节省ZONE_DMA中的页，
	保证满足DMA的使用需求。但是，如果可供分配的资源不够用了（如果内存已经变得很少了），
	那么，内核就会去占用其他可用区的内存。
	
		不是所有的体系结构都定义了全部区，有些64位的体系结构，如Intel的x86-64体系结构
	可以映射和处理64位的内存空间，所以x86-64没有ZONE_HIGHMEM区，所有的物理内存都
	处于ZONE_DMA和ZONE_NORMAL区。
	
		每个区都用struct zone表示，在<linux/mmzone.h>中定义：
		
		struct zone {
			unsigned long				watermark[NR_WMARK];
			unsigned long				lowmem_reserve[MAX_NR_ZONES];
			struct per_cpu_pageset		pageset[NR_CPUS];
			spinlock_t					lock;
			struct free_area			free_area[MAX_ORDER];
			spinlock_t					lru_lock;
			struct zone_lru {
				struct list_head 		list;
				unsigned long 			nr_saved_scan;
			} lru[NR_LRU_LISTS];
			struct zone_reclaim_stat	reclaim_stat;
			unsigned long				page_scanned;
			unsigned long				flags;
			atomic_long_t				vm_stat[NR_VM_ZONE_STAT_ITEMS];
			int							prev_priority;
			unsigned					int inactive_ratio;
			wait_queue_head_t			*wait_table;
			unsigned long				wait_table_hash_nr_entries;
			unsigned long				wait_table_bits;
			struct pglist_data			*zone_pgdat;
			unsigned long				zone_start_pfn;
			unsigned long				spanned_pages;
			unsigned long				present_pages;
			const char					*name;
		};
		
		这个结构体很大，但是，系统中只有三个区，因此，也只有三个这样的结构休。让我们看一下
	其中一些重要的域。
	
		lock域是一个自旋锁，它防止该结构被并发访问。注意，这个域只保护结构，而不保护驻
	留在这个区中的所有页。没有特定的锁来保护单个页，但是，部分内核可以锁住在页中驻留的
	数据。
	
		watermark数组持有该区的最小值、最低和最高水位值。内核使用水位为每个内存区设置合
	适的内存消耗基准。该水位随空闲内存的多少而变化。
	
		name域是一个以NULL结束的字符串表示这个区的名字。内核启动期间初始化这个值，其
	代码位于mm/page_alloc.c中。三个区的名字分别为“DMA"、“Normal”和“HighMem”。
		
		
12.3 获得页

		我们已经对内核如何管理内存（页、区等）有所了解了，现在让我们看一下内核实现的接
	口，我们正是通过这些接口在内核分配和释放内存的。
	
		内核提供了一种请求内存的底层机制，并提供了对它进行访问的几个接口。所有这些接口都
	以页为单位分配内存，定义于<linux/gfp.h>中。最核心的函数是：
		struct page * alloc_pages(gfp_t gfp_mask, unsigned int order)
		
		该函数分配2的order次方(1<<order)个连续的物理页，并返回一个指针，该指针指向第一个页的
	page结构体；如果出错，就返回NULL。在第12.4节我们再研究gft_t类型和gft_mask参数。你可以
	用下面这个函数把给定的页转换成它的逻辑地址：
		void * page_address(struct page *page)
		
		该函数返回一个指针，指向给定物理页当前所在的逻辑地址。如果你无须用到struct page,
	你可以调用：
		unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
		
		这个函数与alloc_pages()作用相同，不过它直接返回所请求的第一个页的逻辑地址。因为页
	是连续的，所以其他页也会紧随其后。
	
		如果你只需一页，就可以用下面两个封装好的函数，它能让你少敲几下键盘：
		struct page * alloc_page(gfp_t gfp_mask)
		unsigned long __get_free_page(gfp_t gfp_mask)
		
		这两个函数与其兄弟函数工作方式相同，只不过传递给order的值为0(2的0次方等于1页)。
	

12.3.1 获得填充为0的页

		如果你需要让返回的页的内容全为0，请用下面这个函数：
		unsigned long get_zeroed_page(unsigned int gfp_mask)
		
		这个函数与__get_free_pages()工作方式相同，只不过把分配好的页都填充成了0————字节中
	的每一位都要取消设置。如果分配的页是给用户空间的，这个函数就非常有用了。虽说分配好
	的页中应该包含的都是随机产生的垃圾信息，但其实这些信息可能并不是完全随机的————它很
	可能“随机地”包含某些敏感数据。用户空间的页在返回之前，所有数据必须填充为0，或做
	其他清理工作，在保障系统安全这一点上，我们决不妥协。表12-2是所有底层的页分配方法
	的列表。
	
								表12-2 低级页分配方法
				标志										描述

			alloc_page(gfp_mask)				只分配一页，返回指向页结构的指针
			alloc_pages(gfp_mask, order)		分配2的order次方个页，返回指向第一页结构的指针
			__get_free_page(gfp_mask)			只分配一页，返回指向其逻辑地址的指针
			__get_free_pages(gfp_mask, order)	分配2的order次方页，返回指向第一页逻辑地址的指针
			get_zeroed_page(gfp_mask)			只分配一页，让其内容填充0，返回指向其逻辑地址的指针


12.3.2 释放页

		当你不再需要页时可以用下面的函数释放它们：
		void __free_pages(struct page *page, unsigned int order)
		void free_pages(unsigned long addr, unsigned int order)
		void free_page(unsigned long addr)
		
		释放页时要谨慎，只能释放属于你的页。传递了错误的struct page或地址，用了错误的
	order值，这些都可能导致系统崩溃。请记住，内核是完全信赖自己的。这点与用户空间不同，
	如果你有非法操作，内核会开开心心地把自己挂起来，停止运行。
	
		让我们看一个例子。其中，我们想得到8个页：
		unsigned long page;
		page = __get_free_pages(GFP_KERNEL, 3);
		if (!page) {
				/* 没有足够的内存：你必须处理这种错误！ */
				return -ENOMEM;
		}
		/* "page"现在指向8个连续页中第1个页的地址... */
		
		在此，我们使用完这8个页之后释放它们：
		free_pages(page, 3);
		
		/* 页现在已经被释放了，我们不应该再访问存放在"page"中的地址了 */

		GFP_KERNEL参数是gfp_mask标志的一个例子。前面我们已经简要讨论过。
		
		调用__get_free_pages()之后要注意进行错误检查。内核分配可能失败，因此你的代码必须进
	行检查并做相应的处理。这意味着在此之前，你所做的所有工作可能前功尽弃，甚至还需要回归到
	原来的状态。正因为如此，在程序开始时就先进行内存分配是很有意义的，这能让错误处理得容
	易一点。如果你不这么做，那么在你想要分配内存的时候如果失败了，局面可能就难以控制了。
	
		当你需要以页为单位的一族连续物理页时，尤其是在你只需要一两页时，这些低级页函数很
	有用。对于常用的以字节为单位的分配来说，内核提供的函数是kmalloc()。
	
	
12.4 kmalloc()

		kmalloc()函数与用户空间的malloc()一族函数非常类似，只不过它多了一个flags参数。
	kmalloc()函数是一个简单的接口，用它可以获得以字节为单位的一块内核内存。如果你需要
	整个页，那么，前面讨论的页分配接口可能是更好的选择。但是，对于大多数内核分配来说，
	kmalloc()接口用得更多。
	
		kmalloc()在<linux/slab.h>中声明：
		void * kmalloc(size_t size, gfp_t flags)
		
		这个函数返回一个指向内存块的指针，其内存块至少要有size大小。所分配的内存区在物
	理上是连续的。在出错时，它返回NULL。除非没有足够的内存可用，否则内核总能分配成功。
	在对kmalloc()调用之后，你必须检查返回的是不是NULL，如果是，要适当地处理错误。
	
		让我们看一个例子。我们随便假定存在一个dog结构体，现在需要为它动态地分配足够的空间：
		struct dog *p;
		p = kmalloc(sizeof(struct dog), GFP_KERNEL);
		if (!p)
				/* 处理错误 */

		如果kmalloc()调用成功，那么，ptr现在指向一个内存块，内存块的大小至少为所请求的大
	小。GFP_KERNEL标志表示在试图获取内存并返回给kmalloc()的调用者的过程中，内存分配器
	将要采取的行为。
		
		
12.4.1 gfp_mask标志

		我们已经看过了几个例子，发现不管是在低级页分配函数中，还是在kmalloc()中，都用到
	了分配器标志。现在，我们就深入讨论一下这些标志。
	
		这些标志可分为三类：行为修饰符、区修饰符及类型。行为修饰符表示内核应当如何分配所需
	的内存。在某些特定情况下，只能使用某些特定的方法分配内存。例如，中断处理程序就要求内核
	在分配内存的过程中不能睡眠（因为中断处理程序不能被重新调度）。区修饰符表示从哪儿分配内
	存。前面我们已经看到，内核把物理内存分为多个区，每个区用于不同的目的。区修饰符指明到底
	从这些区中的哪一区中进行分配。类型标志组合了行为修饰符和区修饰符，将各种可能用到的组合
	归纳为不同类型，简化了修饰符的使用；这样，你只需指定一个类型标志就可以了。GFP_KERNEL
	就是一种类型标志，内核中进程上下文相关的代码可以使用它。我们来看一下这些标志。
		
		1.行为修饰符
		
		所有这些标志，包括行为描述符都是在<linux/gfp.h>中声明的。不过，在<linux/slab.h>中
	包含有这个头文件，因此，你一般不必直接包含引用它。实际上，一般只使用类型修饰符就够
	了，我们随后会看到这点。因此，最好对每个标志都有所了解。表12-3是行为修饰符的列表。
	
									表12-3 行为修饰符

				标志											描述
			__GFP_WAIT							分配器可以睡眠 
			__GFP_HIGH							分配器可以访问紧急事件缓冲池
			__GFP_IO							分配器可以启动磁盘I/O
			__GFP_FS							分配器可以启动文件系统I/O
			__GFP_COLD							分配器应该使用高速缓存中快要淘汰出去的页
			__GFP_NOWARN						分配器将不打印失败警告
			__GFP_REPEAT						分配器在分配失败时重复进行分配，但是这次分配还存在失败的可能
			__GFP_NOFALL						分配器将无限地重复进行分配。分配不能失败
			__GFP_NORETRY						分配器在分配失败时绝不会重新分配
			__GFP_NO_GROW						由slab层内部使用
			__GFP_COMP							添加混合页元数据，在hugetlb的代码内部使用
			
		可以同时指定这些分配标志。例如：
		ptr = kmalloc(size, __GFP_WAIT | __GFP_IO | GFP_FS);
		
		说明页分配器（最终调用alloc_pages()）在分配时可以阻塞、执行I/O，在必要时还可以执
	行文件系统操作。这就让内核有很大的自由度，以便它尽可能找到空闲的内存来满足分配请求。
	
		大多数分配都会指定这些修饰符，但一般不是这样直接指定，而是采用我们随后讨论的类型
	标志。别担心，你不会在分配内存时为怎样使用这些标志而犯愁的！
	
		2.区修饰符
		
		区修饰符表示内存区应当从何处分配。通常，分配可以从任何区开始。不过，内核优先从
	ZONE_NORMAL开始，这样可以确保其他区在需要时有足够的空闲页可供使用。
	
		实际上只有两个区修饰符，因为除了ZONE_NORMAL之外只有两个区（默认都是从
	ZONE_NORMAL区进行分配）。表12-4是区修饰符的列表。
	
									表12-4  区修饰符
				标志										描述
			__GFP_DMA							从ZONE_DMA分配
			__GFP_DMA32							只在ZONE_DMA32分配
			__GFP_HIGHMEM						从ZONE_HIGHMEM或ZONE_NORMAL分配
			
		指定以上标志中的一个就可以改变内核试图进行分配的区。__GFP_DMA标志强制内核从
	ZONE_DMA分配。这个标志在说，有了这种奇怪的标识，我绝对可以拥有进行DMA的内存。
	相反，如果指定__GFP_HIGHMEM标志，则从ZONE_HIGHMEM（优先）或ZONE_NORMAL
	分配。这个标志在说，我可以使用高端内存，因此，我可以是一个玩偶，给你退还一些内存，但
	是，常规内存还照常工作。如果没有指定任何标志，则内核从ZONE_DMA或ZONE_NORMAL
	进行分配，当然优先从ZONE_NORMAL进行分配。不管区标志说什么了，只要它行为正常，我
	就不关心了。
	
		不能给_get_free_pages()或kalloc()指定ZONE_HIGHMEM，因为这两个函数返回的都是逻
	辑地址，而不是page结构，这两个函数分配的内存在当前有可能还没有映射到内核的虚拟地址空
	间，因此，也可能根本就没有逻辑地址。只有alloc_pages()才能分配高端内存。实际上，你的分
	配在大多数情况下都不必指定修饰符，ZONE_NORMAL就足矣。
	
		3. 类型标志

		类型标志指定所需的行为和区描述符以完成特殊类型的处理。正因为这一点，内核代码趋向
	于使用正确的类型标志，而不是一味地指定它可能需要用到的多个描述符。这么做既简单又不容
	易出错误。表12-5是类型标志的列表，而表12-6显示了每个类型标志与哪些修饰符相关联。
	
									表12-5 类型标志
			标志											描述
			
		GFP_ATOMIC			这个标志用在中断处理程序、下半部、持有自旋锁以及其他不能睡眠的地方
		GFP_NOWAIT			与GFP_ATOMIC类似，不同之处在于，调用不会退给紧急内存池。这就增加了内存分配失败
							的可能性。
		GFP_NOIO			这种分配可以阻塞，但不会启动磁盘I/O。这个标志在不能引发更多磁盘I/O时能阻塞I/O代
							码，这可能导致令人不愉快的递归
		GFP_NOFS			这种分配在必要时可能阻塞，也可能启动磁盘I/O，但是不会启动文件系统操作。这个标志在
							你不能再启动另一个文件系统的操作时，用在文件系统部分的代码中
		GFP_KERNEL			这是一种常规分配方式，可能会阻塞。这个标志在睡眠安全时用在进程上下文代码中。为了获
							得调用者所需的内存，内核会尽力而为。这个标志应当是首选标志
		GFP_USER			这是一种常规分配方式，可能会阻塞。这个标志用于为用户空间进程分配内存时
		GFP_HIGHUSER		这是从ZONE_HIGHMEME进行分配，可能会阻塞。这个标志用于为用户空间进程分配内存
		GFP_DMA				这是从ZONE_DMA进行分配。需要获取能供DMA使用的内存的设备驱动程序使用这个标志，
							通常与以上的某个标志组合在一起使用
							
							表12-6 在每种类型标志后隐含的修饰符列表							
				标志							修饰符标志

			GFP_ATOMIC				__GFP_HIGH
			GFP_NOWAIT				0
			GFP_NOIO				__GFP_WAIT
			GFP_NOFS				(__GFP_WAIT | __GFP_IO)
			GFP_KERNEL				(__GFP_WAIT | __GFP_IO | __GFP_FS)
			GFP_USER				(__GFP_WAIT | __GFP_IO | __GFP_FS)
			GFP_HIGHUSER			(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HIGHMEM)
			GFP_DMA					__GFP_DMA
			
		让我们看一下最常用的标志以及你什么时候、为什么需要使用它们。内核中最常用的标志是
	GFP_KERNEL。这种分配可能会引起睡眠，它使用的是普通优先级。因为调用可能阻塞，因此
	这个标志只用在可以重新安全调度的进程上下文中（也就是没有锁被持有等情况）。因为这个标
	志对内核如何获取请求的内存没有任何约束，所以内存分配成功的可能性很高。

		另一个截然相反的标志是GFP_ATOMIC。因为这个标志表示不能睡眠的内存分配，因此想
	要满足调用者获取内存的请求将会受到很严格的限制。即使没有足够的连续内存块可供使用，内
	核也很可能无法释放出可用内存来，因为内核不能让调用者睡眠。相反，GFP_KERNEL分配可
	以让调用者睡眠、交换、刷新一些页到硬盘等。因为GFP_ATOMIC不能执行以上任何操作，因
	此与GFP_KERNEL相比较，它分配成功的机会较小（尤其在内存短缺时）。即便如此，在当前
	代码（例如中断处理程序、软中断和tasklet）不能睡眠时，也只能选择GFP_ATOMIC。
	
		在以上两种标志中间的是GFP_NOIO和GFP_NOFS。以这两个标志进行的分配可能会引起
	阻塞，但它们会避免执行某些其他操作。GFP_NOIO分配绝不会启动任何磁盘I/O来帮助满足
	请求。而GFP_NOFS可能会启动磁盘I/O，但是它不会启动文件系统I/O。你为什么需要这些标
	志？它们分别用在某些低级块I/O或文件系统的代码中。设想，如果文件系统代码中需要分配内
	存，但没有使用GFP_NOFS。这种分配可能会引起更多的文件系统操作，而这些操作又会导致
	另外的分配，从而再引起更多的文件系统操作！这会一直持续下去。这样的代码在调用分配器的
	时候，必须确保分配器不会再执行到代码本身，否则，分配就可能产生死锁。也别紧张，内核使
	用这两个标志的地方是极少的。
	
		GFP_DMA标志表示分配器必须满足从ZONE_DMA进行分配的请求。这个标志用在需要
	DMA的内存的设备驱动程序中。一般你会把这个标志与GFP_ATOMIC和GFP_KERNEL结合起
	来使用。
	
		在你编写的绝大多数代码中，用到的要么是GFP_KERNEL，要么是GFP_ATOMIC。
	表12-7是通常情形和所用标志的列表。不管使用哪种分配类型，你都必须进行检查，并对
	错误进行处理。
	
							表12-7  什么时候用哪种标志
				情形								相应标志
		进程上下文，可以睡眠				使用GFP_KERNEL
		进程上下文，不可以睡眠				使用GFP_ATOMIC，在你睡眠之前或之后以GFP_KERNEL执行内存分配
		中断处理程序						使用GFP_ATOMIC
		软中断								使用GFP_ATOMIC
		tasklet								使用GFP_ATOMIC
		需要用于DMA的内存，可以睡眠			使用（GFP_DMA | GFP_KERNEL）
		需要用于DMA的内存，不可以睡眠		使用（GFP_DMA | GFP_ATOMIC），或在你睡眠之前执行内存分配
		
		
12.4.2 kfree()

		kmalloc()的另一端就是kfree()，kfree()声明于<linux/slab.h>中：
		
		void kfree(const void *ptr)
		
		kfree()函数释放由kmalloc()分配出来的内存块。如果想要释放的内存不是由kmalloc()分配
	的，或者想要释放的内存早就被释放了，比如说释放属于内核其他部分的内存，调用这个函数
	就会导致严重的后果。与用户空间类似，分配和回收要注意配对使用，以避免内存泄漏和其他
	bug。注意，调用kfree(NULL)是安全的。
	
		让我们看一个中断处理程序中分配内存的例子。在这个例子中，中断处理程序想分配一个
	缓冲区来保存输入数据。BUF_SIZE预定义为以字节为单位的缓冲区长度，它应该是大于两个字
	节的。
	
		char *bur;
		buf = kmalloc(BUF_SIZE, GFP_ATOMIC);
		if (!buf)
				/* 内存分配错误 */
		
		之后，当我们不再需要这个内存时，别忘了释放它：
		kfree(buf);
		
		
12.5 vmalloc()

		vmalloc()函数的工作方式类似于kmalloc()，只不过前者分配的内存虚拟地址是连续的，而
	物理地址则无须连续。这也是用户空间分配函数的工作方式：由malloc()返回的页在进程的虚拟
	地址空间内是连续的，但是，这并不保证它们在物理RAM中也是连续的。kmalloc()函数确保页
	在物理地址上是连续的（虚拟地址自然也是连续的）。vmalloc()函数只确保页在虚拟地址空间内
	是连续的。它通过分配非连续的物理内存块，再“修正”页表，把内存映射到逻辑地址空间的连
	续区域中，就能做到这点。
	
		大多数情况，只有硬件设备需要得到物理地址连续的内存。在很多体系结构上，硬件设备
	存在于内存管理单元以外，它根本不理解什么是虚拟地址。因此，硬件设备用到的任何内存区都
	必须是物理上连续的块，而不仅仅是虚拟地址连续上的块。而仅供软件使用的内存块（例如与进
	程相关的缓冲区）就可以使用只有虚拟地址连续的内存块。但在你的编程中，根本察觉不到这种
	差异。对内核而言，所有内存看起来都是逻辑上连续的。
	
		尽管在某些情况下才需要物理上连续的内存块，但是，很多内核代码都用kmalloc()来获
	得内存，而不是vmalloc()。这主要是出于性能的考虑。vmalloc()函数为了把物理上不连续的页
	转换为虚拟地址空间上连续的页，必须专门建立页表项。糟糕的是，通过vmalloc()获得的页必
	须一个一个地进行映射（因为它们物理上是不连续的），这就会导致比直接内存映射大得多的
	TLB抖动。因为这些原因，vmalloc()仅在不得已时才会使用————典型的就是为了获得大块内存
	时，例如，当模块被动态插入到内核中时，就把模块装载到由vmalloc()分配的内存上。
	
		vmalloc()函数声明在<linux/vmalloc.h>中，定义在<mm/vmalloc.c>中。用法与用户空间的
	malloc()相同：
	
		void * vmalloc(unsigned long size)
		
		该函数返回一个指针，指向逻辑上连续的一块内存区，其大小至少为size。在发生错误时，
	该函数返回NULL。函数可能睡眠，因此，不能从中断上下文中进行调用，也不能从其他不允许阻
	塞的情况下进行调用。
	
		要释放通过vmalloc()所获得的内存，使用下面的函数：
		
		void vfree(const void *addr)
		
		这个函数会释放从addr开始的内存块，其中addr是以前由vmalloc()分配的内存块地址。
	这个函数也可以睡眠，因此，不能从中断上下文中调用。它没有返回值。
	
		这个函数用起来比较简单：
		
		char *buf;
		bur = vmalloc(16 * PAGE_SIZE);  /* get 16 pages */
		if (!buf)
				/* 错误！不能分配内存 */
		/*
		 *  buf现在指向虚拟地址连续的一块内存区，其大小至少为16*PGAE_SIZE
		 */
		
		在分配内存之后，一定要释放它；
		vfree(buf);
		
		
12.6 slab层

		分配和释放数据结构是所有内核中最普遍的操作之一。为了便于数据的频繁分配和回收，编
	程人员常常会用到空闲链表。空闲链表包含可供使用的、已经分配好的数据结构块。当代码需要
	一个新的数据结构实例时，就可以从空闲链表中抓取一个，而不需要分配内存，再把数据放进
	去。以后，当不再需要这个数据结构的实例时，就把它放回空闲链表，而不是释放它。从这个意
	义上说，空闲链表相当于对象高速缓存————快速存储频繁使用的对象类型。
	
		在内核中，空闲链表面临的主要问题之一是不能全局控制。当可用内存变得紧缺时，内核无
	法通知每个空闲链表，让其收缩缓存的大小以便释放出一些内存来。实际上，内核根本就不知道
	存在任何空闲链表。为了弥补这一缺陷，也为了使代码更加稳固，Linux内核提供了slab层（也
	就是所谓的slab分配器）。slab分配器扮演了通用数据结构缓存层的角色。
	
		slab分配器的概念首先在Sun公司的SunOS 5.4操作系统中得以实现。Linux数据结构缓
	存层具有同样的名字和基本设计思想。
	
		slab分配器试图在几个基本原则之间寻求一种平衡：
		* 频繁使用的数据结构也会频繁分配和释放，因此应当缓存它们。
		* 频繁分配和回收必然会导致内存碎片（难以找到大块连续的可用内存）。为了避免这种现
		  象，空闲链表的缓存会连续地存放。因为已释放的数据结构又会放回空闲链表，因此不会
		  导致碎片。
		* 回收的对象可以立即投入下一次分配，因此，对于频繁的分配和释放，空闲链表能够提高
		  其性能。
		* 如果分配器知道对象大小、页大小和总的高速缓存的大小这样的概念，它会做出更明智的
		  决策。
		* 如果让部分缓存专属于单个处理器（对系统上的每个处理器独立而唯一），那么，分配和
		  释放就可以在不加SMP锁的情况下进行。
		* 如果分配器是与NUMA相关的，它就可以从相同的内存节点为请求者进行分配。
		* 对存放的对象进行着色（color），以防止多个对象映射到相同的高速缓存行（cache line）。
		
		Linux的slab层在设计和实现时充分考虑了上述原则。
		

12.6.1 slab层的设计

		slab层把不同的对象划分为所谓高速缓存组，其中每个高速缓存组都存放不同类型的对象。
	每种对象类型对应一个高速缓存。例如，一个高速缓存用于存放进程描述符（task_struct结构的
	一个空闲链表），而另一个高速缓存存放索引节点对象（struct inode）。有趣的是，kmalloc()接口
	建立在slab层之上，使用了一组通用高速缓存。
	
		然后，这些高速缓存又被划分为slab（这也是这个子系统名字的来由）。slab由一个或多个物
	理上连续的页组成。一般情况下，slab也就仅仅由一页组成。每个高速缓存可以由多个slab组成。
	
		每个slab都包含一些对象成员，这里的对象指的是被缓存的数据结构。每个slab处于三种
	状态之一：满、部分满或空。一个满的slab没有空闲的对象（slab中的所有对象都已被分配）。
	一个空的slab没有分配出任何对象（slab中的所有对象都是空闲的）。一个部分满的slab有一些
	对象已分配出去，有些对象还空闲着。当内核的某一部分需要一个新的对象时，先从部分满的
	slab中进行分配。如果没有部分满的slab，就从空的slab中进行分配。如果没有空的slab，就要
	创建一个slab了。显然，满的slab无法满足请求，因为它根本就没有空闲的对象。这种策略能
	减少碎片。
	
		作为一个例子，让我们考察一下inode结构，该结构是磁盘索引节点在内存中的体现（参
	见第13章）。这些数据结构会频繁地创建和释放，因此，用slab分配器来管理它们就很有必要。
	因而struct inode就由inode_cachep高速缓存（这是一种标准的命名规范）进行分配。这种高速
	缓存由一个或多个slab组成————由多个slab组成的可能性大一些，因为这样的对象数量很大。
	每个slab包含尽可能多的struct inode对象。当内核请求分配一个新的inode结构时，内核就从部
	分满的slab或空的slab（如果没有部分满的slab）返回一个指向已分配但未使用的结构的指针。
	当内核用完inode对象后，slab分配器就把该对象标记为空闲。图12-1显示了高速缓存、slab及
	对象之间的关系。

		每个高速缓存都使用kmem_cache结构来表示。这个结构包含三个链表：slabs_full、slabs_
	partial和slabs_empty，均存放在kmem_list3结构内，该结构在mm/slab.c中定义。这些链表包含
	高速缓存中的所有slab。slab描述符struct slab用来描述每个slab：
	
		struct slab {
				struct list_head list;			/* 满、部分满或空链表 */
				unsigned long    colouroff;		/* slab着色的偏移量 */
				void			 *s_mem;		/* 在slab中的第一个对象 */
				unsigned int	 inuse;			/* slab中已分配的对象数 */
				kmem_bufctl_t	 free;			/* 第一个空闲对象（如果有的话） */
		};
		
		slab描述符要么在slab之外另行分配，要么就放在slab自身开始的地方。如果slab很小，
	或者slab内部有足够的空间容纳slab描述符，那么描述符就存放在slab里面。
	
		slab分配器可以创建新的slab，这是通过__get_free_pages()低级内核页分配器进行的：
		
		static void *kmem_getpages(struct kmem_cache *cachep, gfp_t flags, int nodeid)
		{
			struct page *page;
			void *addr;
			int i;
			
			flags |= cachep->gfpflags;
			if (likely(nodeid == -1)) {
					addr = (void*)__get_free_pages(flags, cachep->gfporder);
					if (!addr)
						return NULL;
					page = virt_to_page(addr);
			} else {
					page = alloc_pages_node(nodeid, flags, cachep->gfporder);
					if (!page)
						return NULL;
					addr = page_address(page);
			}
			
			i = (1 << cachep->gfporder);
			if (cachep->flags & SLAB_RECLAIM_ACCOUNT)
					atomic_add(i, &slab_reclaim_pages);
			add_page_state(nr_slab, i);
			while (i--) {
				SetPageSlab(page);
				page++;
			}
			return addr;
		}

		该函数使用__get_free_pages()来为高速缓存分配足够多的内存。该函数的第一个参数就指向
	需要很多页的特定高速缓存。第二个参数是要传给__get_free_pages()的标志，注意这个标志是如
	何与另一个值进行二进制“或”运算的，这相当于把高速缓存需要的缺省标志加到flags参数上。
	分配的页大小为2的幂次方，存放在cachep->gfporder中。由于与分配器NUMA相关的代码的
	关系前面这个函数比想象的要复杂一些。当nodeid是一个非负数时，分配器就试图对从相同的
	内存节点给发出的请求进行分配。这在NUMA系统上提供了较好的性能，但是访问节点之外的
	内存会导致性能的损失。
	
		为了便于理解，我们可以忽略与NUMA相关的代码，写一个简单的kmem_getpages()函数：
		
		static inline void * kmem_getpages(struct kmem_cache *cachep, gfp_t flags)
		{
				void *addr;
				flag |= cachep->gfpflags;
				addr = (void*)__get_free_pages(flags, cachep->gfporder);
				return addr;
		}
		
		接着，调用kmem_freepages()释放内存，而对给定的高速缓存页，kmem_freepages()最终调
	用的是free_pages()。当然，slab层的关键就是避免频繁分配和释放页。由此可知，slab层只有当
	给定的高速缓存部分中既没有满也没有空的slab时才会调用页分配函数。而只有在下列情况下
	才会调用释放函数：当可用内存变得紧缺时，系统试图释放出更多的内存以供使用；或者当高速缓
	存显式地被撤销时。

		slab层的管理是在每个高速缓存的基础上，通过提供给整个内核一个简单的接口来完成的。
	通过接口就可以创建和撤销新的高速缓存，并在高速缓存内分配和释放对象。高速缓存及其内
	slab的复杂管理完全通过slab层的内部机制来处理。当你创建一个高速缓存后，slab层所起的
	作用就像一个专用的分配器，可以为具体的对象类型进行分配。
	

12.6.2 slab分配器的接口

		一个新的高速缓存通过以下函数创建：
		
		struct kmem_cache * kmem_cache_create(const char *name,
											  size_t size,
											  size_t align,
											  unsigned long flags,
											  void (*ctor)(void *));
		
		第一个参数是一个字符串，存放着高速缓存的名字；第二个参数是高速缓存中每个元素的大
	小；第三个参数是slab内第一个对象的偏移，它用来确保在页内进行特定的对齐。通常情况下，
	0就可以满足要求，也就是标准对齐。flags参数是可选的设置项，用来控制高速缓存的行为。它
	可以为0，表示没有特殊的行为，或者与以下标志中的一个或多个进行“或”运算：
		* SLAB_HWCACHE_ALIGN————这个标志命令slab层把一个slab内的所有对象按高速缓
		  存行对齐。这就防止了“错误的共享”（两个或多个对象尽管位于不同的内存地址，但映
		  射到相同的高速缓存行）。这可以提高性能，但以增加内存开销为代价，因为对齐越严格，
		  浪费的内存就越多。到底会耗费掉多少内存，取决于对象的大小以及对象相对于系统高速
		  缓存行对齐的方式。对于会频繁使用的高速缓存，而且代码本身对性能要求又很严格的情
		  况，设置该选项是理想的选择；否则，请三思而后行。
		* SLAB_POISON————这个标志使slab层用已知的值（a5a5a5a5）填充slab。这就是所谓的
		  “中毒”，有利于对未初始化内存的访问。
		* SLAB_RED_ZONE—————这标志导致slab层在已分配的内存周围插入“红色警界区”以探
		  测缓冲越界。
		* SLAB_PANIC————这个标志当分配失败时提醒slab层。这在要求分配只能成功的时候非常
		  有用。比如，在系统初启时分配一个VMA结构的高速缓存（参见第15章）。
		* SLAB_CACHE_DMA————这个标志命令slab层使用可以执行DMA的内存给每个slab分配
		  空间。只有在分配的对象用于DMA，而且必须驻留在ZONE_DMA区时才需要这个标志。
		  否则，你既不需要也不应该设置这个标志。
		  
		最后一个参数ctor是高速缓存的构造函数。只有在新的页追加到高速缓存时，构造函数才
	被调用。实际上，Linux内核的高速缓存不使用构造函数。事实上这里曾经还有过一个析构函数
	参数，但是由于内核代码并不需要它，因此已经被抛弃了。你可以将ctor参数赋值为NULL。
	
		kmem_cache_create()在成功时会返回一个指向所创建高速缓存的指针；否则，返回NULL。
	这个函数不能在中断上下文中调用，因为它可能会睡眠。
	
		要撤销一个高速缓存，则调用：
		int kmem_cache_destroy(struct kmem_cache *cachep)
		
		顾名思义，这样就可以撤销给定的高速缓存。这个函数通常在模块的注销代码中被调用，当
	然，这里指创建了自己的高速缓存的模块。同样，也不能从中断上下文中调用这个函数，因为它
	也可能睡眠。调用该函数之前必须确保存在以下两个条件：
		
		* 高速缓存中的所有slab都必须为空。其实，不管哪个slab中，只要还有一个对象被分配出
		  去并正在使用的话，那怎么可能撤销这个高速缓存呢？
		* 在调用kmem_cache_destroy()过程中（更不用说在调用之后了）不再访问这个高速缓存。
		  调用者必须确保这种同步。
		  
		该函数在成功时返回0，否则返回非0值。
		
		1.从缓存中分配
		
		创建高速缓存之后，就可以通过下列函数获取对象：
		void * kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
		
		该函数从给定的高速缓存cachep中返回一个指向对象的指针。如果高速缓存的所有slab中
	都没有空闲的对象，那么slab层必须通过kmem_getpages()获取新的页，flags的值传递给_get_
	free_pages()。这与我们前面看到的标志相同，你用到的应该是GFP_KERNEL或GFP_ATOMIC。
		
		最后释放一个对象，并把它返回给原先的slab，可以使用下面这个函数：
		void kmem_cache_free(struct kmem_cache *cachep, void *objp)
			
		这样就能把cachep中的对象objp标记为空闲。
			
		2. slab分配器的使用实例
		
		让我们考察一个鲜活的实例，这个例子用的是task_struct结构（进程描述符）。代码稍微有
	点复杂，取自kernel/fork.c。
	
		首先，内核用一个全局变量存放指向task_struct高速缓存的指针：
		struct kmem_cache *task_struct_cachep;
		
		在内核初始化期间，在定义于kernel/fork.c的fork_init()中会创建高速缓存：
		
		task_struct_cachep = kmem_cache_create("task_struct",
												sizeof(struct task_struct),
												ARCH_MIN_TASKALIGN,
												SLAB_PANIC | SLAB_NOTRACK,
												NULL);
												
		这样就创建了一个名为task_struct的高速缓存，其中存放的就是类型为struct task_struct
	的对象。该对象被创建后存放在slab中偏移量为ARCH_MIN_TASKALIGN个字节的地方，
	ARCH_MIN_TASKALIG预定义值与体系结构相关。通常将它定义为L1_CACHE_BYTES————
	L1高速缓存的字节大小。没有构造函数或析构函数。注意不用检查返回值是否为失败标记
	NULL，因为SLAB_PANIC标志已经被设置了。如果分配失败，slab分配器就调用panic()函数。
	如果没有提供SLAB_PANIC标志，就必须自己检查返回值。SLAB_PANIC标志用在这儿是因为
	这是系统操作必不可少的高速缓存（没有进程描述符，机器自然不能正常运行）。
	
		每当进程调用fork()时，一定会创建一个新的进程描述符（回忆一下第3章）。这是在dup_
	task_struct()中完成的，而该函数会被do_fork()调用：
		
		struct task_struct *tsk;
		
		tsk = kmem_cache_alloc(task_struct_cachep, GFP_KERNEL);
		if (!tsk)
				return NULL;
		
		进程执行完后，如果没有子进程在等待的话，它的进程描述符就会被释放，并返回给task_
	struct_cachep slab高速缓存。这是在free_task_struct()中执行的（这里，tsk是现有的进程）：
		
		kmem_cache_free(task_struct_cachep, tsk);
		
		由于进程描述符是内核的核心组成部分，时刻都要用到，因此task_struct_cachep高速缓存
	绝不会被撤销掉。即使真能撤销，我们也要通过下列函数阻止其被撤销：
		
		int err;
		
		err = kmem_cache_destroy(task_struct_cachep);
		if (err)
			/* 出错，撤销高速缓存 */

		很容易吧？slab层负责内存紧缺情况下所有底层的对齐、着色、分配、释放和回收等。如
	果你要频繁创建很多相同类型的对象，那么，就应该考虑使用slab高速缓存。也就是说，不要
	自己去实现空闲链表！


12.7 在栈上的静态分配

		在用户空间，我们以前所讨论到的那些分配的例子，有不少都可以在栈上发生。因为我们毕
	竟可以事先知道所分配空间的大小。用户空间能够奢侈地负担起非常大的栈，而且栈空间还可以
	动态增长，相反，内核却不能这么奢侈————内核栈小而且固定。当给每个进程分配一个固定大小
	的小栈后，不但可以减少内存的消耗，而且内核也无须负担太重的栈管理任务。
	
		每个进程的内核栈大小既依赖体系结构，也与编译时的选项有关。历史上，每个进程都有两
	页的内核栈。因为32位和64位体系结构的页面大小分别是4KB和8KB，所以通常它们的内核
	栈的大小分别是8KB和16KB。
	

12.7.1 单页内核栈

		但是，在2.6系列内核的早期，引入了一个选项设置单页内核栈。当激活这个选项时，每个
	进程的内核栈只有一页那么大，根据体系结构的不同，或为4KB，或为8KB。这么做出于两个
	原因：首先，可以让每个进程减少内存消耗。其次，也是最重要的，随着机器运行时间的增加，
	寻找两个未分配的、连续的页变得越来越困难。物理内存渐渐变为碎片，因此，给一个新进程分
	配虚拟内存（VM）的压力也在增大。
	
		还有一个更复杂的原因。继续跟随我：我们几乎掌握了关于内核栈的全部知识。现在，每个
	进程的整个调用链必须放在自己的内核栈中。不过，中断处理程序也曾经使用它们所中断的进程
	的内核栈，这样，中断处理程序也要放在内核栈中。这当然有效而简单，但是，这同时会把更严
	格的约束条件加在这可怜的内核栈上。当我们转而使用只有一个页面的内核栈时，中断处理程序
	就不放在栈中了。
	
		为了矫正这个问题，内核开发者们实现了一个新功能：中断栈。中断栈为每个进程提供一个
	用于中断处理程序的栈。有了这个选项，中断处理程序不用再和被中断进程共享一个内核栈，它
	们可以使用自己的栈了。对每个进程来说仅仅耗费了一页而已。

		总的来说，内核栈可以是1页，也可以是2页，这取决于编译时配置选项。栈大小因此在
	4~16KB的范围内。历史上，中断处理程序和被中断进程共享一个栈。当1页栈的选项激活时，
	中断处理程序获得了自己的栈。在任何情况下，无限制的递归和alloca()显然是不被允许的。
	
		好，就讲到这里。大家明白了吗？


12.7.2 在栈上光明正大地工作

		在任意一个函数中，你都必须尽量节省栈资源。这并不难，也没有什么窍门，只需要在具体
	的函数中让所有局部变量（即所谓的自动变量）所占空间之和不要超过几百字节。在栈上进行大
	量的静态分配（比如分配大型数组或大型结构体）是很危险的。要不然，在内核中和在用户空间
	中进行的栈分配就没有什么差别了。栈溢出时悄无声息，但势必会引起严重的问题。因为内核没
	有在管理内核栈上做足工作，因此，当栈溢出时，多出的数据就会直接溢出来，覆盖掉紧邻堆栈
	末端的东西。首先面临考验的就是thread_info结构（回想一下第3章，这个结构就贴着每个进
	程内核堆栈的末端）。在堆栈之外，任何内核数据都可能存在潜在的危险。当栈溢出时，最好的
	情况是机器宕机，最坏的情况是悄无声息地破坏数据。
	
		因此，进行动态分配是一种明智的选择，本章前面有关大块内存的分配就是采用这种方式。


12.8 高端内存的映射

		根据定义，在高端内存中的页不能永久地映射到内核地址空间上。因此，通过alloc_pages()
	函数以__GFP_HIGHMEM标志获得的页不可能有逻辑地址。
	
		在x86体系结构上，高于896MB的所有物理内存的范围大都是高端内存，它并不会永久
	地或自动地映射到内核地址空间，尽管x86处理器能够寻址物理RAM的范围达到4GB（启用
	PAE可以寻址到64GB）。一旦这些页被分配，就必须映射到内核的逻辑地址空间上。在x86
	上，高端内存中的页被映射到3GB~4GB。
	

12.8.1 永久映射

		要映射一个给定的page结构到内核地址空间，可以使用定在文件<linux/highmem.h>中的
	这个函数：
		
		void *kmap(struct page *page)
		
		这个函数在高端内存或低端内存上都能用。如果page结构对应的是低端内存中的一页，函
	数只会单纯地返回该页的虚拟地址。如果页位于高端内存，则会建立一个永久映射，再返回地
	址。这个函数可以睡眠，因此kmap()只能用在进程上下文中。
	
		因为允许永久映射的数量是有限的（如果没有这个机制，我们就不必搞得这么复杂，把所有
	内存通通映射为永久内存就行了），当不再需要高端内存时，应该解除映射，这可以通过下列函
	数完成：
		
		void kunmap(struct page *page)
		
	
12.8.2 临时映射

		当必须创建一个映射而当前的上下文又不能睡眠时，内核提供了临时映射（也就是所谓的原
	子映射）。有一组保留的映射，它们可以存放新创建的临时映射。内核可以原子地把高端内存中
	的一个页映射到某个保留的映射中。因此，临时映射可以用在不能睡眠的地方，比如中断处理程
	序中，因为获取映射时绝不会阻塞。
	
		通过下列函数建立一个临时映射:
	
		void *kmap_atomic(struct page *page, enum km_type type)
		
		参数type是下列枚举类型之一，这些枚举类型描述了临时映射目的。它们定义于<asm/
	kmap_types.h>中：
	
		enum km_type {
				KM_BOUNCE_READ,
				KM_SKB_SUNRPC_DATA,
				KM_SKB_DATA_SOFTIRQ,
				KM_USER0,
				KM_USER1,
				KM_BIO_SRC_IRQ,
				KM_BIO_DST_IRQ,
				KM_PTE0,
				KM_PTE1,
				KM_PTE2,
				KM_IRQ0,
				KM_IRQ1,
				KM_SOFTIRQ0,
				KM_SOFTIRQ1,
				KM_SYNC_ICACHE,
				KM_SYNC_DCACHE,
				KM_UML_USERCOPY,
				KM_IRQ_PTE,
				KM_NMI,
				KM_NMI_PTE,
				KM_TYPE_NR
		};

		这个函数不会阻塞，因此可以用在中断上下文和其他不能重新调度的地方。它也禁止内核抢
	占，这是有必要的，因为映射对每个处理器都是唯一的（调度可能对哪个处理器执行哪个进程做
	变动）。
	
		通过下列函数取消映射：
		
		void kunmap_atomic(void *kvaddr, enum km_type type)
		
		这个函数也不会阻塞。在很多体系结构中，除非激活了内核抢占，否则kmap_atomic()根本
	就无事可做，因为只有在下一个临时映射到来前上一个临时映射才有效。因此，内核完全可以
	“忘掉”kmap_atomic()映射，kunmap_atomic()也无须做什么实际的事情。下一个原子映射将自动
	覆盖前一个映射。
	
	
12.9 每个CPU的分配

		支持SMP的现代操作系统使用每个CPU上的数据，对于给定的处理器其数据是唯一的。一
	般来说，每个CPU的数据存放在一个数组中。数组中的每一项对应着系统上一个存在的处理器。
	按当前处理器号确定这个数组的当前元素，这就是2.4内核处理每个CPU数据的方式。这种方
	式还不错，因此，2.6内核的很多代码依然用它。可以声明数据如下：
	
		unsigned long my_percpu[NU_CPUS];
		
		然后，按如下方式访问它：
		
		int cpu;
		
		cpu = get_cpu();          // 获得当前处理器，并禁止内核抢占
		my_percpu[cpu]++;		  // ... 或者无论什么
		printk("my_percpu on cpu=%d is %lu\n", cpu, my_percpu[cpu]);
		put_cpu();				  // 激活内核抢占
		
		注意，上面的代码中并没有出现锁，这是因为所操作的数据对当前处理器来说是唯一的。除
	了当前处理器之外，没有其他处理器可接触到这个数据，不存在并发访问的问题，所以当前处理器
	可以在不用锁的情况下安全访问它。

		现在，内核抢占成为了唯一需要关注的问题了，内核抢占会引起下面提到的两个问题：
		* 如果你的代码被其他处理器抢占并重新调度，那么这时CPU变量就会无效，因为它指向
		  的是错误的处理器（通常，代码获得当前处理器后是不可以睡眠的）。
		* 如果另一个任务抢占了你的代码，那么有可能在同一个处理器上发生并发访问my_percpu
		  的情况，显然这属于一个竞争条件。
		  
		虽然如此，但是你大可不必惊慌，因为在获取当前处理器号，即调用get_cpu()时，就已经
	禁止了内核抢占。相应的在调用put_cpu()时又会重新激活当前处理器号。注意，只要你总使用
	上述方法来保护数据安全，那么，内核抢占就不需要你自己去禁止。
		  
		
12.10 新的每个CPU接口

		2.6内核为了方便创建和操作每个CPU数据，而引进了新的操作接口，称作percpu。该接口
	归纳了前面所述的操作行为，简化了创建和操作每个CPU的数据。
	
		但前面我们讨论的创建和访问每个CPU的方法依然有效，不过大型对称多处理器计算机要
	求对每个CPU数据操作更简单，功能更强大，正是在这种背景下，新接口应运而生。
	
		头文件<linux/percpu.h>声明了所有的接口操作例程，你可以在文件mm/slab.c和<asm/
	percpu.h>中找到它们的定义。
	
	
12.10.1 编译时的每个CPU数据

		在编译时定义每个CPU变量易如反掌：
		
		DEFINE_PER_CPU(type, name);
		
		这个语句为系统中的每一个处理器都创建了一个类型为type，名字为name的变量实例，如
	果你需要在别处声明变量，以防范编译时警告，那么下面的宏将是你的好帮好：
		
		DECLARE_PER_CPU(type, name);
		
		你可以利用get_cpu_var()和put_cpu_var()例程操作变量。调用get_cpu_var()返回当前处理
	器上的指定变量，同时它将禁止抢占；另一方面put_cpu_var()将相应的重新激活抢占。
	
		get_cpu_var(name)++;        // 增加该处理器上的name变量的值
		put_cpu_var(name);          // 完成：重新激活内核抢占
		
		你也可以获得别的处理器上的每个CPU数据：
		
		per_cpu(name, cpu)++;       // 增加指定处理器上的name变量值
		
		使用此方法你需要格外小心，因为per_cpu()函数既不会禁止内核抢占，也不会提供任何形
	式的锁保护。如果一些处理器可以接触到其他处理器的数据，那么你就必须要给数据上锁。注
	意，第9章和第10章详细讨论了数据上锁问题。
	
		另外还有一个需要提醒的问题：这些编译时每个数据的例子并不能在模块内使用，因
	为连接程序实际上将它们创建在一个唯一的可执行段中（.data.percpu）。如果你需要从模块中访
	问每个CPU数据，或者如果你需要动态创建这些数据，那还是有希望的。
	
	
12.10.2 运行时的每个CPU数据

		内核实现每个CPU数据的动态分配方法类似于kmalloc()。该例程为系统上的每个处理器创
	建所需内存的实例，其原型在文件<linux/percpu.h>中：
	
		void *alloc_percpu(type);      // 一个宏
		void *__alloc_percpu(size_t size, size_t align);
		void free_percpu(const void *);
		
		宏alloc_percpu()给系统中的每个处理器分配一个指定类型对象的实例。它其实是宏__alloc_
	percpu()的一个封装，这个原始宏接收的参数有两个：一个是要分配的实际字节数，一个是分配
	时要按多少字节对齐。而封装后的alloc_percpu()按照单字节对齐————按照给定类型的自然边界
	对齐。这种对齐方式最为常用。比如：
		
		struct rabid_cheetah = alloc_percpu(struct rabid_cheetah);
		
		它等价于
		
		struct rabid_cheetah = __alloc_percpu(sizeof (struct rabid_cheetah),
		__alignof__ (struct rabid_cheetah));
		
		__alignof__是gcc的一个功能，它会返回指定类型或lvalue所需的（或建议的，要知道有
	些古怪的体系结构并没有字节对齐的要求）对齐字节数。它的语义和sizeof一样，比如，下列程
	序在x86体系中将返回4：
		
		__alignof__ (unsigned long)
		
		如果指定一个lvalue,那么将返回lvalue的最大对齐字节数。比如一个结构中的lvalue相比
	结构外的lvalue可能有更大的对齐字节需求，这是结构本身的对齐要求缘故。有关对齐的进一
	步讨论我们放在第19章中介绍。
		
		相应的调用free_percpu()将释放所有处理器上指定的每个CPU数据。
		
		无论是alloc_percpu()或是__alloc_percpu()都会返回一个指针，它用来间接引用动态创建的
	每个CPU数据，内核提供了两个宏来利用指针获取每个CPU数据：
		
		get_cpu_var(ptr);         // 返回一个void类型指针，该指针指向处理器的ptr的拷贝
		put_cpu_var(ptr);         // 完成：重新激活内核抢占
		
		get_cpu_var()宏返回了一个指向当前处理器数据的特殊实例，它同时会禁止内核抢占；而在
	et_cpu_var()宏中会重新激活内核抢占。
	
		我们看一个使用这些函数的完整例子。当然这个例子有点无聊，因为通常你会一次分配够
	内存（比如，在某些初始化函数中），就可以在各种地方使用它，或再一次释放（比如，有一些
	清理函数中）。不过，这个例子可清楚地说明如何使用这些函数。
	
		void *percpu_ptr;
		unsigned long *foo;
		
		percpu_ptr = alloc_percpu(unsigned long);
		if (!ptr)
				// 内存分配错误...
				
		foo = get_cpu_var(percpu_ptr);
		// 操作foo ...
		put_cpu_var(percpu_ptr);
		
		
12.11 使用每个CPU数据的原因

		使用每个CPU数据具有不少好处。首先是减少了数据锁定。因为按照每个处理器访问每个
	CPU数据的逻辑，你可以不再需要任何锁。记住“只有这个处理器能访问这个数据”的规则纯
	粹是一个编程约定。你需要确保本地处理器只会访问它自己的唯一数据。系统本身并不存在任何
	措施禁止你从事欺骗活动。
	
		第二个好处是使用每个CPU数据可以大大减少缓存失败。失效发生在处理器试图使用它们的
	缓存保持同步时。如果一个处理器操作某个数据，而该数据又存放在其他处理器缓存中，那么存
	放该数据的那个处理器必须清理或刷新自己的缓存。持续不断的缓存失败称为缓存抖动，这样对
	系统性能影响颇大。使用每个CPU数据将使得缓存影响降至最低，因为理想情况下只会访问自
	己的数据，percpu接口缓存一对齐（cache_align）所有数据，以便确保在访问一个处理器的数据
	时，不会将另一个处理器的数据带入同一个缓存线上。
	
		综上所述，使用每个CPU数据会省去许多（或最小化）数据上锁，它唯一的安全要求就
	是要禁止内核抢占。而这点代价相比上锁要小得多，而且接口会自动帮你完成这个步骤。每个
	CPU数据在中断上下文或进程上下文中使用都很安全。但要注意，不能在访问每个CPU数据过
	程中睡眠————否则，你就可能醒来后已经到了其他处理器上了。
	
		目前并不要求必须使用每个CPU的新接口。只要你禁止了内核抢占，用手动方法（利用我
	们原来讨论的数组）就很好，但是新接口的将来更容易使用，而且功能也会得到长足的优化。如
	果确实决定在你的内核中使用每个CPU数据，请考虑使用新接口。但我要提醒的是————新接口
	并不向后兼容之前的的内核。
	
	
12.12 分配函数的选择
	
		在这么多分配函数和方法中，有时并不能搞清楚到底该选择哪种方式分配————但这确实很重
	要。如果你需要连续的物理页，就可以使用某个低级页分配器或kmalloc()。这是内核中内存分
	配的常用方式，也是大多数情况下你自己应该使用的内存分配方式。回忆一下，传递给这些函数
	的两个最常用的标志是GFP_ATOMIC和GFP_KERNEL。GFP_ATOMIC表示进行不睡眠的高优
	先级分配，这是中断处理程序和其他不能睡眠的代码段的需要。对于可以睡眠的代码，（比如没
	有持自旋锁的进程上下文代码）则应该使用GFP_KERNEL获取所需的内存。这个标志表示如果
	有必要，分配时可以睡眠。
	
		如果你想从高端内存进行分配，就使用alloc_pages()。alloc_pages()函数返回一个指向struct
	page结构的指针，而不是一个指向某个逻辑地址的指针。因为高端内存很可能并没有被映射，
	因此，访问它的唯一方式就是通过相应的struct page结构。为了获得真正的指针，应该调用
	kmap()，把高端内存映射到内核的逻辑地址空间。
		
		如果你不需要物理上连续的页，而仅仅需要虚拟地址上连续的页，那么就使用vmalloc()（不
	过要记住vmalloc()相对kmalloc()来说，有一定的性能损失）。vmalloc()函数分配的内存虚拟地址
	是连续的，但它本身并不保证物理上的连续。这与用户空间的分配非常类似，它也是把物理内存
	块映射到连续的逻辑地址空间上。
	
		如果你要创建和撤销很多大的数据结构，那么考虑建立slab高速缓存。slab层会给每个处理
	器维持一个对象高速缓存（空闲链表），这种高速缓存会极大地提高对象分配和回收的性能。slab
	层不是频繁地分配和释放内存，而是为你把事先分配好的对象存放到高速缓存中。当你需要一块
	新的内存来存放数据结构时，slab层一般无须另外去分配内存，而只需要从高速缓存中得到一个
	对象就可以了。
	
	
12.13 小结

		本章中，我们学习了Linux内核如何管理内存。我们首先看到了内存空间的各种不同的描述
	单位，包括字节、页面和区（在第15章的进程地址空间中可看到4种不同层次的内存单位）。我
	们接着讨论了各种内存分配机制，其中包括页分配器和slab分配器。在内核中分配内存并非总
	是轻而易举，因为你必须小心地确保分配过程遵从内核特定的状态约束。比如分配过程中不得堵
	塞，或者访问文件系统等约束。为此我们讨论了gfp标识以及使用每个标识的针对场景。分配内
	存相对复杂是内核开发和用户程序开发的最大区别之一，本章使用大量篇幅描述内存分配的各种
	不同接口————通过这些不同调用接口，你应该能感觉到内核中分配内存为什么更复杂的原因。在
	本章基础上，在第13章我们讨论虚拟文件系统（VFS）————负责管理文件系统且为用户空间程
	序提供一致性接口的内核子系统。我们继续深入！





    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    


*****************************************************************************************

									第13章 虚拟文件系统
								
*****************************************************************************************



		虚拟文件系统（有时也称作虚拟文件交换，更常见的是简称VFS）作为内核子系统，为用
	户空间程序提供了文件和文件系统相关的接口。系统中所有文件系统不但依赖VFS共存，而且
	也依靠VFS系统协同工作，通过虚拟文件系统，程序可以利用标准的Unix系统调用对不同的文
	件系统，甚至不同介质上的文件系统进行读写操作，如图13-1所示。
	
	
13.1 通用文件系统接口

		VFS使得用户可以直接使用open()、read()和write()这样的系统调用而无须考虑具体文件系统
	和实际物理介质。现在听起来这并没什么新奇的（我们早就认为这是理所当然的），但是，使得这
	些通用的系统调用可以跨越各种文件系统和不同介质执行，绝非微不足道的成绩。更了不起的
	是，系统调用可以在这些不同的文件系统和介质之间执行————我们可以使用标准的系统调用从一
	个文件系统拷贝或移动数据到另一个文件系统。老式的操作系统（比如DOS）是无力完成上述
	工作的，任何对非本地文件系统的访问都必须依靠特殊工具才能完成。正是由于现代操作系统引
	入抽象层，比如Linux，通过虚拟接口访问文件系统，才使得这种协作性和泛型存取成为可能。
	
		新的文件系统和新类型的存储介质都能找到进入Linux之路，程序无需重写，甚至无须重新
	编译。在本章中，我们将讨论VFS，它把各种不同的文件系统抽象后采用统一的方式进行操作。
	在第14章中，我们将讨论块I/O层，它支持各种各样的存储设备————从CD到蓝光光盘，从硬
	件设备再到压缩闪存。VFS与块I/O相结合，提供抽象、接口以及交融，使得用户空间的程序调
	用统一的系统调用访问各种文件，不管文件系统是什么，也不管文件系统位于何种介质，采用的
	命名策略是统一的。
	
	
13.2 文件系统抽象层

		之所以可以使用这种通用接口对所有类型的文件系统进行操作，是因为内核在它的底层文件
	系统接口上建立了一个抽象层。该抽象层使Linux能够支持各种文件系统，即便是它们在功能和
	行为上存在很大差别。为了支持多文件系统，VFS提供了一个通用文件系统模型，该模型囊括了
	任何文件系统的常用功能集和行为。当然，该模型偏重于Unix风格的文件系统（我们将在后面
	的小节看到Unix风格的文件系统的构成）。但即使这样，Linux仍然可以支持很多种差异很大的
	文件系统，从DOS系统的FAT到Windows系统的NTFS，再到各种Unix风格文件系统和Linux
	特有的文件系统。
	
		VFS抽象层之所以能衔接各种各样的文件系统，是因为它定义了所有文件系统都支持的、
	基本的、概念上的接口和数据结构。同时实际文件系统也将自身的诸如“如何打开文件”，“目录
	是什么”等概念在形式上与VFS的定义保持一致。因为实际文件系统的代码在统一的接口和数
	据结构下隐藏了具体的实现细节，所以在VFS层和内核的其他部分看来，所有文件系统都是相
	同的，它们都支持像文件和目录这样的概念，同时也支持像创建文件和删除文件这样的操作。
	
		内核通过抽象层能够方便、简单地支持各种类型的文件系统。实际文件系统通过编程提供
	VFS所期望的抽象接口和数据结构，这样，内核就可以毫不费力地和任何文件系统协同工作，并
	且这样提供给用户空间的接口，也可以和任何文件系统无缝地连接在一起，完成实际工作。
	
		其实在内核中，除了文件系统本身外，其他部分并不需要了解文件系统的内部细节。比如一
	个简单的用户空间程序执行如下的操作：
	
		ret = write(fd, buf, len);
		
		该系统调用将buf指针指向的长度为len字节的数据写入文件描述符fd对应的文件的当前
	位置。这个系统调用首先被一个通用系统调用sys_write()处理，sys_write()函数要找到fd所在
	的文件系统实际给出的是哪个写操作，然后再执行该操作。实际文件系统的写方法是文件系统实
	现的一部分，数据最终通过该操作写入介质（或执行这个文件系统想要完成的写动作）。图13-2
	描述了从用户空间的write()调用到数据被写入磁盘介质的整个流程。一方面，系统调用是通用
	VFS接口，提供给用户空间的前端；另一方面，系统调用是具体文件系统的后端，处理实现细
	节。接下来的小节中我们会具体看到VFS抽象模型以及它提供的接口。
	
	
13.3 Unix文件系统

		Unix使用了四种和文件系统相关的传统抽象概念：文件、目录项、索引节点和安装点
	（mount point）。
	
		从本质上讲文件系统是特殊的数据分层存储结构，它包含文件、目录和相关的控制信息。文
	件系统的通用操作包含创建、删除和安装等。在Unix中，文件系统被安装在一个特定的安装点
	上，该安装点在全局层次结构中被称作命名空间，所有的已安装文件系统都作为根文件系统树
	的枝叶出现在系统中。与这种单一、统一的树形成鲜明对照的就是DOS和Windows的表现，它
	们将文件的命名空间分类为驱动字母，例如C:。这种将命名空间划分为设备和分区的做法，相
	当于把硬件细节“泄露”给文件系统抽象层。对用户而言，如此的描述有点随意，甚至产生混
	淆，这是Linux统一命名空间所不屑一顾的。
		
		文件其实可以做一个有序字节串，字节串中第一个字节是文件的头，最后一个字节是文件的
	尾。每一个文件为了便于系统和用户识别，都被分配了一个便于理解的名字。典型的文件操作有
	读、写、创建和删除等。Unix文件的概念与面向记录的文件系统（如OpenVMS的File-11）形
	成鲜明的对照。面向记录的文件系统提供更丰富、更结构化的表示，而简单的面向字节流抽象的
	Unix文件则以简单性和相当的灵活性为代价。
	
		文件通过目录组织起来。文件目录好比一个文件夹，用来容纳相关文件。因为目录也可以包
	含其他目录，即子目录，所以目录可以层层嵌套，形成文件路径。路径中的每一部分都被称作目
	录条目。“/home/wolfman/butter”是文件路径的一个例子————根目录/，目录hoem，wolfman和
	文件butter都是目录条目，它们统称为目录项。在Unix中，目录属于普通文件，它列出包含在
	其中的所有文件。由于VFS把目录当作文件对待，所以可以对目录执行和文件相同的操作。
	
		Unix系统将文件的相关信息和文件本身这两个概念加以区分，例如访问控制权限、大小、
	拥有者、创建时间等信息。文件相关信息，有时被称作文件的元数据（也就是说，文件的相关
	数据），被存储在一个单独的数据结构中，该结构被称为索引节点（inode），它其实是index node
	的缩写，不过近来术语“inode”使用得更为普遍一些。

		所有这些信息都和文件系统的控制信息密切相关，文件系统的控制信息存储在超级块中，超
	级块是一种包含文件系统信息的数据结构。有时，把这些收集起来的信息称为文件系统数据元，
	它集单独文件信息和文件系统的信息于一身。
	
		一直以来，Unix文件系统在它们物理磁盘布局中也是按照上述概念实现的。比如说在磁盘
	上，文件（目录也属于文件）信息按照索引节点形式存储在单独的块中；控制信息被集中存储
	在磁盘的超级块中，等等。Unix中文件的概念从物理上被映射到存储介质。Linux的VFS的设
	计目标就是要保证能与支持和实现了这些概念的文件系统协同工作。像如FAT或NTFS这样的
	非Unix风格的文件系统，虽然也可以在Linux上工作，但是它们必须经过封装，提供一个符合
	这些概念的界面。比如，即使一个文件系统不支持索引节点，它也必须在内存中装配索引节点结
	构体，就像它本身包含索引节点一样。再比如，如果一个文件系统将目录看做一种特殊对象，那
	么要想使用VFS，就必须将目录重新表示为文件形式。通常，这种转换需要在使用现场（on the
	fly）引入一些特殊处理，使得非Unix文件系统能够兼容Unix文件系统的使用规则并满足VFS
	的需求。这种文件系统当然仍能工作，但是其带来的开销则不可思议（开销太大了）。


13.4 VFS对象及其数据结构

		VFS其实采用的是面向对象的设计思路，使用一组数据结构来代表通用文件对象。这些数
	据结构类似于对象。因为内核纯粹使用C代码实现，没有直接利用面向对象的语言，所以内核
	中的数据结构都使用C语言的结构体实现，而这些结构体包含数据的同时也包含操作这些数据
	的函数指针，其中的操作函数由具体文件系统实现。
	
		VFS中有四个主要的对象类型，它们分别是：
		* 超级块对象，它代表一个具体的已安装文件系统。
		* 索引节点对象，它代表一个具体文件。
		* 目录项对象，它代表一个目录项，是路径的一个组成部分。
		* 文件对象，它代表由进程打开的文件。
		
		注意，因为VFS将目录作为一个文件来处理，所以不存在目录对象。回忆本章前面所提到
	的目录项代表的是路径中的一个组成部分，它可能包括一个普通文件。换句话说，目录项不同于
	目录，但目录却是另一种形式的文件，明白了吗？
	
		每个主要对象中都包含一个操作对象，这些操作对象描述了内核针对主要对象可以使用的
	方法：
		* super_operations对象，其中包括内核针对特定文件系统所能调用的方法，比如write_
		  inode()和sync_fs()等方法。
		* inode_operations对象，其中包括内核针对特定文件所能调用的方法，比如create()和link()
		  等方法。
		* dentry_operations对象，其中包括内核针对特定目录所能调用的方法，比如d_compare()和
		  d_delete()等方法。
		* file_operations对象，其中包括进程针对已打开文件所能调用的方法，比如read()和write()
		  等方法。
		  
		操作对象作为一个结构体指针来实现，此结构体中包含指向操作其父对象的函数指针。对于
	其中许多方法来说，可以继承使用VFS提供的通用函数，如果通用函数提供的基本功能无法满
	足需要，那么就必须使用实际文件系统的独有方法填充这些函数指针，使其指向文件系统实例。
	
		再次提醒，我们这里所说的对象就是指结构体，而不是像C++或Java那样的真正的对象数
	据类类型。但是这些结构体的确代表的是一个对象，它含有相关的数据和对这些数据的操作，所
	以可以说它们就是对象。
	
		VFS使用了大量结构体对象，它所包括的对象远远多于上面提到的这几种主要对象。比如
	每个注册的文件系统都由file_system_type结构体来表示，它描述了文件系统及其性能；另外，
	每一个安装点也都用vfsmount结构体表示，它包含的是安装点的相关信息，如位置和安装标
	志等。
	
		在本章的最后还要介绍两个与进程相关的结构体，它们描述了文件系统以及和进程相关的文
	件，分别是fs_struct结构体和file结构体。
	
		13.5节将讨论这些对象以及它们在VFS层的实现中扮演的角色。
		
		
13.5 超级块对象

		各种文件系统都必须实现超级块对象，该对象用于存储特定文件系统的信息，通常对应于存
	放在磁盘特定扇区中的文件系统超级块或文件系统控制块（所以称为超级块对象）。对于并非基
	于磁盘的文件系统（如基于内存的文件系统，比如sysfs），它们会在使用现场创建超级块并将其
	保存到内存中。
	
		超级块对象由super_block结构体表示，定义在文件<linux/fs.h>中，下面给出它的结构和各
	个域的描述：
	
		struct super_block {
			struct list_head				s_list;					//指向所有超级块的链表
			dev_t							s_dev;					//设备标识符
			unsigned long					s_blocksize;			//以字节为单位的块大小
			
			unsigned char					s_blocksize_bits;		//以位为单位的块大小
			unsigned char					s_dirt;					//修改（脏）标志
			unsigned long long				s_maxbytes;				//文件大小上限
			struct file_system_type			s_type;					//文件系统类型
			struct super_operations			s_op;					//超级块方法
			struct dquot_operations			*dq_op;					//磁盘限额方法
			struct quotactl_ops				*s_qcop;				//限额控制方法
			struct export_operations		*s_export_op			//导出方法
			unsigned long					s_flags;				//挂载标志
			unsigned long					s_magic;				//文件系统的幻数
			struct dentry 					*s_root;				//目录挂载点
			struct rw_semaphore				s_umount;				//卸载信号量
			struct semaphore				s_lock;					//超级块信号量
			int								s_count;				//超级块引用计数
			int								s_need_sync;			//尚未同步标志
			atomic_t						s_active;				//活动引用计数
			void							*s_security;			//安全模块
			struct xattr_handler			**s_xattr;				//扩展的属性操作
			struct list_head				s_inodes;				//inodes链表
			struct list_head				s_dirty;				//脏数据链表
			struct list_head				s_io;					//回写链表
			struct list_head				s_more_io;				//更多回写的链表
			struct hlist_head				s_anon;					//匿名目录项
			struct list_head				s_files;				//被分配文件链表
			struct list_head				s_dentry_lru;			//未被使用目录项链表
			int								s_nr_dentry_unused;		//链表中目录项的数目
			struct block_device				*s_bdev;				//相关的块设备
			
			struct mtd_info					*s_mtd;					//存储磁盘信息
			struct list_head				s_instances;			//该类型文件系统
			struct quota_info				s_dquot;				//限额相关选项
			int								s_frozen;				//frozen标志位
			wait_queue_head_t				s_wait_unfrozen;		//冻结的等待队列
			char							s_id[32];				//文本名字
			void							*s_fs_info;				//文件系统特殊信息
			fmode_t							s_mode;					//安装权限
			struct semaphore				s_vfs_rename_sem;		//重命名信号量
			u32								s_time_gran;			//时间戳粒度
			char							*s_subtype;				//子类型名称
			char							*s_options;				//已存安装选项
		};
		
		创建、管理和撤销超级块对象的代码位于文件fs/super.c中。超级块对象通过alloc_super()
	函数创建并初始化。在文件系统安装时，文件系统会调用该函数以便从磁盘读取文件系统超级
	块，并且将其信息填充到内存中的超级块对象中。
	
	
13.6 超级块操作

		超级块对象中最重要的一个域是s_op，它指向超级块的操作函数表。超级块操作函数表由
	super_operations结构体表示，定义在文件<linux/fs.h>中，其形式如下：
	
		struct super_operations {
			struct inode *(*alloc_inode)(struct super_block *sb);
			void (*destroy_inode)(struct inode *);
			void (*dirty_inode) (struct inode*);
			int (*write_inode) (struct inode *, int);
			void (*drop_inode) (struct inode *);
			void (*delete_inode) (struct inode *);
			void (*put_super) (struct super_block *);
			void (*write_super) (struct super_block *);
			int (*sync_fs)(struct super_block *sb, int wait);
			int (*freeze_fs)(struct super_block *);
			int (unfreeze_fs)(struct super_block *);
			int (*statfs) (struct dentry *, struct kstatfs *);
			int (*remount_fs) (struct super_block *, int *, char *);
			void (*clear_inode) (struct inode *);
			void (*umount_begin) (struct super_block *);
			int (*show_options)(struct seq_file *, struct vfsmount *);
			int (*show_stats)(struct seq_file *, struct vfsmount *);
			ssize_t (*quota_read)(struct super_block *, int, char *,size_t, loff_t);
			ssize_t (*quota_write)(struct super_block *, int, const char *, size_t, loff_t);
			int (*bdev_try_to_free_page)(struct super_block *, struct page*, gfp_t);
		};
		
		该结构体中的每一项都是一个指向超级块操作函数的指针，超级块操作函数执行文件系统和
	索引节点的低层操作。
	
		当文件系统需要对其超级块执行操作时，首先要在超级块对象中寻找需要的操作方法。比
	如，如果一个文件系统要写自己的超级块，需要调用：
		
		sb->s_op->write_super(sb);
		
		在这个调用中，sb是指向文件系统超级块的指针，沿着该指针进入超级块操作函数表s_op，
	并从表中取得希望得到的write_super()函数，该函数执行写入超级块的实际操作。注意，尽管
	write_super()方法来自超级块，但是在调用时，还是要把超级块作为参数传递给它，这是因为C
	语言中缺少对面向对象的支持，而在C++中，使用如下的调用就足够了：
		sb.write_super();
		
		由于在C语言中无法直接得到操作函数的父对象，所以必须将父对象以参数形式传给操作函数。
		下面给出super_operation中，超级块操作函数的用法。
		
		struct inode *alloc_inode(struct super_block *sb)
		
		在给定的超级块下创建和初始化一个新的索引节点对象。
		
		void destroy_inode(struct inode *inode)
		
		用于释放给定的索引节点。

		void dirty_inode(struct inode *inode)
		
		VFS在索引节点脏（被修改）时会调用此函数。日志文件系统（如ext3和ext4）执行该函
	数进行日志更新。
	
		void write_inode(struct inode *inode, int wait)
		
		用于将给定的索引节点写入磁盘。wait参数指明写操作是否需要同步。
		
		void drop_inode(struct inode *inode)
		
		在最后一个指向索引节点的引用被释放后，VFS会调用该函数。VFS只需要简单地删除这
	个索引节点后，普通Unix文件系统就不会定义这个函数了。
	
		void delete_inode(struct inode *inode)
		
		用于从磁盘上删除给定的索引节点。
		
		void put_super(struct super_block *sb)
		
		在卸载文件系统时由VFS调用，用来释放超级块。调用者必须一直持有s_lock锁。
		
		void write_super(struct super_block *sb)
		
		用给定的超级块更新磁盘上的超级块。VFS通过该函数对内存中的超级块和磁盘中的超级
	块进行同步。调用者必须一直持有s_lock锁。
	
		int sync_fs(struct super_block *sb, int wait)
		
		使文件系统的数据元与磁盘上的文件系统同步。wait参数指定操作是否同步。
		
		void write_super_lockfs(struct super_block *sb)
		
		首先禁止对文件系统做改变，再作用给定的超级块更新磁盘上的超级块。目前LVM（逻辑卷
	标管理）会调用该函数。
	
		void unlockfs(struct super_block *sb)
		
		对文件系统解除锁定，它是write_super_lockfs()的逆操作。
		
		int statfs(struct super_block *sb, struct statfs *statfs)
		
		VFS通过调用该函数获取文件系统状态，指定文件系统相关的统计信息将放置在statfs中。
		
		int remount_fs(struct super_block *sb, int *flags, char *data)
		
		当指定新的安装选项重新安装文件系统时，VFS会调用该函数。调用者必须一直持有s_
	lock锁。
	
		void clear_inode(struct inode *inode)
		
		VFS调用该函数释放索引节点，并清空包含相关数据的所有页面。
		
		void umount_begin(struct super_block *sb)
		
		VFS调用该函数中断安装操作。该函数被网络文件系统使用，如NFS。
		
		所有以上函数都是由VFS在进程上下文中调用。除了dirty_inode()，其他函数在必要时都可
	以阻塞。
	
		这其中的一些函数是可选的。在超级块操作表中，文件系统可以将不需要的函数指针设置成
	NULL。如果VFS发现操作函数指针是NULL，那它要么就会调用通用函数执行相应操作，要么
	什么也不做，如何选择取决于具体操作。
	
	
13.7 索引节点对象

		索引节点对象包含了内核在操作文件或目录时需要的全部信息。对于Unix风格的文件系统
	来说，这些信息可以从磁盘索引节点直接读入。如果一个文件系统没有索引节点，那么，不管这
	些相关信息在磁盘上是怎么存放的，文件系统都必须从中提取这些信息。没有索引节点的文件系
	统通常将文件的描述信息作为文件的一部分来存放。这些文件系统与Unix风格的文件系统不同，
	没有将数据与控制信息分开存放。有些现代文件系统使用数据库来存储文件的数据。不管哪种情
	况、采用哪种方式，索引节点对象必须在内存中创建，以便于文件系统使用。
	
		索引节点对象由inode结构体表示，它定义在文件<linux/fs.h>中，下面给出它的结构和
	各项的描述：
	
		struct inode {
				struct hlist_node			i_hash;				//散列表
				struct list_head			i_list;				//索引节点链表
				struct list_head			i_sb_list;			//超级块链表
				struct list_head			i_dentry;			//目录项链表
				unsigned long				i_ino;				//节点号
				
				atomic_t					i_count;			//引用计数
				unsigned int				i_nlink;			//硬链接数
				uid_t						i_uid;				//使用者的id
				gid_t						i_gid;				//使用组的id
				kdev_t						i_rdev;				//实际设备标识符
				u64							i_version;			//版本号
				loff_t						i_size;				//以字节为单位的文件大小
				seqcount_t					i_size_seqcount;	//对i_size进行串行计数
				struct timespec				i_atime;			//最后访问时间
				struct timespec				i_mtime;			//最后修改时间
				struct timespec				i_ctime;			//最后改变时间
				unsigned int				i_blkbits;			//以位为单位的块大小
				blkcnt_t					i_blocks;			//文件的块数
				unsigned short				i_bytes;			//使用的字节数
				umode_t						i_mode;				//访问权限
				spinlock_t					i_lock;				//自旋锁
				struct rw_semaphore			i_alloc_sem;		//嵌入i_sem内部
				struct semaphore			i_sem;				//索引节点信号量
				struct inode_operations		*i_op;				//索引节点操作表
				struct file_operations		*i_fop;				//缺省的索引节点操作
				struct super_block			*i_sb;				//相关的超级块
				struct file_lock			*i_flock;			//文件锁链表
				struct address_space		*i_mapping;			//相关的地址映射
				struct address_space		i_data;				//设备地址映射
				struct dquot				*i_dquot[MAXQUOTAS];//索引节点的磁盘限额
				struct list_head			i_devices;			//块设备链表
				union {
					struct pipe_inode_info	*i_pipe;			//管道信息
					struct block_device		*i_bdev;			//块设备驱动
					struct cdev				*i_cdev;			//字符设备驱动
				};
				unsigned long				i_dnotify_mask;		//目录通知掩码
				struct dnotify_struct		*i_dnotify;			//目录通知
				struct list_head			inotify_watches;	//索引节点通知监测链表
				struct mutex				inotify_mutex;		//保护inotify_watches
				unsigned long				i_state;			//状态标志
				unsigned long				dirtied_when;		//第一次弄脏数据的时间
				unsigned int				i_flags;			//文件系统标志
				atomic_t					i_writecount;		//写者计数
				void						*i_security;		//安全模块
				void						*i_private;			//fs私有指针
		};
		
		一个索引节点代表文件系统中（但是索引节点仅当文件被访问时，才在内存中创建）的一个
	文件，它也可以是设备或管道这样的特殊文件。因此索引节点结构体中有一些和特殊文件相关的
	项，比如i_pipe项就指向一个代表有名管道的数据结构，i_bdev指向块设备结构体，i_cdev指向
	字符设备结构体。这三个指针被存放在一个公用体中，因为一个给定的索引节点每次只能表示三
	者之一（或三者均不）。
	
		有时，某些文件系统可能并不能完整地包含索引节点结构体要求的所有信息。举个例子，有
	的文件系统可能并不记录文件的访问时间，这时，该文件系统就可以在实现中选择任意合适的办
	法来解决这个问题。它可以在i_atime中存储0，或者让i_atime等于i_mtime，或者只在内存中
	更新i_atime而不将其写回磁盘，或者由文件系统的实现者来决定。
	
	
13.8 索引节点操作

		和超级块操作一样，索引节点对象中的inode_operations项也非常重要，因为它描述了VFS
	用以操作索引节点对象的所有方法，这些方法由文件系统实现。与超级块类似，对索引节点的操
	作调用方式如下：
		
		i->i_op->truncate(i)
		
		i指向给定的索引节点，truncate()函数是由索引节点i所在的文件系统定义的。inode_
	operations结构体定义在文件<linux/fs.h>中：
	
		struct inode_operations {
			int (*create) (struct inode *, struct dentry *, int, struct nameidata *);
			struct dentry * (*lookup) (struct inode *, struct dentry *, struct nameidata *);
			int (*link) (struct dentry *, struct inode *, struct dentry *);
			int (*unlink) (struct inode *, struct dentry *);
			int (*symlink) (struct inode *, struct dentry *, const char *);
			int (*mkdir) (struct inode *, struct dentry *, int);
			int (*rmdir) (struct inode *, struct dentry *);
			int (*mknod) (struct inode *, struct dentry *, int, dev_t);
			int (*rename) (struct inode *, struct dentry *,
						   struct inode *, struct dentry *);
			int (*readlink) (struct dentry *, char __user *, int);
			void * (*follow_link) (struct dentry *, struct nameidata *);
			void (*put_link) (struct dentry *, struct nameidata *, void *);
			void (*truncate) (struct inode *);
			int (*permission) (struct inode *, int);
			int (*setattr) (struct dentry *, struct iattr *);
			int (*getattr) (struct vfsmount *mnt, struct dentry *, struct kstat *);
			int (*setxattr) (struct dentry *, const char *, const void *, size_t, int);
			ssize_t (*getxattr) (struct dentry *, const char *, void *, size_t);
			ssize_t (*listxattr) (struct dentry *, char *, size_t);
			int (*removexattr) (struct dentry *, const char *);
			void (*truncate_range)(struct inode *, loff_t, loff_t);
			long (*fallocate)(struct inode *inode, int mode, loff_t offset,
								loff_t len);
			int (*fiemap)(struct inode *, struct fiemap_extent_info *, u64 start,
							u64 len);
		};

		下面这些接口由各种函数组成，在给定的节点上，可能由VFS执行这些函数，也可能由具
	体的文件系统执行：
	
		int create(struct inode *dir, struct dentry *dentry, int mode)
		
		VFS通过系统调用create()和open()来调用该函数，从而为dentry对象创建一个新的索引节
	点。在创建时使用mode指定的初始模式。
	
		struct dentry * lookup(struct inode *dir, struct dentry *dentry)
		
		该函数在特定目录中寻找索引节点，该索引节点要对应于dentry中给出的文件名。
		
		int link(struct dentry *old_dentry,
				 struct inode *dir,
				 struct dentry *dentry)
				 
		该函数被系统调用link()调用，用来创建硬连接。硬连接名称由dentry参数指定，连接对象
	是dir目录中old_dentry目录项所代表的文件。
		
		int unlink(struct inode *dir, struct dentry *dentry)
		
		该函数被系统调用unlink()调用，从目录dir中删除由目录项dentry指定的索引节点对象。
		
		int symlink(struct inode *dir,
					struct dentry *dentry,
					const char *symname)
					
		该函数被系统调用symlik()调用，创建符号连接。该符号连接名称由symname指定，连接
	对象是dir目录中的dentry目录项。
		
		int mkdir(struct inode *dir,
				  struct dentry *dentry,
				  int mode)
				  
		该函数被系统调用mkdir()调用，创建一个新目录。创建时使用mode指定的初始模式。
		
		int rmdir(struct inode *dir,
				  struct dentry *dentry)
		
		该函数被系统调用rmdir()调用，删除dir目录中的dentry目录项代表的文件。
		
		int mknod(struct inode *dir,
				  struct dentry *dentry,
				  int mode, dev_t rdev)
				  
		该函数被系统调用mknod()调用，创建特殊文件（设备文件、命名管道或套接字）。要创建
	的文件放在dir目录中，其目录项为dentry，关联的设备为rdev，初始权限由mode指定。
	
		int rename(struct inode *old_dir,
				   struct dentry *old_dentry,
				   struct inode *new_dir,
				   struct dentry *new_dentry)
				   
		VFS调用该函数来移动文件。文件源路径在old_dir目录中，源文件由old_dentry目录项指
	定，目标路径在new_dir目录中，目标文件由new_dentry指定。
	
		int readlink(struct dentry *dentry,
					 char * buffer, int buflen)
		
		该函数被系统调用readlink()调用，拷贝数据到特定的缓冲buffer中。拷贝的数据来自
	dentry指定的符号连接，拷贝大小最大可达buflen字节。
	
		int follow_link(struct dentry *dentry,
						struct nameidata *nd)
						
		该函数由VFS调用，从一个符号连接查找它指向的索引节点。由dentry指向的连接被解析，
	其结果存放在由nd指向的nameidata结构体中。
		
		int put_link(struct dentry *dentry,
					 struct nameidata *nd)
					 
		在follow_link()调用之后，该函数由VFS调用进行清除工作。
		
		void truncate(struct inode *inode)
		
		该函数由VFS调用，修改文件的大小。在调用前，索引节点的i_size项必须设置为预期的大小。
		
		int permission(struct inode *inode, int mask)
		
		该函数用来检查给定的inode所代表的文件是否允许特定的访问模式。如果允许特定的访
	问模式，返回零，否则返回负值的错误码。多数文件系统都将此区域设置为NULL，使用VFS
	提供的通用方法进行检查。这种检查操作仅仅比较索引节点对象中的访问模式位是否和给定的
	mask一致。比较复杂的系统（比如支持访问控制链（ACLS）的文件系统），需要使用特殊的
	permission()方法。
	
		int setattr(struct dentry *dentry
					struct iattr *attr)
		
		该函数被notify_change()调用，在修改索引节点后，通知发生了“改变事件”。
		
		int getattr(struct vfsmount *mnt,
					struct dentry *dentry,
					struct kstat *stat)
					
		在通知索引节点需要从磁盘中更新时，VFS会调用该函数。
		
		扩展属性允许key/value这样的一对值与文件相关联。
		
		int setxattr(struct dentry *dentry,
					 const char *name,
					 const void *value,
					 size_t size,int flags)
		
		该函数由VFS调用，给dentry指定的文件设置扩展属性。属性名为name,值为value。
		
		ssize_t getxattr(struct dentry *dentry,
						 const char *name,
						 void *value, size_t size)
						 
		该函数由VFS调用，向value中拷贝给定文件的扩展属性name对应的数值。
		
		ssize_t listxattr(struct dentry *dentry,
						  char *list, size_t size)
						  
		该函数将特定文件的所有属性列表拷贝到一个缓冲列表中。
		
		该函数从给定文件中删除指定的属性。
		

13.9 目录项对象

		VFS把目录当作文件对待，所以在路径/bin/vi中，bin和vi都属于文件————bin是特殊的目录文
	件而vi是一个普通文件，路径中的每个组成部分都由一个索引节点对象表示。虽然它们可以统一由
	索引节点表示，但是VFS经常需要执行目录相关的操作，比如路径名查找等。路径名查找需要解析
	路径中的每一个组成部分，不但要确保它有效，而且还需要再进一步寻找路径中的下一个部分。
	
		为了方便查找操作，VFS引入了目录项的概念。每个dentry代表路径中的一个特定部分。
	对前一个例子来说，/、bin和vi都属于目录项对象。前两个是目录，最后一个是普通文件。必须
	明确一点：在路径中（包括普通文件在内），每一个部分都是目录项对象。解析一个路径并遍历
	其分量绝非简单的演练，它是耗时的、常规的字符串比较过程，执行耗时、代码繁琐。目录项对
	象的引入使得这个过程更加简单。
	
		目录项也可包括安装点。在路径/mnt/cdrom/foo中，构成元素/、mnt、cdrom和foo都属于
	目录项对象。VFS在执行目录操作时（如果需要的话）会现场创建目录项对象。
	
		目录项对象由dentry结构体表示，定义在文件<linux/dcache.h>中。下面给出该结构体和其
	中各项的描述：
		struct dentry {
				atomic_t				d_count;			//使用计数
				unsigned int			d_flags;			//目录项标识
				spinlock_t				d_lock;				//单目录项锁
				int						d_mounted;			//是登陆点的目录项吗？
				struct inode			*d_inode;			//相关联的索引节点
				struct hlist_node		d_hash;				//散列表
				struct dentry			*d_parent;			//父目录的目录对象
				struct qstr				d_name;				//目录项名称
				struct list_head		d_lru;				//未使用的链表
				union {
					struct list_head	d_child;			//目录项内部形成的链表
					struct rcu_head		d_rcu;				//RCU加锁
				} d_u;
				struct list_head		d_subdirs;			//子目录链表
				struct list_head		d_alias;			//索引节点别名链表
				unsigned long			d_time;				//重置时间
				struct dentry_operations *d_op;				//目录项操作指针
				struct super_block		*d_sb;				//文件的超级块
				void					*d_fsdata;			//文件系统特有数据
				unsigned char			d_iname[DNAME_INLINE_LEN_MIN]	//短文件名
		};
		
		与前面两个对象不同，目录项对象没有对应的磁盘数据结构，VFS根据字符串形式的路
	径名现场创建它。而且由于目录项对象并非真正保存在磁盘上，所以目录项结构体没有是否被修
	改的标志（也就是是否为脏、是否需要写回磁盘的标志）。
	

13.9.1 目录项状态

		目录项对象有三种有效状态：被使用、未被使用和负状态。
		
		一个被使用的目录项对应一个有效的索引节点（即d_inode指向相应的索引节点）并且表明
	该对象存在一个或多个使用者（即d_count为正值）。一个目录项处于被使用状态，意味着它正
	被VFS使用并且指向有效的数据，因此不能被丢弃。
	
		一个未被使用的目录项对应一个有效的牵引节点（d_inode指向一个索引节点），但是应指
	明VFS当前并未使用它（d_count为0）。该目录项对象仍然指向一个有效对象，而且被保留在缓
	存中以便需要时再使用它。由于该目录项不会过早地被撤销，所以以后再需要它时，不必重新创
	建，与未缓存的目录项相比，这样使路径查找更迅速。但如果要回收内存的话，可以撤销未使用
	的目录项。
		
		一个负状态的目录项没有对应的有效索引节点（d_inode为NULL），因为牵引节点已被删
	除了，或路径不再正确了，但是目录项仍然保留，以便快速解析以后的路径查询。比如，一个守
	护进程不断地去试图打开并读取一个不存在的配置文件。open()系统调用不断地返回ENOENT，
	直到内核构建了这个路径、遍历磁盘上的目录结构体并检查这个文件的确不存在为止。即便这
	个失败的查找很浪费资源，但是将负状态缓存起来还是非常值得的。虽然负状态的目录项有些用
	处，但是如果有需要，可以撤销它，因为毕竟实际上很少用到它。
	
		目录项对象释放后也可以保存到slab对象缓存中去，这点在第12章讨论过。此时，任何
	VFS或文件系统代码都没有指向该目录项对象的有效引用。
	
	
13.9.2 目录项缓存

		如果VFS层遍历路径名中所有的元素并将它们逐个地解析成目录项对象，还要到达最深层
	目录，将是一件非常费力的工作，会浪费大量的时间。所以内核将目录项对象缓存在目录项缓存
	（简称dcache）中。
	
		目录项缓存包括三个主要部分：
		* “被使用的”目录项链表。该链表通过索引节点对象中的i_dentry项连接相关的索引节点，
		  因为一个给定的索引节点可能有多个链接，所以就可能有多个目录项对象，因此用一个链
		  表来连接它们。
		* “最近被使用的”双向链表。该链表含有未被使用的和负状态的目录项对象。由于该链总
		  是在头部插入目录项，所以链头节点的数据总比链尾的数据要新。当内核必须通过删除节
		  点项回收内存时，会从链尾删除节点项，因为尾部的节点最旧，所以它们在近期内再次被
		  使用的可能性最小。
		* 散列表和相应的散列函数用来快速地将给定路径解析为相关目录项对象。
		
		散列表由数组dentry_hashtable表示，其中每一个元素都是一个指向具有相同键值的目录项
	对象链表的指针。数组的大小取决于系统中物理内存的大小。
	
		实际的散列值由d_hash()函数计算，它是内核提供给文件系统的唯一的一个散列函数。
		
		查找散列表要通过d_lookup()函数，如果该函数在dcache中发现了与其相匹配的目录项对
	象，则匹配的对象被返回；否则，返回NULL指针。
	
		举例说明，假设你需要在自己目录中编译一个源文件，/home/dracula/src/the_sun_sucks.c，
	每一次对文件进行访问（比如说，首先要打开它，然后要存储它，还要进行编译等），VFS都必
	须沿着嵌套的目录依次解析全部路径：/、home、dracula、src和最终的the_sun_sucks.c。为了避
	免每次访问该路径名都进行这种耗时的操作，VFS会先在目录项缓存中搜索路径名，如果找到了，
	就无须花费那么大的力气了。相反，如果该目录项在目录项缓存中并不存在，VFS就必须自己通
	过遍历文件系统为每个路径分量解析路径，解析完毕后，再将目录项对象加入dcache中，以便
	以后可以快速查找到它。
	
		而dcache在一定意义上也提供对索引节点的缓存，也就是icache。和目录项对象相关的索
	引节点对象不会被释放，因为目录项会让相关索引节点的使用计数为正，这样就可以确保索引节
	点留在内存中。只要目录项被缓存，其相应的索引节点也就被缓存了。所以像前面的例子，只要
	路径名在缓存中找到了，那么相应的索引节点肯定也在内存中缓存着。
	
		因为文件访问呈现空间和时间的局部性，所以对目录项和索引节点进行缓存非常有益。文
	件访问有时间上的局部性，是因为程序可能会一次又一次地访问相同的文件。因此，当一个文件
	被访问时，所缓存的相关目录项和索引节点不久被命中的概率较高。文件访问具有空间的局部性
	是因为程序可能在同一个目录下访问多个文件，因此一个文件对应的目录项缓存后极有可能被命
	中，因为相关的文件可能在下次又被使用。
	

13.10 目录项操作

		dentry_operations结构体指明了VFS操作目录项的所有方法。
        
        该结构定义在文件<linux/dcache.h>中。
        
        struct dentry_operations {
            int (*d_revalidate) (struct dentry *, struct nameidata *);
            int (*d_hash) (struct dentry *, struct qstr *);
            int (*d_compare) (struct dentry *, struct qstr *, struct qstr *);
            int (*d_delete) (struct dentry *);
            void (*d_release) (struct dentry *);
            void (*d_input) (struct dentry *, struct inode *);
            char *(*d_dname) (struct dentry *, char *, int);
        };
        
        下面给出函数的具体用法：
        
        int d_revalidate(struct dentry *dentry,
                         struct nameidata*);
                         
        该函数判断目录对象是否有效。VFS准备从dcache中使用一个目录项时，会调用该函数。
    大部分文件系统将该方法置NULL，因为它们认为dcache中的目录项对象总是有效的。
    
        int_d_hash(struct dentry *dentry, struct qstr *name)
        
        该函数为目录项生成散列值，当目录项需要加入到散列表中时，VFS调用该函数。
        
        int d_compare(struct dentry *dentry, struct qstr *name1, struct qstr *name2)
        
        VFS调用该函数来比较name1和name2这两个文件名，多数文件系统使用VFS默认的操
    作，仅仅作字符串比较。对有些文件系统，比如FAT，简单的字符串比较不能满足其需要。因为
    FAT文件系统不区分大小写，所以需要实现一种不区分大小写的字符串比较函数。注意使用该函
    数时需要加dcache_lock锁。
    
        int d_delete(struct dentry *dentry)
        
        当目录项对象的d_count计数值等于0时，VFS调用该函数。注意使用该函数需要加dcache_
    lock锁和目录项的d_lock。
    
        void d_release(struct dentry *dentry)
        
        当目录项对象将要被释放时，VFS调用该函数，默认情况下，它什么也不做。
        
        void d_iput(struct dentry *dentry, struct inode *inode)
        
        当一个目录项对象丢失了其相关的索引节点时（也就是说磁盘索引节点被删除），VFS调
    用该函数。默认情况下VFS会调用iput()函数释放索引节点。如果文件系统重载了该函数，那么
    除了执行此文件系统特殊的工作外，还必须调用iput()函数。

    
13.11 文件对象

        VFS的最后一个主要对象是文件对象。文件对象表示进程已打开的文件。如果我们站在用
    户角度来看待VFS，文件对象会首先进入我们的视野。进程直接处理的是文件，而不是超级块、
    索引节点或目录项。所以不必奇怪：文件对象包含我们非常熟悉的信息（如访问模式，当前偏移
    等），同样道理，文件操作和我们非常熟悉的系统调用read()和write()等也很类似。
    
        文件对象是已打开的文件在内存中的表示。该对象（不是物理文件）由相应的open()系统
    调用创建，由close()系统调用撤销，所有这些文件相关的调用实际上都是文件操作表中定义的
    方法。因为多个进程可以同时打开和操作同一个文件，所以同一个文件也可能存在多个对应的文
    件对象。文件对象仅仅在进程观点上代表已打开的实际文件。虽然一个文件对应的文件对象不是唯
    一的，但对应的索引节点和目录项对象无疑是唯一的。
    
        文件对象由file结构体表示，定义在文件<linux/fs.h>中，下面给出该结构体和各项的描述。
        
        struct file {
                union {
                    struct list_head            fu_list;        //文件对象链表
                    struct rcu_head             fu_rcuhead;     //释放之后的RCU链表
                }f_u;
                struct path                     f_path;         //包含目录项
                struct file_operations          *f_op;          //文件操作表
                spinlock_t                      f_lock;         //单个文件结构锁
                atomic                          f_count;        //文件对象的使用计数
                unsigned int                    f_flags;        //当打开文件时所指定的标志
                mode_t                          f_mode;         //文件的访问模式
                loff_t                          f_pos;          //文件当前的位移量（文件指针）
                struct fown_struct              f_owner;        //拥有者通过信号量进行异步I/O数据的传送
                const struct cred               *f_cred;        //文件的信任状
                struct file_ra_state            f_ra;           //预读状态
                u64                             f_version;      //版本号
                void                            *f_security;    //安全模块
                void                    *private_data;          //tty设备驱动钩子
                struct list_head        f_ep_links;             //事件池链表
                spinlock_t              f_ep_lock;              //事件池锁
                struct address_space    *f_mapping;             //页缓存映射
                unsigned long           f_mnt_write_state;      //调试状态
        };
        
        类似于目录项对象，文件对象实际上没有对应的磁盘数据。所以在结构体中没有代表其对象
    是否为脏、是否需要写回磁盘的标志。文件对象通过f_dentry指针指向相关的目录项对象。目录
    项会指向相关的索引节点，索引节点会记录文件是否是脏的。
    
    
13.12 文件操作

        和VFS的其他对象一样，文件操作表在文件对象中也非常重要。跟file结构体相关的操作
    与系统调用很类似，这些操作是标准Unix系统调用的基础。
    
        文件对象的操作由file_operations结构体表示，定义在文件<linux/fs.h>中：
        struct file_operations {
                struct module *owner;
                loff_t (*llseek) (struct file *, loff_t, int);
                ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);
                ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);
                ssize_t (*aio_read) (struct kiocb *, const struct iovec *,
                                     unsigned long, loff_t);
                ssize_t (*aio_write) (struct kiocb *, const struct iovec *,
                                      unsigned long, loff_t);
                int (*readdir) (struct file *, void *, filldir_t);
                unsigned int (*poll) (struct file *, struct poll_table_struct *);
                int (*ioctl) (struct inode *, struct file *, unsigned int,
                                unsigned long);
                long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long);
                long (*compat_ioctl) (struct file *, unsigned int, unsigned long);
                int (*mmap) (struct file *, struct vm_area_struct *);
                int (*open) (struct inode *, struct file *);
                int (*flush) (struct file *, fl_owner_t id);
                int (*release) (struct inode *, struct file *);
                int (*fsync) (struct file *, struct dentry *, int datasync);
                int (*aio_fsync) (struct kiocb *, int datasync);
                int (*fasync) (int, struct file *, int);
                int (*lock) (struct file *, int, struct file_lock *);
                ssize_t (*sendpage) (struct file *, struct page *,
                                     int, size_t, loff_t *, int);
                unsigned long (*get_unmapped_area) (struct file *,
                                                    unsigned long,
                                                    unsigned long,
                                                    unsigned long,
                                                    unsigned long);
                int (*check_flags) (int);
                int (*flock) (struct file *, int, struct file_lock *);
                ssize_t (*splice_write) (struct pipe_inode_info *,
                                         struct file *,
                                         loff_t *,
                                         size_t,
                                         unsigned int);
                ssize_t (*splice_read) (struct file *,
                                        loff_t *,
                                        struct pipe_inode_info *,
                                        size_t,
                                        unsigned int);
                int (*setlease) (struct file *, long, struct file_lock **);
        };
        
        具体的文件系统可以为每一种操作做专门的实现，或者如果存在通用操作，也可以使用通用
    操作。一般在基于Unix的文件系统上，这些通用操作效果都不错。并不要求实际实际文件系统实现
    文件操作函数表中的所有方法————虽然不实现最基础的那些操作显然是很不明智的，对不感兴趣
    的操作完全可以简单地将该函数指针置为NULL。
    
        下面给出操作的用法说明：
        loff_t lleek(struct file *file,
                     loff_t offset, int origin)
                     
        该函数用于更新偏移量指针，由系统调用lleek()调用它。
        
        ssize_t read(struct file *file,
                     char *buf, size_t count,
                     loff_t *offset)
                     
        该函数从给定文件的offset偏移处读取count字节的数据到buf中，同时更新文件指针。由
    系统调用read()调用它。
    
        ssize_t aio_read(struct kiocb *iocb,
                         char *buf, size_t count,
                         loff_t offset)
        
        该函数从iocb描述的文件里，以同步方式读取count字节的数据到buf中。由系统调用aio_
    read()调用它。
    
        ssize_t write(struct file *file,
                      const char *buf, size_t count,
                      loff_t *offset)
        
        该函数从给定的buf中取出count字节的数据，写入给定文件的offset偏移处，同时更新文
    件指针。由系统调用write()调用它。
    
        ssize_t aio_write(struct kiocb *iocb,
                          const char *buf,
                          size_t count, loff_t offset)
                          
        该函数以同步方式从给定的buf中取出count字节的数据，写入由iocb描述的文件中。由系
    统调用aio_write()调用它。
    
        int readdir(struct file *file, void *dirent, filldir_t filldir)
        
        该函数返回目录列表中的下一个目录。由系统调用readdir()调用它。
        
        unsigned int poll(struct file *file,
                          struct poll_table_struct *poll_table)
                          
        该函数睡眠等待给定文件活动。由系统调用poll()调用它。
        
        int ioctl(struct inode *inode,
                  struct file *file,
                  unsigned int cmd,
                  unsigned long arg)
                  
        该函数用来给设备发送命令参数对。当文件是一个被打开的设备节点时，可以通过它进行设
    置操作。由系统调用ioctl()调用它。调用者必须持有BKL。
    
        int unlocked_ioctl(struct file *file,
                           unsigned int cmd,
                           unsigned long arg)
                           
        其实现与ioctl()有类似的功能，只不过不需要调用者持有BKL。如果用户空间调用ioctl()
    系统调用，VFS便可以调用unlocked_ioctl()（凡是ioctl()出现的场所）。因此文件系统只需要实
    现其中的一个，一般优先实现unlocked_ioctl()。
    
        int compat_ioctl(struct file *file,
                         unsigned int cmd,
                         unsigned long arg)
        
        该函数是ioctl()函数的可移植变种，被32位应用程序在64位系统上。这个函数被设
    计成即使在64位的体系结构上对32位也是安全的，它可以进行必要的字大小转换。新的驱动
    程序应该设计自己的ioctl命令以便所有的驱动程序都是可移植的，从而使得compat_ioctl()和
    unlocked_ioctl()指向同一个函数。像compat_ioctl()和unlocked_ioctl()一样都不必持有BKL。
    
        int mmap(struct file *file, struct vm_area_struct *vma)
        
        该函数将给定的文件映射到指定的地址空间上。由系统调用mmap()调用它。
        
        int open(struct inode *inode,
                 struct file *file)
                 
        该函数创建一个新的文件对象，并将它和相应的索引节点对象关联起来。由系统调用open()
    调用它。
    
        int flush(struct file *file)
        
        当已打开文件的引用计数减少时，该函数被VFS调用。它的作用根据具体文件系统而定。
        
        int release(struct inode *inode,
                    struct file *file)
                    
        当文件的最后一个引用被注销时（比如，当最后一个共享文件描述符的进程调用了close()
    或退出时），该函数会被VFS调用。它的作用根据具体文件系统而定。
    
        int fsync(struct file *file,
                  struct dentry *dentry,
                  int datasync)
        
        将给定文件的所有被缓存数据写回磁盘。由系统调用fsync()调用它。
        
        int aio_fsync(struct kiocb *iocb,
                      int datasync)
                      
        将iocb描述的文件的所有被缓存数据写回到磁盘。由系统调用aio_fsync()调用它。
        
        int fasync(int fd, struct file *file, int on)
        
        该函数用于打开或关闭异步I/O的通告信号。
        
        int lock(struct file *file, int cmd, struct file_lock *lock)
        
        该函数用于给指定文件上锁。
        
        ssize_t readv(struct file *file,
                      const struct iovec *vector,
                      unsigned long count,
                      loff_t *offset)
                      
        该函数从给定文件中读取数据，并将其写入由vector描述的count个缓冲中去，同时增加文件
    的偏移量。由系统调用readv()调用它。
    
        ssize_t writev(struct file *file,
                       const struct iovec *vector,
                       unsigned long count,
                       loff_t *offset)
                       
        该函数将由vector描述的count个缓冲中的数据写入到由file指定的文件中去，同时减小文
    件的偏移量。由系统调用writev()调用它。
    
        ssize_t sendfile(struct file *file,
                         loff_t *offset, 
                         size_t size,
                         read_actor_t actor,
                         void *target)
                         
        该函数用于从一个文件拷贝数据到另一个文件中，它执行的拷贝操作完全在内核中完成，避
    免了向用户空间进行不必要的拷贝。由系统调用sendfile()调用它。
    
        ssize_t sendpage(struct file *file,
                         struct page *page,
                         int offset, size_t size,
                         loff_t *pos, int more)
                         
        该函数用来从一个文件向另一个文件发送数据。
        
        unsigned long get_unmapped_area(struct file *file,
                                        unsigned long addr,
                                        unsigned long len,
                                        unsigned long offset,
                                        unsigned long flags)
                                        
        该函数用于获取未使用的地址空间来映射给定的文件。
        
        int check_flags(int flags)
        
        当给出SETFL命令时，这个函数用来检查传递给fcntl()系统调用的flags有效性。与大
    多数VFS操作一样，文件系统不必实现check_flags()————目前，只有在NFS文件系统上实现了。
    这个函数能使文件系统限制无效的SETFL标志，不进行限制的话，普通的fcntl()函数能使标志
    生效。在NFS文件系统中，不允许把O_APPEND和O_DIRECT相结合。

        int flock(struct file *filp,
                  int cmd,
                  struct file_lock *fl)
                  
        这个函数用来实现flock()系统调用，该调用提供忠告锁。
        
    如此之多的ioctls
    
        不久之前，只有一个单独的ioctl方法。如今，有三个相关的方法。unlocked_ioctl()和
    ioctl相同，不过前者在无大内核锁（BKL）情况下被调用。因此函数的作者必须确保适当的
    同步。因为大内核锁是粗粒度、低效的锁，驱动程序应当实现unlocked_ioctl()而不是ioctl()。
    
        compat_ioctl()也在无大内核锁的情况下被调用，但是它的目的是为64位的系统提供32
    位ioctl的兼容方法。至于你如何实现它取决于现有的ioctl命令。早期的驱动程序隐含有确
    定大小的类型（如long），应该实现适用于32位应用的compat_ioctl()方法。这通常意味着把
    32位值转换为64位内核中合适的类型。新驱动程序重新设计ioctl命令，应该确保所有的参
    数和数据都有明确大小的数据类型，在32位系统上运行32位应用是安全的，在64位系统上
    运行32位应用也是安全的，在64位系统上运行64位应用更是安全的。这些驱动程序可以让
    compat_ioctl()函数指针和unlocked_ioctl()函数指针指向同一函数。


13.13 和文件系统相关的数据结构

        除了以上几种VFS基础对象外，内核还使用了另外一些标准数据结构来管理文件系统的其
    他相关数据。第一个对象是file_system_type，用来描述各种特定文件系统类型，比如ext3、ext4
    或UDF。第二个结构体是vfsmount，用来描述一个安装文件系统的实例。
    
        因为Linux支持众多不同的文件系统，所以内核必须由一个特殊的结构来描述每种文件系统
    的功能和行为。file_system_type结构体被定义在<linux/fs.h>中，具体实现如下：
    
        struct file_system_type {
                const char                  *name;       // 文件系统的名字
                int                         fs_flags;    // 文件系统类型标志
                
                /* 下面的函数用来从磁盘中读取超级块 */
                struct super_block          *(*get_sb)  (struct file_system_type *, int,
                                                         char *, void *);
                
                /* 下面的函数用来终止访问超级块 */
                void                        (*kill_sb)  (struct super_block *);
                
                struct module               *owner;     // 文件系统模块
                struct file_system_type     *next;      // 链表中下一个文件系统类型
                struct list_head            fs_supers;  // 超级块对象链表
                
                /* 剩下的几个字段运行时使锁生效 */
                struct lock_class_key       s_lock_key;
                struct lock_class_key       s_umount_key;
                struct lock_class_key       i_lock_key;
                struct lock_class_key       i_mutex_key;
                struct lock_class_key       i_mutex_dir_key;
                struct lock_class_key       i_alloc_sem_key;
        };
        
        get_sb()函数从磁盘上读取超级块，并且在文件系统被安装时，在内存中组装超级块对象。
    剩余的函数描述文件系统的属性。
        
        每种文件系统，不管有多少个实例安装到系统中，还是根本就没有安装到系统中，都只有一
    个file_system_type结构。
    
        更有趣的事情是，当文件系统被实际安装时，将有一个vfsmount结构体在安装点被创建。
    该结构体用来代表文件系统的实例————换句话说，代表一个安装点。
    
        vfsmount结构被定义在<linux/mount.h>中，下面是具体结构：
        
        struct vfsmount {
            struct list_head            mnt_hash;           // 散列表
            struct vfsmount             *mnt_parent;        // 父文件系统
            struct dentry               *mnt_mountpoint;    // 安装点的目录项
            struct dentry               *mnt_root;          // 该文件系统的根目录项
            struct super_block          *mnt_sb;            // 该文件系统的超级块
            struct list_head            mnt_mounts;         // 子文件系统链表
            struct list_head            mnt_child;          // 子文件系统链表
            int                         mnt_flags;          // 安装标志
            char                        *mnt_devname;       // 设备文件名
            struct list_head            mnt_list            // 描述符链表
            struct list_head            mnt_expire;         // 在到期链表中的入口
            struct list_head            mnt_share;          // 在共享安装链表中的入口
            struct list_head            mnt_slave_list;     // 从安装链表
            struct list_head            mnt_slave;          // 从安装链表中的入口
            struct vfsmount             *mnt_master;        // 从安装链表的主人
            struct mnt_namespace        *mnt_namespace;     // 相关的命名空间
            int                         mnt_id;             // 安装标识符
            int                         mnt_group_id;       // 组标识符
            atomic_t                    mnt_count;          // 使用计数
            int                         mnt_expiry_mark;    // 如果标记为到期，则值为真
            int                         mnt_pinned;         // “钉住”进程计数
            int                         mnt_ghosts;         // “镜像”引用计数
            atomic_t                    __mnt_writers;      // 写者引用计数
        };
        
        理清文件系统和所有其他安装点间的关系，是维护所有安装点链表中最复杂的工作。所以
    vfsmount结构体中维护的各种链表就是为了能够跟踪这些关联信息。
    
        vfsmount结构还保存了在安装时指定的标志信息，该信息存储在mnt_flags域中。表13-1
    列出了标准的安装标志。
    
                        表13-1  标准安装标志列表
            标志                                      描述
        MNT_NOSUID                      禁止该文件系统的可执行文件设置setuid和setgid标志
        MNT_MODEV                       禁止访问该文件系统上的设备文件
        MNT_NOEXEC                      禁止执行该文件系统上的可执行文件
        
        安装那些管理员不充分信任的移动设备时，这些标志很有用处。它们和其他一些很少用的标
    志一起定义在<linux/mount.h>中。
    
    
13.14 和进程相关的数据结构

        








































































