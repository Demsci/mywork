

*****************************************************************************************

									第1章 Linux内核简介

*****************************************************************************************


1.6 内核开发的特点
	*内核编程时不能访问C库。
	*内核编程时必须使用GNU C。
	*内核编程时缺乏像用户空间那样的内存保护机制。
	*内核编程时浮点数很难使用。
	*内核只有一个很小的定长堆栈。
	*由于内核支持异步中断、抢占和SMP，因此必须时刻注意同步和并发。
	*要考虑可移植的重要性。

1.6.1 没有libc库、
	与用户空间的应用程序不同，内核不能链接使用标准C函数库（其他的那些库也不行）。
	造成这种情况的原因有许多，其中就包括先有鸡还是先有蛋这个驳论。不过最主要的原因还
	是在于速度和大小。对内核来说，完整的C库太大了--即便是从中抽取一个合适的子集--
	大小和效率都不能被接受。
	在所有没有实现的函数中，最著名的就数printf()函数了。内核代码虽然无法调用printf()，
	但它可以调用printk()函数。printk()函数负责把格式化好的字符串拷贝到内核日志缓冲区上，
	这样，syslog程序就可以通过读取该缓冲区来获取内核信息。printk()的用法很像printf()。
	printk()和printf()之间的一个显著区别在于printk()允许你通过指定一个标志来设置优先级。
	syslog会根据这这个优先级标志来决定在什么地方显示这条系统消息。下面是一个使用这种优
	先级标志的例子：
		printk(KERN_ERR "this is an error!\n");


		
		
*****************************************************************************************
		
									第2章 进程管理

*****************************************************************************************

	进程是Unix操作系统最基本的抽象之一。一个进程就是处于执行期的程序（目标代码存放
	在某种存储介质上）。但进程并不仅仅局限于一段可执行程序代码（Unix称其为代码段（text
	section））。通常进程还要包含其他资源，像用来存放全局变量的数据段（text section)、打开
	的文件、挂起的信号等，当然还包含地址空间及一个或几个执行线程（threads of execution)。

	执行线程，简称线程（threads),是在进程中活动的对象。每个线程都拥有一个独立的程
	序计数器、进程栈和一组进程寄存器。内核调度的对象是线程，而不是进程。在传统的Unix
	系统中，一个进程只包含一个线程，但现在的系统大都支持多线程应用程序。
	Linux系统的线程实现非常特别---它对线程和进程并不特别区分。
	
	进程提供两种虚拟机制：虚拟处理器和虚拟内存。虽然实际上可能是许多进程正在分享
	一个处理器，但虚拟处理器给进程一种假象，让这些进程觉得自己在独享处理器。第3章将详
	细描述这种虚拟机制。而虚拟内存让进程在获取和使用内存时觉得自己拥有整个系统的所有
	内存资源。第10章将描述虚拟内存机制。线程之间（这里指包含在同一进程中的线程）可以
	共享虚拟内存，但拥有各自的虚拟处理器。

	要注意的是并不能说程序就是进程：进程是处于执行期的程序以及它所包含的资源的总
	称。实际上完全可能存在两个或多个不同的进程实际上执行的是同一个程序。并且两个或两
	个以上并存的进程还可以共享许多诸如打开的文件、地址空间之类的资源。无疑，进程在它
	被创建的时刻开始存活。在Linux系统中，这通常是调用fork()系统调用的结果，该系统调
	用通过复制一个现有进程来创建一个全新的进程。调用fork()的进程被称为父进程，新产生的
	进程被称为子进程。在该调用结束时，在返回点这个相同位置上，父进程恢复执行，子进程
	开始执行。通常创建新的进程都是为了执行新的、不同的程序、而接着调用exec()这族函数
	就可以创建新的地址空间，并把新的程序载入。最终，程序通过exit()系统调用退出执行。
	这个函数会终结进程并将其占用的资源释放掉。父进程可以通过wait4()系统调用查询子进程
	是否终结，这其实使得进程拥有了等待特定进程执行完毕的能力。进程退出执行后被设置为僵死
	状态，直到它的父进程调用wait()或waitpid()为止。
	
	进程的另一个名字是任务(task)。Linux内核通常把进程也叫做任务。在本书中，我会交替使用
	这两个术语，不过我尽量把内核中运行的程序叫做任务，而把用户空间运行的程序叫做进程。


2.1 进程描述符及任务队列
2.1.1 分配进程描述符

	内核把进程存放在叫做任务队列(task list)的双向循环链表中。链表中的每一项都是
	类型为task_struct、称为进程描述符(process descriptor)的结构，该结构定义在
	inclue/linux/sched.h文件中。进程描述符中包含一个具体进程的所有信息。

	进程描述中包含的数据能完整的描述一个正在执行的程序：它打开的文件，进程的地址空间，挂
	起的信号，进程的状态，还有其他更多信息。
	
	Linux通过slab分配器分配task_struct结构，这样能达到对象复用和缓存着色(cache
	coloring)(参见第10章）的目的（通过预先分配和重复使用task_struct，可以避免动
	态分配和释放所带来的资源消耗）。由于现在用slab分配器动态生成task_struct，所以
	只需在栈底（对于向下增长的栈来说）或栈顶（对于向上增长的栈来说）创建一个新的
	结构struct_thread_info。


2.1.2 进程描述符的存放

	内核通过一个唯一的进程标识值(process identification value)或PID来标识每个进程。
	PID是一个数，表示为pid_t隐含类型，实际上就是一个int类型。为了与老版本的Unix和
	Linux兼容，PID的最大值默认设置为32767（short int短整型的最大值）。内核把每个进程的
	PID存放在它们各自的进程描述符中。

	在内核中，访问任务通常需要获得指向其task_struct指针。实际上，内核中大部分处理进
	程的代码都是直接通过task_struct进行的。因此，通过current宏查找到当前正在运行进程的进
	程描述符的速度就显得尤为重要。硬件体系结构不同，该宏的实现也不同。它必须针对专门
	的硬件体系结构做处理。

2.1.3 进程状态
	
	进程描述符中的state域描述了进程的当前状态。系统中的每个进程都必然处
	于五种进程状态中的一种。该域的值也必为下列五种状态标志之一：
	* TASK_RUNNING（运行）---进程是可执行的；它或者正在执行，或者在运行队列中
	  等待执行（运行队列将会在第3章讨论）。
	* TASK_INTERRUPTIBLE（可中断）---进程正在睡眠（也就是说它被阻塞），等待某
	  些条件的达成。一旦这些条件达成，内核就会把进程状态设置为运行。处于此状态地进
	  程也会因为接收到信号而提前被唤醒并投入运行。
	* TASK_UNINTERRUPTIBLE（不可中断）---除了不会因为接收到信号而被唤醒从而
	  投入运行，这个状态与可打断状态相同。这个状态通常在进程必须在等待时不受干扰或
	  等待事件很快就会发生时出现。由于处于此状态的任务对信号不作响应，所以较之可打
	  断状态，使用得较少。
	* TASK_ZOMBIE（僵死）---该进程已经结束了，但是其父进程还没有调用wait4()系统
	  调用。为了父进程能够获知它的消息，子进程的进程描述符仍然被保留着。一旦父进
	  程调用了wait4()，进程描述符就会被释放。
	* TASK_STOP PED（停止）---进程停止执行；进程没有投入运行也不能投入运行。通
	  常这种状态发生在接收到SIGSTOP、SIGTSTP、SIGTTIN、SIGTTOU等信号的时候。
	  此外，在调试期间接收到任何信号，都会使进程进入这种状态。

2.1.4 设置当前进程状态
	
	内核经常需要调整某个进程的状态。这时最好使用set_task_state(task, state)函数，该函数
	将指定的进程设置为指定的状态。必要的时候，它会设置内存屏障来强制其他处理器作重新
	排序（一般只有在SMP系统中有此必要）。否则，它等价于：
		task->state = state;
	set_current_state(state)和set_task_state(current, state)含义是等同的。

2.1.5 进程上下文

	可执行程序代码是进程的重要组成部分。这些代码从可执行文件载入到进程的地址空间
	执行。一般程序在用户空间执行。当一个程序执行了系统调用（参见第4章）或者触发了某
	个异常，它就陷入了内核空间。此时，我们称内核“代表进程执行”并处于进程上下文中。
	在此上下文中current宏是有效的。除非在此间隙有更高优先级的进程需要执行并由调度器
	做出了相应调整，否则在内核退出的时候，程序恢复在用户空间继续执行。

	系统调用和异常处理程序是对内核明确定义的接口。进程只有通过这些接口才能陷入内
	核执行---对内核的所有访问都必须通过这些接口。

	Linux进程之间存在一个明显的继承关系。所有的进程都是PID为1的init进程的后代。内
	核在系统启动的最后阶段启动init进程。该进程读取系统的初始化脚本（initscripts）并执行其
	他的相关程序，最终完成系统启动的整个过程。

	系统中的每个进程必有一个父进程。相应的，每个进程也可以拥有一个或多个子进程。
	拥有同一个父进程的所有进程被称为兄弟。进程间的关系存放在进程描述符中。每个
	task_struct都包含一个指向其父进程task_struct、叫做parent的指针，还包含一个称为children的
	子进程链表。所以，对于当前进程，可以通过下面的代码获得其父进程的进程描述符：
		struct task_struct *task = current->parent;
	同样，也可以按以下方式依次访问子进程：
		struct task_struct *task;
		struct list_head *list;
		list_for_each(list, &current->children) {
			task = list_entry(list, struct task_struct, sibling);
			/* task 现在指向当前的某个子进程 */
		}
	init进程的进程描述符是作为init_task静态分配的。下面的代码可以很好地演示所有进程
	之间的关系：
		struct task_struct *task;
		for (task = current; task != &init_task; task = task->parent)
			;
		/* task 现在指向init */



2.2 进程创建

	Unix的进程创建很特别。许多其他的操作系统都提供了产生(spawn)进程的机制，首先
	在新的地址空间里创建进程，读入可执行文件，最后开始执行。Unix采用了与众不同的实现
	方式，它把上述步骤分解到两个单独的函数中去执行：fork()和exec()。首先，fork()通过拷
	贝当前进程创建一个子进程。子进程与父进程的区别仅仅在于PID（每个进程唯一）、PPID
	（父进程的进程号，子进程将其设置为被拷贝进程的PID）和某些资源和统计量（例如挂起的
	信号，它没有必要被继承）。exec()函数负责读取可执行文件并将其载入地址空间开始运行。
	把这两个函数组合起来使用的效果跟其他系统使用的单一函数的效果相似。

2.2.1 写时拷贝
	
	传统的fork()系统调用直接把所有的资源复制给新创建的进程。这种实现过于简单并且效
	率低下。Linux的fork()使用写时拷贝（copy-on-write）页实现。写时拷贝是一种可以推迟甚至
	免除拷贝数据的技术。内核此时并不复制整个进程地址空间，而是让父进程和子进程共享同
	一个拷贝。只在在需要写入的时候，数据才会被复制，从而使各个进程拥有各自的拷贝。也
	就是说资源的复制只有在需要写入的时候才进行，在此之前，只是以只读方式共享。这种
	技术使地址空间上的页的拷贝推迟到实际发生写入的时候。在页根本不会被写入的情况下
	---举例来说，fork()后立即调用exec()---它们就无需复制了。fork()的实际开销就是复制父
	进程的页表以及给子进程创建唯一的进程描述符。这种优化可以避免拷贝大量根本就不会被
	使用的数据（地址空间里常常包含数十兆的数据）；因为在一般情况下，进程创建后都会马
	上运行一个可执行的文件。由于Unix强调进程快速执行的能力，所以这个优化是很重要的。


2.3 线程在Linux中的实现
	
	线程机制是现代编程技术中常用的一种抽象。该机制提供了在同一程序内共享内存地址
	空间运行的一组线程。这些线程还可以共享打开的文件和其他资源。线程机制支持并发程序设计
	技术(concurrent programming)，在多处理器系统上，它也能保证真正的并行处理（parallelism）。

	Linux实现线程的机制非常独特。从内核的角度来说，它并没有线程这个概念。Linux把
	所有的线程都当作进程来实现。内核并没有特别的调度算法或是定义特别的数据结构来
	表征线程。相反，线程仅仅被视为一个使用某些共享资源的进程。每个线程都拥有唯一隶属
	于自己的task_struct，所以在内核中，它看起来就像是一个普通的进程（只是该进程和其他一
	些进程共享某些资源，如地址空间）。
	
	上述线程机制的实现与Microsoft Windows或是Sun Solaris等操作系统的实现差异非常大。
	这些系统都在内核中提供了专门支持线程的机制（这些系统常常把线程称作轻量级进程）
	（lightweight process））。“轻量级进程”这种叫法本身就概括了Linux在此处与其他系统的差
	异。在其他的系统中，相较于重量级的进程，线程被抽象成一种耗费较少资源，运行迅速的
	执行单元。而对于Linux来说，它只是一种进程间共享资源的手段（Linux的进程本身就够轻
	了）。举个例子来说，假如我们有一个包含四个线程的进程，在提供专门线程支持的系统中，
	通常会有一个包含指向四个不同线程的指针的进程描述符。该描述符负责描述像地址空间、
	打开的文件这样的共享资源。线程本身再去描述它独占的资源。相反，Linux仅仅创建四个进
	程并分配四个普通的task_struct结构。建立这四个进程时指定他们共享某些资源就行了。

	线程的创建和普通进程的创建类似，只不过在调用clone()的时候需要传递一些参数标志
	来指明需要共享的资源：
		clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0);
	上面的代码产生的结果和调用fork()差不多，只是父子俩共享地址空间、文件系统资源、
	文件描述符和信号处理程序。换个说法就是新建的进程和它的父进程都叫做线程。


    内核线程
	
	内核经常需要在后台执行一些操作。这种任务可以通过内核线程（kernel thread）完
	成---独立运行在内核空间的标准进程。内核线程和普通的进程间的区别在于内核线程没有
	独立的地址空间（实际上它的mm指针被设置为NULL）。它们只在内核空间运行，从来不切
	换到用户空间去。内核进程和普通进程一样，可以被调度，也可以被抢占。

	Linux确实会把一些任务交给内核线程去做，像pdflush和ksofirqd这些任务就是明显的例
	子。这些线程在系统启动时由另外一些内核线程启动。实际上，内核线程也只能由其他内核
	线程创建。在现有内核线程中创建一个新的内核线程的方法如下：
		int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
	新的任务也是通过向普通的clone()系统调用传递特定的flags参数而创建的。在上面的函
	数返回时，父线程退出，并返回一个指向子线程task_struct的指针。子线程开始运行fn指向的
	函数，arg是运行时需要用到的参数。一个特殊的clone标志CLONE_KERNEL定义了内核线程
	常用到的参数标志：CLONE_FS、CLONE_FILES、CLONE_SIGHAND。大部分的内核线程
	把这个标志传递给它们的flags参数。

	一般情况下，内核线程会将它在创建时得到的函数永远执行下去（除非系统重启）。该函数
	通常由一个循环构成，在需要的时候，这个内核线程就会被唤醒和执行，完成了当前任务，
	它会自行休眠。



	
	
*****************************************************************************************

										第3章 调度

*****************************************************************************************

	调度程序是内核的组成部分，它负责选择下一个要运行的进程。调度程序（有时也称作
	进程调度程序）可看作在可运行态进程之间分配有限的处理器时间资源的代码。调度程序是
	像Linux这样的多任务操作系统的基础。只有通过调度程序的合理调度，系统资源才能最大限
	度地发挥作用，多进程才会有并发执行的效果。

	调度程序没有太复杂的原理。最大限度地利用处理器时间的原则是，只要有可以执行的
	进程，那么就总会有进程正在执行。但是只要系统中进程的数目比处理器的个数多，就注定
	会有一些进程不能一直执行。这些进程在等待运行。在一组处于可运行状态的进程中选择一
	个来执行，是调度程序所需完成的基本工作。

	多任务系统可以划分为两类：非抢占式多任务（cooperative multitasking）和抢占式多任
	务（preemptive multitasking）。Linux提供了抢占式的多任务模式。在此模式下， 由调度
	程序来决定什么时候停止一个进程的运行以便其他进程能够得到执行机会。这个强制的挂起动作
	叫做抢占（preemption）。进程在被抢占之前能够运行的时间是预先设置好的，而且有一个专门
	的名字，叫进程的时间片（timeslice）。时间片实际上是分配给每个进程的处理器时间段。
	有效管理时间片能使调度程序从系统全局的角度做出调度决定，这样做还可以避免个别进程
	独占系统资源。Linux调度程序采用动态方法计算时间片，这样带来了许多好处。

	在Linux2.5系列的内核中，调度程序做了大手术。开始采用了一种叫做O(1)调度程序的
	新调度程序---它是因为其算法的行为而得名的。它解决了先前版本Linux调度程序的许多
	不足，引入了许多强大的新特性和性能特征。


3.1 策略

	策略决定调度程序在何时让什么进程运行。调度器的策略往往就决定系统的整体印象，
	并且，还要负责优化使用处理器时间。无论从哪个方面来看，它都是至关重要的。

3.1.1 I/O消耗型和处理器消耗型的进程
	
	进程可以被分为I/O消耗型和处理器消耗型。前者指进程的大部分时间用来提交I/O请求或
	是等待I/O请求。因此，这样的进程经常处于可运行的状态，但通常都是运行短短的一会儿，因
	为它在等待更多的I/O请求时最后总会阻塞（这里指的是所有的I/O操作，像键盘活动等都包括，
	并不仅仅局限于磁盘I/O）。相反，处理器耗费型进程把时间大多用在执行代码上。除非被抢
	占，否则它们通常都一直不停地运行，因为它们没有太多的I/O需求。但是，因为它们不属于
	I/O驱动类型，所以从系统响应速度考虑，调度器不应该经常让它们运行。对于这类处理器消
	耗型的进程，调度策略是尽量降低它们的运行频率，而将它们的运行时间拖长一些。当然，
	这种划分方法并非是绝对的。Unix各种变体的调度策略倾向于I/O消耗型的进程。

	调度策略通常要在两个矛盾的目标中间寻找平衡：进程响应迅速（响应时间短）和进程
	吞吐量高。为了满足上述需求，调度程序通常采用一套非常复杂的算法来决定将最值得运行的
	进程投入运行，但是它往往并不保证低优先进程会被公平对待。因为交互式程序都是I/O消
	耗型的，所以调度程序向这种类型的进程倾斜会缩短系统响应时间。Linux为了保证交互式应
	用，所以对进程的响应作了优化（缩短响应时间），更倾向于优先调度I/O消耗型进程。虽然
	如此，调度程序也并未忽略处理器消耗型的进程。


3.1.2 进程优先级

	调度算法中最基本的一类就是基于优先级的调度。这是一种根据进程的价值和其对处理
	器时间的需求来对进程分级的想法。优先级高的进程先运行，低的后运行，相同优先级的进
	程按轮转方式进行调度（一个接一个，重复进行）。在包括Linux在内的某些系统中，优先级
	高的进程使用的时间片也较长。调度程序总是选择时间片未用尽而且优先级最高的进程运行。
	用户和系统都可以通过设置进程的优先级来影响系统的调度。

	Linux根据以上思想实现了一种基于动态优先级的调度方法。一开始，该方法先设置基本
	的优先级，然而它允许调度程序根据需要来加、减优先级。举个列子，如果一个进程在I/O等
	待上耗费的时间多于其运行时间，那么该进程明显属于I/O消耗型进程。它的优先级会被动态
	提高。作为一个反例，如果一个进程的全部时间片一下就被耗尽，那么该进程属于处理器消
	耗型进程---它的优先级会被动态地降低。

	Linux内核提供了两组独立的优先级范围。第一种是nice值，范围从-20到19，默认值是0。
	nice的值越大优先级越低---你正在为系统中其他进程做好事（being nice）。nice值小的进程
	（优先级高）在nice值大的进程（优先级低）之前执行。另外nice值也用来决定分配给进程的
	时间片的长短。nice值为-20的进程获得的时间片最长，nice值为19的进程获得的时间片最短。
	nice是所有Unix系统都用到的标准优先级范围。

	第二个范围是实时优先级，它将在后面讨论。默认情况下它的变化范围是从0到99。任何
	实时进程的优先级都高于普通的进程。Linux提供对POSIX实时优先级的支持。大部分现代的
	Unix操作系统也都提供类似的机制。


3.13 时间片
		
	时间片是一个数值，它表明进程在被抢占前所能持续运行的时间。调度策略必须规定
	一个默认的时间片，但这并不是件简单的事。时间片过长会导致系统对交互的响应表现欠
	佳；让人觉得系统无法并发执行应用程序。时间片太短会明显增大进程切换带来的处理器耗
	时，因为肯定会有相当一部分系统时间用在进程切换上，而这些进程能够用来运行的时间片
	却很短。此外，I/O消耗型和处理器消耗型的进程之间的矛盾在这里也再次显露出来：I/O消耗
	型不需要长的时间片，而处理器消耗型的进程则希望越长越好（比如说这样可以让它们的高
	速缓存命中率更高）。

	从上面的争论中可以看出，任何长时间片都将导致系统交互表现欠佳。很多操作系统中
	都特别重视这一点，所以默认的时间片很短---如20毫秒。不过Linux利用了优先级最高的进
	程总是在运行的这一原则。

	Linux调度程序提高交互式程序的优先级，让它们运行的更频繁。于是，调度程序提供较
	长的默认时间片给交互式程序。此外，Linux调度程序还能根据进程的优先级
	动态调整分配给它的时间片。从而保证了优先级高的进程，也应该是重要性高的进程，执行
	的频率高，执行的时间长。通过实现这样一种动态调整优先级和时间片长度的机制，Linux调度
	性能不但非常稳定而且也很强健。

	注意，进程并不是一定非要一次就用完它所有的时间片。举例来说，一个拥有100毫秒时
	间片的进程并不一定在一次运行中就要用完所有这些时间。相反，进程可以通过重复调度，
	分五次每次20毫秒用完这些时间片。这样，即使是交互式程序也能从中获益---当它们没必
	要一次用这么多时间的时候，它们可以分几次使用，这样能保证它们尽可能长时间的处于可
	运行状态。

	当一个进程的时间片耗尽时，就认为进程到期了。没有时间片的进程不会再投入运行，
	除非等到其他所有的进程都耗尽了它们的时间片（也就是说它们的剩余时间片为0）。在那个
	时候，所有进程的时间片会被重新计算。以后的章节里，我们会讨论Linux进程调度程序处理
	时间片耗尽的一些有趣的算法。
	
	
3.1.4 进程抢占

	像前面所说的，Linux系统是抢占式的。当一个进程进入TASK_RUNNING状态，内核会
	检查它的优先级是否高于当前正在执行的进程。如果是这样，调度程序会被唤醒，重新选择
	新的进程执行（应该会是刚刚进入可运行状态的这个进程）。此外，当一个进程的时间片为
	0时，它会被抢占，调度程序被唤醒以选择一个新的进程。
	
3.1.5 调度策略的活动

	想象下面这样一个系统，它拥有两个运行中的进程：一个文字编辑程序和一个视频编码
	程序。文字编辑程序显然是I/O消耗型的，因为它在大部分时间都在等待用户的键盘输入（无
	论用户的输入速度有多快，都不可能赶上处理的速度）。用户是希望一按下键系统就能马上
	响应。相反，视频编码程序是处理器消耗型的。除了最开始从磁盘上读出原始数据流和最后
	把处理好的视频输出，程序所有的时间都用来对原始数据进行进行编码。它对什么时间开始
	运行没有太严格的要求---用户几乎分辨不出它到底是立刻就运行还是半秒钟以后才开始的。
	当然，它完成的越早越好。
	
	在此系统中，调度程序给文字编辑程序较高的优先级和较长的时间片，因为它是交互式
	的。文字编辑程序有充足的时间片用。此外，由于拥有较高的优先级，所以文字编辑程序还
	能在需要的时候抢占视频编码程序。这样才能保证文字编辑程序对用户键盘输入的即时响应。
	这样做当然会影响视频编码程序，但由于文字编辑程序仅仅只在有输入的那一会儿运行，所
	以视频编码程序可以在所有剩余时间中独享处理器。这样的优化同时提高了这两种应用的性能。
	
	
3.2 调度算法

3.2.1 可执行队列

	调度程序中最基本的数据结构是运行队列(runqueue)。可执行队列定义于kernel/sched.c
	中，由结构runqueue表示。可执行队列是给定处理器上的可执行进程的链表，每个处理器一
	个。每个可投入运行的进程都唯一的归属于一个可执行队列。此外，可执行队列中还包含每
	个处理器的调度信息。所以，可执行队列也是每个处理器最重要的数据结构。

	观察一下这个结构体，其中的注释对每项进行了描述：
	struct rq {
		spinlock_t lock;						/* 保护运行队列的自旋锁 */
		unsigned long nr_running;				/* 可运行任务数目 */
		unsigned long long nr_switches;			/* 上下文切换数目 */
		unsigned long nr_uninterruptible;		/* 处于不可中断睡眠状态的任务数目 */
		unsigned long expired_timestamp;		/* 队列最后被换出时间 */
		struct task_struct *curr； 				/* 该处理器的当前运行任务 */
		struct task_struct *idle;				/* 该处理器的空任务 */
		struct mm_struct *prev_mm;				/* 最后那个运行任务的mm_struct结构体 */
		struct prio_array *active；				/* 指向活动优先级数组的指针 */
		struct prio_array *expired；			/* 指向超时优先级数组的指针 */
		struct prio_array arrays[2]；			/* 实际优先级数组 */
		atomic_t nr_iowait;						/* 等待I/O操作的任务数 */
		struct task_struct *migration_thread;	/* 该处理器上的移出线程 */
		struct list_head migration_queue;		/* 该处理器上的移出队列 */
	};
	
	在对可执行队列进行操作以前，应该先锁住它（锁机制在第７章讨论）。因为每个可执
	行队列唯一地对应一个处理器，所以很少出现一个处理器需要锁其他处理器的可执行队列的
	情况（我们将会看到，这种情况还确实可能会出现）。在其拥有者读取或改写队列成员的时候，
	可执行队列包含的锁用来防止队列被其他代码改动。锁住某个队列的最常见情况发生在一个
	特定的任务在运行队列执行时，此时需要用到task_rq_lock()和task_rq_unlock()函数：
		struct runqueue *rq;
		unsigned long_flags;
		rq = task_rq_lock(task, &flags);
		...... // 对任务的队列进行操作
		task_rq_unlock(rq, &flags);
	或者可以用this_rq_lock()来锁住当前的可执行队列，用rq_unlock(struct runqueue *rq)释放
	给定队列上的锁。

3.2.2 优先级数组

	每个运行队列都有两个优先级数组，一个活跃的和一个过期的。优先级数组在kernel/
	sched.c中被定义，它是prio_array类型的结构体。优先级数组是一种能够提供O(1)级算法复杂
	度的数据结构。优先级数组使可运行处理器的每一种优先级都包含一个相应的队列，而这些
	队列包含对应优先级上的可执行进程链表。优先级数组还拥有一个优先级位图，当需要查找
	当前系统内拥有最高优先级的可执行进程时，它可以帮助提高效率。
	struct prio_array {
		int               nr_active;					/* 任务数目 */
		unsigned long     bitmap[BITMAP_SIZE];			/* 优先级位图 */		
		struct list_head  queue[MAX_PRIO];				/* 优先级队列 */
	};
	MAX_PRIO定义了系统拥有的优先级个数。默认值是140。这样，每个优先级都有一个
	struct list_head结构体。BITMAP_SIZE是优先级位图数组的大小，类型为unsigned long长整
	型，长32位，如果每一位代表一个优先级的话，140个优先级需要5个长整整型数才能表示。所
	以，bitmap就正好含有5个数组项，总共160位。
	
	每个优先级数组都包含一个这样的位图成员，至少为每个优先级准备一位。一开始，
	所有的位都被置为0。当某个拥有一定优先级的进程开始准备执行时（也就是状态变为
	TASK_RUNNING)，位图中相应的位就会被置为1。比如如果一个优先级为7的进程变为可
	执行状态，第7位就被置为1。这样，查找系统中最高的优先级就变成了查找位图中被设置的
	第一个位。因为优先级个数是个定值，所以查找时间恒定，并不受系统到底有多少可执行进
	程的影响。此外，Linux对它支持的每一种体系结构都提供了对应的快速查找算法，以保证对
	位图的快速查找。提供该功能的函数叫sched_find_first_bit()。
	
	每个优先级数组还包含一个叫做struct list_head的队列，每个优先级对应一个队列。每个
	链表与一个给定的优先级相对应，事实上，每个链表都包含该处理器队列上相应优先级的全
	部可运行进程。所以要找到下一个要运行的任务非常简单，就像在链表中选择下一个元素一
	样。对于给定的优先级，按轮转方式调度任务。
	优先级数组还包含一个计数器nr_active。它保存了该优先级数组内可执行进程的数目。
	
3.2.3 重新计算时间片

	新的Linux调度程序减少了对循环的依赖。取而代之的是它为每个处理器维护两个优先级
	数组：既有活动数组也有过期数组。活动数组内的可执行队列上的进程都还有时间片剩余；
	而过期数组内的可执行队列上的进程都耗尽了时间片。当一个进程的时间片耗尽时，它会被
	移至过期数组，但在此之前，时间片已经给它重新计算好了。重新计算时间片现在变得非常
	简单，只要在活动和过期数组之间来回切换就行了。因为数组是通过指针进行访问的，所以
	交换它们用的时间就是交换指针需要的时间。这个动作由schedule()完成：
		struct prio_array * array = rq->active;
		if (!array->nr_active) {
			rq->active = rq->expired;
			rq->expired = array;
		}
	这种交换是O(1)级调度程序的核心。O(1)级的调度程序根本不需要从头到尾忙着重新
	计算时间片，它只要完成一个两个步骤就能实现数组的切换。这种实现完美地解决了前面列
	举的所有弊端。
	schedule()
	选定下一个进程并切换到它去执行是通过schedule()函数实现的。当内核代码想要休眠时，
	会直接调用该函数，另外，如果有哪个进程将被抢占，那么该函数也会被唤醒执行。
	
	考虑到它要完成如此繁多的任务，schedule()函数的实现实在算是简洁了。下面的代码用
	来判断谁是优先级最高的进程：
		struct task_struct *prev, *next;
		struct list_head *queue;
		struct prio_array array;
		int idx;
		
		prev = current;
		array = rq->active;
		idx = sched_find_first_bit(array->bitmap);
		queue = array->queue + idx;
		next = list_entry(queue->next, struct task_struct, run_list);
		
	首先，要在活动优先级数组中找到第一个被设置的位。该位对应着优先级最高的可执行
	进程。然后，调度程序选择这个级别链表里的头一个进程。这就是系统中优先级最高的可执
	行程序，也是马上会被调度执行的进程。
	如果prev和next不等，说明被选中的进程不是当前进程。此时与体系结构相关的函数
	context_switch()被调用，负责从prev切换到next。我们将在后续章节中讨论上下文切换。
	
	
3.2.4 计算优先级和时间片

	进程拥有一个初始的优先级，叫做nice值。该数值变化范围为-20到19，默认为0。19优先
	级最低，-20最高。进程task_struct的static_prio域就存放着这个值。之所以起名为静态优先级
	(static priority),是因为它从一开始由用户指定后，就不能改变。而调度程序要用到的动态优
	先级存放在prio域里。动态优先级通过一个关于静态优先级和进程交互性的函数关系计算而来。
	
	effective_prio()函数可以返回一个进程的动态优先级。这个函数以nice值为基数，再加
	上-5到+5之间的进程交互性的奖励或罚分。举例来说，一个交互性很强的进程，即使它的
	nice值为10，它的动态优先级最终也有可能达到5。相反，一个温和的处理器吞噬者，虽然本
	来nice值一样是10，它最后的动态优先级却可能是12。交互性不强不弱的进程不会得到优先
	级的奖励，同样也不会被罚分，所以它的动态优先级和它的nice值相等。
	
	当然，调度程序不可能通过魔法来了解一个进程的交互性到底强不强。它需要做些推断
	来获取准确反映进程到底是I/O消耗型的还是处理器消耗型的。最明显的标准莫过于进程休眠
	的时间长短了。如果一个进程的大部分时间都在休眠，那么它就是I/O消耗型的。如果一个进
	程执行的时间比休眠的时间长，那它就是处理器消耗型的。这个标准可以向极端延伸；一个
	进程如果几乎所有的时间都用在休眠上，那么它就是一个纯粹的I/O消耗型进程，相反，一个
	进程如果几乎所有的时间都在执行，那么它就是纯粹的处理器消耗型进程。
	
	为了支持这种推断机制，Linux记录了一个进程用于休眠和用于执行的时间。该值存放在
	task_struct的sleep_avg域中。它的范围从0到MAX_SLEEP_AVG。它的默认值为10毫秒。当一个进
	程从休眠状态恢复到执行状态时，sleep_avg会根据它休眠时间的长短而增长，直到达到
	MAX_SLEEP_AVG为止。相反，进程每运行一个时钟节拍，sleep_avg就做相应的递减，到0为止。
	
	这种推断准确得让人吃惊。它的计算不仅仅基于休眠时间有多长，而且运行时间的长短
	也要被计算进去。所以，尽管一个进程休眠了不少时间，但它如果总是把自己的时间片用得
	一干二净，那么它就不会得到大额的优先级奖励――――这种推断机制不仅会奖励交互性强的进
	程，它还会惩罚处理器耗费量大的进程，并且不会滥用这些奖惩手段。如果一个进程发生了
	变化，开始大量占用处理器时间，那么，它很快就会失去曾经得到的优先级提升。最后要说
	的是，这种衡量标准还有很快的反应速度。一个新创建的交互性进程的sleep_avg很快就会涨
	得很高。即便如此，由于奖励和罚分都加在作为基数的nice值上，所以用户还是可以通过改变
	进程的nice值来对调度程序施加影响。
	另一方面，由于动态优先级本身就已经以nice值和交互性(调度程序推断优先级的标准很
	重要)为基础来取值了，所以重新计算时间片相对就简单了。它只要以动态优先级为基础就
	可以了。在一个进程创建的时候，新建的子进程和父进程均分父进程剩余的进程时间片。这
	样的分配很公平并且防止用户通过不断创建进程来不停地攫取时间片。然而，当一个任务
	的时间片用完之后，就要根据任务的动态优先级重新计算时间片。task_timeslice()函数为给定
	任务返回一个新的时间片。时间片的计算只需要把优先级按比例缩放，使其符合时间片的数
	值范围要求就可以了。进程的优先级越高，它每次执行得到的时间片就越长。优先级最高的
	进程能获得的最大的时间片长度(MAX_TIMESLICE)是200毫秒。而优先级最低的进程获得的
	最短时间片长度(MIN_TIMESLIC)为10毫秒。默认优先级(nice值为0，没有交互性的奖励，
	也没有罚分)的进程得到的时间片长度为100毫秒。
	
	调度程序还提供了另外一种机制以支持交互进程：如果一个进程的交互性非常强，那么
	当它时间片用完后，它会被再放置到活动数组而不是过期数组中。回忆一下，重新计算时间
	片是通过活动数组与过期数组之间的切换来进行的。一般进程在用尽它们的时间片后，都会
	被移至过期数组，当活动数组中没有剩余进程的时候，这两个数组就会被交换；活动数组变
	成过期数组，过期数组替代活动数组。这种操作提供了时间复杂度为O(1)的时间片重新计算。
	但在这种交换发生之前，交互性很强的一个进程有可能已经牌过期数组中了，当它需要交
	互的时候，它却无法执行，因为必须等到数组交换发生为止才可执行。将交互式的进程重新
	插入到活动数组可以避免这种问题。注意，该进程不会被立即执行，它会和优先级相同的进
	程轮流着被调度和执行。该逻辑在scheduler_tick()中实现，该函数会被定时器调用（在第
	9章中讨论）：
		struct task_struct *task = current;
		struct runqueue *rq = this _rq();
		
		if (!--task->time_slice) {
			if (!TASK_INTERACTIVE(task) || EXPIRED_STARVING(rq))
				enqueue_task(task, rq->expired);
			else
				enqueue_task(task, rq->active);
		}
	首先，这段代码减小进程时间片的值，再看它是否为0。如果是的话就说明进程的时间片
	已经耗尽，需要把它插入到一个数组中，所以该代码先通过TASK_INTERACTIVE()宏来查看
	这个进程是不是交互型的进程。该宏主要基于进程的nice值来判定它是不是一个“交互性十足”
	的进程。nice值越小(优先级越高)，越能说明该进程交互性越高。一个nice值为19的进程交
	互性表现的再强，它也不可能被重新加入到活动数组中去。而一个nice值为-20的进程要想不
	被重新加入，只有拼命的占用处理器才行。一个拥有默认优先级的进程，需要表现出一定的
	交互性才能被重新加入，但这通常很容易就能满足。接着，EXPIRED_STARVING()宏负责检
	查过期数组内的进程是否处于饥饿状态――――是否已经有相对较长的时间没有发生数组切换
	了。如果最近一直没有发生切换，那么再把当前的进程放置到活动数组会进一步拖延切换
	――――过期数组内的进程会越来越饿。只要不发生这种情况，进程就会被重新放置在活动数组
	里。否则，进程会被放入过期数组里，这也是最普通的一种操作。
	
	
3.2.5 睡眠和唤醒

	休眠(被阻塞)的进程处于一个特殊的不可执行状态。这点非常重要，否则调度程序就
	可能选出一个本不愿意被执行的进程，更糟糕的是，休眠就必须以轮询的方式实现了。进程
	休眠有各种原因，但肯定都是为了等待一些事件。事件可能是一段时间，从文件I/O读更多数
	据，或者是某个硬件事件。一个进程还有可能在尝试获取一个已被占用的内核信号量时被迫
	进入休眠(这部分在第8章中讨论)。休眠的一个常见原因就是文件I/O――――如进程对一个文件
	执行了read()操作，而这需要从磁盘里读取。还有，进程在获取键盘输入的时候也需要等待。
	无论哪种情况，内核的操作都相同：进程把它自己标记成休眠状态，把自己从可执行队列移
	出，放入等待队列，然后调用schedule()选择和执行一个其他进程。唤醒的过程刚好相反：进
	程被设置为可执行状态，然后再从等待队列中移到可执行队列。
	
	休眠通过等待队列进行处理。等待队列是由等待某些事件发生的进程组成的简单链表。
	内核用wake_queue_head_t来代表等待队列。等待队列可以通过DECLARE_WAITQUEUE()静
	态创建，也可以由init_waitqueue_head()动态创建。进程把自己放入等待队列中并设置成不可
	执行状态。当与等待队列相关的事件发生的时候，队列上的进程会被唤醒。为了避免产生竞
	争条件，休眠和唤醒的实现不能有纰漏。
	
	针对休眠，以前曾经使用过一些简单的接口。但那些接口会带来竞争条件；有可能导致
	在判定条件变为真后进程却开始了休眠，那样就会使进程无限期休眠下去。所以，在内核
	中进行休眠的推荐操作就相对复杂了一些：
		/* 'q' 是我们希望睡眠的等待队列  */
		DECLARE_WAITQUEUE(wait, current);
		
		add_wait_queue(q, &wait);
		set_current_state(TASK_INTERRUPTIBLE); /* 或TASK_UNINTERRUPTIBLE状态 */
		while (!condition) /* 'condition'是我们在等待的事件 */
			schedule();
		set_current_state(TASK_RUNNING);
		romove_wait_queue(q, &wait);
		
	进程通过下面几个步骤将自己加入到一个等待队列中：
	1，调用DECLARE_WAITQUEUE()创建一个等待队列的项。
	2，调用add_wait_queue()把自己加入到队列中。该队列会在进程等待的条件满足时唤醒它。
	   当然我们必须在其它地方撰写相关代码，在事件发生时，对等待队列执行wake_up()操作。
	3，将进程的状态变更为TASK_INTERRUPTIBLE或TASK_UNINTERRUPTIBLE。
	4，检查条件是否为真；如果是的话，就没必要休眠了。如果条件不为真，调用schedule()。
	5，当进程被唤醒的时候，它会再次检查条件是否为真。如果是，它就退出循环，如果不是，
	   它再次调用schedule()并一直重复这步操作。
	6，当条件满足后，进程将自己设置为TASK_RUNNING并调用remove_wait_queue()把自己
	   移出等待队列。
	   
	如果在进程开始休眠之前条件就已经达成了，那么循环会退出，进程不会存在错误的进
	入休眠的倾向。需要注意的是，内核代码在循环体内常常需要完成一些其他的任务，比如，
	它可能在调用schedule()之前需要释放掉锁，而在这以后再重新获取它们，或者检查是否有信
	号到来并引起返回一个-ERESTARTSYS，又或者响应其他什么事件。
	
	唤醒操作通过函数wake_up()进行，它会唤醒指定的等待队列上的所有进程。它调用函数
	try_to_wake_up()，该函数负责将进程设置为TASK_RUNNING状态，调用activate_task()将此
	进程放入可执行队列，如果被唤醒的进程优先级比当前正在执行的进程的优先级高，还要设
	置need_resched标志。通常哪段代码促使等待条件达成，它就要负责随后调用wake_up()函数。
	举例来说，当磁盘数据到来以后，VFS就要负责对等待队列调用wake_up()，以便唤醒队列中
	等待这些数据的进程。
	
	关于休眠有一点需要注意，存在虚假的唤醒。有时候进程被唤醒并不是因为它所等待的
	条件达成了，所以才需要用一个循环处理来保证它等待的条件真正达成。


3.3 抢占和上下文切换

	上下文切换，也就是从一个可执行进程切换到另一个可执行进程，由定义在kernel/
	sched.c中的context_switch()函数负责处理。每当一个新的进程被选出来准备投入运行的时候，
	schedule()就会调用该函数。它完成了两项基本的工作：
	*调用定义在include/asm/mmu_context.h中的switch_mm()，该函数负责把虚拟内存从上一
	 个进程映射切换到新的进程中。
	*调用定义在include/asm/system.h中的switch_to(),该函数负责从上一个进程的处理器状
	 态切换到新进程的处理器状态。这包括保存、恢复栈信息和寄存器信息。
	
	内核必须知道在什么时候调用schedule()。如果仅靠用户程序代码显式地调用schedule(),
	它们可能就会永远地执行下去。相反，内核提供了一个need_resched标志来表明是否需要重新
	执行一次调度。当某个进程耗尽它的时间片时，scheduler_tick()就会设置这个标
	志；当一个优先级高的进程进入可执行状态的时候，try_to_wake_up()也会设置这个标志。

	再返回用户空间以及从中断返回的时候，内核也会检查need_resched标志。如果已被设置，
	内核会在继续执行之前调用调度程序。
	
	每个进程都包含一个need_resched标志，这是因为访问进程描述符内的数值要比访问一个
	全局变量快（因为current宏速度很快并且描述符通常都在高速缓存中）。在2.2以前的内核版本
	中，该标志曾经是一个全局变量。2.2到2.4版内核中它在task_struct中。而在2.6版中，它被移
	到thread_info结构体里，用一个特别的标志变量中的一位来表示。可见，内核开发者总是在
	不断改进。

	
3.3.1 用户抢占

	内核即将返回用户空间的时候，如果need_resched标志被设置，会导致schedule()被调用，
	此时就会发生用户抢占。在内核返回用户空间的时候，它知道自己是安全的，因为既然它可
	以继续去执行当前进程，那么它当然可以再去选择一个新的进程去执行。所以，内核无论是
	在从中断处理程序还是在系统调用后返回，都会检查need_resched标志。如果它被设置了，那
	么，内核会选择一个其他（更合适的）进程投入运行。从中断处理程序或系统调用返回的代
	码都是跟体系结构相关的，在entry.S(此文件不仅包含内核入口部分的程序，内核退出部分
	的相关代码也在其中)文件中通过汇编语言来实现。
	
	简而言之，用户抢占在以下情况时产生：
	*从系统调用返回用户空间。
	*从中断处理程序返回用户空间。
	
	
3.3.2 内核抢占

	与其他大部分的Unix变体和其他大部分的操作系统不同，Linux完整地支持内核抢占。在
	不支持内核抢占的内核中，内核代码可以一直执行，到它完成为止。也就是说，调度程序没
	有办法在一个内核级的任务正在执行的时候重新调度――――内核中的各任务是协作方式调度的，
	不具备抢占性。内核代码一直要执行到完成(返回用户空间)或明显的阻塞为止。在2.6版的
	内核中，内核引入了抢占内力；现在，只要重新调度是安全的，那么内核就可以在任何时间
	抢占正在执行的任务。
	
	那么，什么时候重新调度才是安全的呢？只要没有持有锁，内核就可以进行抢占。锁是
	非抢占区域的标志。由于内核是支持SMP的，所以，如果没有持有锁，那么正在执行的代码
	就是可重新导入的，也就是可以抢占的。
	
	为了支持内核抢占所作的第一处变动就是为每个进程的thread_info引入了preempt_count
	计数器。该计数器初始值为0，每当使用锁的时候数值加1，释放锁的时候数值减1。当数值为
	0的时候，内核就可执行抢占。从中断返回内核空间的时候，内核会检查need_resched和
	preempt_count的值。如果need_resched被设置，并且preempt_count为0的话，这说明有一个更
	为重要的任务需要执行并且可以安全地抢占，此时，调度程序就会被调用。如果preempt_
	count不为0，说明有当前任务持有锁，所以抢占是不安全的。这时，就会像通常那样直接从
	中断返回当前执行进程。如果当前进程持有的所有的锁都被释放了，那么preempt_count就会
	重新为0。此时，释放锁的代码会检查need_resched是否被设置。如果是的话，就会调用调度
	程序。有些内核代码需要允许或禁止内核抢占，相关内容会在第8章讨论。
	
	如果内核中的进程被阻塞了，或它显式地调用了schedule()，内核抢占也会显式地发生。
	这种形式的内核抢占从来都是受支持的，因为根本无需额外的逻辑来保证内核可以安全地被
	抢占。如果代码显式的调用了schedule()，那么它应该清楚自己是可以安全地被抢占的。
	
	内核抢占会发生在：
	*当从中断处理程序返回内核空间的时候。
	*当内核代码再一次具有可抢占性的时候。
	*如果内核中的任务显式的调用schedule()。
	*如果内核中的任务阻塞(这同样也会导致调用schedule())。
	
	
3.4 实时
	
	Linux提供了两种实时调度策略：SCHED_FIFO和SCHED_RR。而普通的、非实时的调度
	策略是SCHED_OTHER。SCHED_FIFO实现了一种简单的、先入先出的调度算法，它不使用
	时间片。SHCED_FIFO级的进程会比任何SCHED_OTHER级的进程都先得到调度。一旦一个
	SCHED_FIFO级进程处于可执行状态，就会一直执行，直到它自己受阻塞或显式地释放处理
	器为止；它不基于时间片，可以一直执行下去。如果有两个或者更多的SCHED_FIFO级进程，
	它们会轮流执行。只要有SCHED_FIFO级进程在执行，其他级别较低的进程就只能等待它结
	束后才有机会执行。
	
	SCHED_RR与SCHED_FIFO大体相同，只是SCHED_RR级的进程在耗尽事先分配给它的
	时间后就不能再接着执行了。也就是说，SCHED_RR是带有时间片的SCHED_FIFO――――这是
	一种实时轮流调度算法。
	
	这两种实时算法实现的都是静态优先级。内核不为实时进程计算动态优先级。这能保证
	给定优先级别的实时进程总能抢占优先级比它低的进程。
	
	Linux的实时调度算法提供了一种软实时工作方式。软实时的含义是，内核调度进程，尽
	力使进程在它的限定时间到来前运行，但内核不保证总能满足这些进程的要求。相反，硬实
	时系统保证在一定条件下，可以满足任何调度的要求。Linux对于实时任务的调度不做任何保
	证。它的调度算法只保证只要实时进程可运行，就尽量去执行它们。虽然不能保证硬实时工
	作方式，但Linux的实时调度算法的性能还是很不错的。2.6版的内核可以满足非常严格的时间
	要求。
	
	实时优先级范围从0到MAX_RT_PRIO减1。默认情况下，MAX_RT_PRIO为100――――所以
	默认的实时优先级范围是从0到99。SCHED_OTHER级进程的nice值共享了这个取值空间；它
	的取值范围是从MAX_RT_PRIO到(MAX_RT_PRIO+40)。也就是说，在默认情况下，nice值
	从-20到+19直接对应的是从100到140的实时优先级范围。
	

3.5 与调度相关的系统调用

	Linux提供了一族系统调用，用于管理与调度程序相关的参数。这些系统调用可以用来操
	作和处理进程优先级、调度策略及处理器，同时还提供了显式的将处理器交给其他进程的机制。
	
	许多书籍――――还有友善的man帮助文件――――都提供了这些系统调用(它们都包含在C库中，
	没用什么太多的封装，基本上只调用了系统调用而已)的说明。下面列举了这些系统调用并
	给出了简短的说明。第4章会讨论它们是如何实现的。
	
							与调度相关的系统调用
		系统调用                                   描述
		
		nice()                                     设置进程的nice值
		sched_setscheduler()					   设置进程的调度策略
		sched_getscheduler()                       获取进程的调度策略
		sched_setparam()                           设置进程的实时优先级
		shced_getparam()                           获取进程的实时优先级
		sched_get_priority_max()                   获取实时优先级的最大值
		sched_get_priority_min()                   获取实时优先级的最小值
		sched_rr_get_interval()                    获取进程的时间片值
		sched_setaffinity()                        设置进程的处理器的亲和力
		sched_getaffinity()                        获取进程的处理器的亲和力
		sched_yield()                              暂时让出处理器

		
		

*****************************************************************************************

									第4章 系统调用

*****************************************************************************************

	为了和用户空间上运行的进程进行交互，内核提供了一组界面。透过该界面，应用程序
	可以访问硬件设备和其他操作系统资源。这组界面在应用程序和内核之间扮演了使者的角色，
	应用程序发送各种请求，而内核负责满足这些请求(或者让应用程序暂时搁置)。实际上提供
	这组界面主要是为了保证系统稳定可靠，避免应用程序恣意妄为，惹出大麻烦。
	
	系统调用在用户空间进程和硬件设备之间添加了一个中间层。该层主要作用有三个。首先，
	它为用户空间提供了一种硬件的抽象界面。举例来说，当需要读些文件的时候，应用程序就可
	以不去管磁盘类型和介质，甚至不用去管文件所在的文件系统到底是哪种类型。第二，系统调
	用保证了系统的稳定和安全。作为硬件设备和应用程序之间的中间人，内核可以基于权限和其
	他一些规则对需要进行的访问进行裁决。举例来说，这样可以避免应用程序不正确地使用硬件
	设备，窃取其他进程的资源，或做出其他什么危害系统的事情。第三，在第2章中曾经提到过，
	每个进程都运行在虚拟系统中，而在用户空间和系统的其余部分提供这样一层公共界面，也是
	出于这种考虑。如果应用程序可以随意访问硬件而内核又对此一无所知的话，几乎就没法实现
	多任务和虚拟内存。在Linux中，系统调用是用户空间访问内核的唯一手段；它们是内核唯一
	的合法入口。
	
	本章重点强调Linux系统调用的规则和实现方法。
	
	
4.1 API、POSIX和C库

	一般情况下，应用程序通过应用编程接口(API)而不是直接通过系统调用来编程。这点很重
	要，因为应用程序使用的这种编程接口实际上并不需要和内核提供的系统调用对应。一个API
	定义了一组应用程序使用的编程接口。它们可以实现成一个系统调用，也可以通过调用多个系
	统调用来实现，而完全不使用任何系统调用也不存在问题。实际上，API可以在各种不同的操
	作系统上实现，给应用程序提供完全相同的接口，而它们本身在这些系统上的实现却可能迥异。
	
	在Unix世界中，最流行的应用编程接口是基于POSIX标准的。从纯技术的角度看，
	POSIX是由IEEE的一组标准组成，其目标是提供一套大体上基于Unix的可移植操作系统标
	准。Linux是与POSIX兼容的。
	
	POSIX是说明API和系统调用之间关系的一个极好例子。在大多数Unix系统上，根据
	POSIX而定义的API函数和系统调用之间有着直接关系。实际上，POSIX标准就是仿照早期
	Unix系统的界面建立的。另一方面，许多操作系统，像Windows NT，尽管和Unix没有什么关
	系，也提供了与POSIX兼容的库。
	
	Linux的系统调用像大多数Unix系统一样，作为C库的一部分提供。C库实现了Unix系统的
	主要API，包括标准C库函数和系统调用。所有的C程序都可以使用C库，而由于C语言本身
	的特点，其他语言也可以很方便地把它们封装起来使用。此外，C库提供了POSIX的绝大
	部分API。
	
	从程序员的角度看，系统调用无关紧要；他们只需要跟API打交道就可以了。相反，内核
	只跟系统调用打交道；库函数及应用程序是怎么使用系统调用不是内核所关心的。但是，内
	核必须时刻牢记系统调用所有潜在的用途并保证它们有良好的通用性和灵活性。
	
	关于Unix的界面设计有一句通用的格言“提供机制而不是策略”。换句话说，Unix的系统
	调用抽象出了用于完成某种确定的目的的函数。至于这些函数怎么用完全不需要内核去关心。
	(区别对待机制(mechanism)和策略(policy)是Unix设计中的一大亮点。大部分的编程问
	题都可以被切割成两个部分：“需要提供什么功能”(机制)和“怎样实现这些功能”(策略)。
	如果由程序中的独立部分分别负责机制和策略的实现，那么开发软件就更容易，也更容易适
	应不同的需求。――――译者注)
	

4.2 系统调用

	系统调用(在Linux中常称作syscalls)通常通过函数进行调用。它们通常都需要定义一个
	或几个参数(输入)而且可能产生一些副作用，例如写某个文件或向给定的指针拷贝数据
	等等。系统调用还会通过一个long类型的返回值来表示成功或者错误。Unix系统
	调用在出现错误的时候会把错误码写入errno全局变量。通过调用perror()库函数，可以把该变
	量翻译成用户可以理解的错误字符串。

	当然系统调用最终具有一种明确的操作。举例来说，如getpid()系统调用，根据定义它
	会返回当前进程的PID。内核中它的实现非常简单：
		asmlinkage long sys_getpid(void)
		{
			return current->tgid;
		}

	注意，定义中并没有规定它要如何实现。内核必须提供系统调用所希望完成的功能，但
	它完全可以按自己预期的方式去实现，只要最后的结果正确就行了。当然，上面的系统调
	用太简单，也没有什么更多的实现手段(当然没有更简单的方法)。

	上述的系统调用尽管非常简单，但我们还是可以从中发现两个特别之处。首先，注意函
	数声明中的asmlinkage限定词。其次，注意系统调用get_pid()在内核中被定义成sys_
	getpid()。这是Linux中所有系统调用都应该遵守的命名规则：系统调用bar()在内核中也实现为
	sys_bar()函数。
	
4.2.1 系统调用号

	在Linux中，每个系统调用被赋予一个系统调用号，这样，通过这个独一无二的号就可以
	关联系统调用。当用户空间的进程执行一个系统调用的时候，这个系统调用号就被用来指明
	到底是要执行哪个系统调用；进程不会提及系统调用的名称。
	
	系统调用号相当关键；一旦分配就不能再有任何变更，否则编译好的应用程序就会崩溃。
	此外，如果一个系统调用被删除，它所占用的系统调用号也不允许被回收利用。Linux有一个
	“未实现”系统调用sys_ni_syscall()，它除了返回-ENOSYS外不做任何其他工作，这个错误号
	就是专门针对无效的系统调用而设的。虽然很罕见，但如果一个系统调用被删除，这个函数
	就要负责“填补空位”。
	
	内核记录了系统调用表中的所有已注册过的系统调用的列表，存储在sys_call_table中。
	它与体系结构有关，一般在entry.s中定义。这个表中为每一个有效的系统调用指定了唯一的系
	统调用号。

	
4.3 系统调用处理程序

	用户空间的程序无法直接执行内核代码。它们不能直接调用内核空间中的函数，因为内
	核驻留在受保护的地址空间上。如果进程可以直接在内核的地址空间上读写的话，系统安全
	就会失去控制。
	
	所以，应用程序应该以某种方式通知系统，告诉内核自己需要执行一个系统调用，希望
	系统切换到内核态，这样内核就可以代表应用程序来执行该系统调用了。
	
	通知内核的机制是靠软中断实现的：通过引发一个异常来促使系统切换到内核态去执行
	异常处理程序。此时的异常处理程序实际上就是系统调用处理程序。x86系统上的软中断由int
	$0x80指令产生。这条指令会触发一个异常导致系统切换到内核态并执行第128号异常处理程
	序，而该程序正是系统调用处理程序。这个处理程序名字起得很贴切，叫system_call()。它与
	硬件体系结构紧密相关，通常在entry.S文件中用汇编语言编写。


4.3.1 指定恰当的系统调用

	因为所有的系统调用陷入内核的方式都一样，所以仅仅是陷入内核空间是不够的。因此
	必须把系统调用号一并传给内核，在x86上，这个传递动作是通过在触发软中断前把调用号装
	入eax寄存器实现的。这样系统调用处理程序一旦运行，就可以从eax中得到数据。其他体系
	结构上的实现也都类似。
	
	system_call()函数通过将给定的系统调用号与NR_syscalls做比较来检查其有效性。如果它
	大于或等于NR_syscalls,该函数就返回-ENOSYS。否则，就执行相应的系统调用。
	
	call *sys_call_table(, %eax, 4)
	
	由于系统调用表中的表项是以32位（4字节）类型存放的，所以内核需要将给定的系统调
	用号乘以四，然后用所得的结果在该表中查询其位置，参见图4-2。


4.3.2 参数传递

	除了系统调用号以外，大部分系统调用都还需要一些外部的参数输入。所以，在发生异
	常的时候，应该把这些参数从用户空间传给内核。最简单的办法就是像传递系统调用号一样：
	把这些参数也存放在寄存器里。在x86系统上，ebx,ecx,edx,dsi和edi按照顺序存放前五个
	参数。需要六个或六个以上参数的情况不多见，此时，应该用一个单独的寄存器存放指向所
	有这些参数在用户空间地址的指针。
	
	给用户空间的返回值也通过寄存器传递。在x86系统上，它存放在eax寄存器中。


4.4 系统调用的实现

	实际上，一个Linux的系统调用在实现时并不需要太关心它和系统调用处理程序之间的关
	系。给Linux添加一个新的系统调用是件相对容易的工作。怎样设计和实现一个系统调用是难
	题所在，而把它加到内核里却无须太多周折。让我们关注一下实现一个新的Linux系统调用所
	需的步骤。
	
	实现一个新的系统调用的第一步是决定它的用途。它要做些什么？每个系统调用都应该
	有一个明确的用途。在Linux中不提倡采用多用途的系统调用（一个系统调用通过传递不同的
	参数值来选择完成不同的工作）。ioctl()就应该被视为一个反例。
	
	新系统调用的参数、返回值和错误码又该是什么呢？系统调用的接口应该力求简洁，参
	数尽可能少。系统调用的语义和行为非常关键；因为应用程序依赖于它们，所以它们应力求
	稳定，不作改动。
	
	设计接口的时候要尽量为将来多做考虑。你是不是对函数做了不必要的限制？系统调用
	设计得越通用越好。不要假设这个系统调用现在怎么用将来也一定就是这么用。系统调用的
	目的可能不变，但它的用法却可能改变。这个系统调用可移植吗？别对机器的字节和字
	节序做假设。第16章将讨论这个话题。要确保不对系统调用做错误的假设，否则将来这个调
	用就可能会崩溃。记住Unix的格言：“提供机制而不是策略。”。

	参数验证

	系统调用必须仔细检查它们所有的参数是否合法有效。系统调用在内核空间执行，如果
	任由用户将不合法的输入传递给内核，那么系统的安全和稳定将面临极大的考验。
	
	举例来说，与文件I/O相关的系统调用必须检查文件描述符是否有效。与进程相关的函数
	必须检查提供的PID是否有效。必须检查每个参数，保证它们不但合法有效，而且正确。
	
	最重要的一种检查就是检查用户提供的指针是否有效。试想，如果一个进程可以给内核
	传递指针而又无须被检查，那么它就可以给出一个它根本就没有访问权限的指针，哄骗内核
	去为它拷贝本不允许它访问的数据，如原本属于其他进程的数据。在接收一个用户空间的指
	针之前，内核必须保证：
	*指针指向的内存区域属于用户空间。
	*指针指向的内存区域在进程的地址空间里。
	*如果是读，该内存应被标记为可读。如果是写，该内存应被标记为可写。
	
	内核提供了两个方法来完成必须的检查和内核空间与用户空间之间数据的来回拷贝。注
	意，内核无论何时都不能轻率地接受来自用户空间的指针！这两个方法中必须有一个被调用。
	
	为了向用户空间写入数据，内核提供了copy_to_user()，它需要三个参数。第一个参数是
	进程空间中的目的内存地址。第二个是内核空间内的源地址。最后一个参数是需要拷贝的数
	据长度（字节数）。
	
	为了从用户空间读取数据，内核提供了copy_from_user()，它和copy_to_user()相似。该函
	数把第二个参数指定的位置上的数据拷贝到第一个参数指定的位置上，拷贝的数据长度由第
	三个参数决定。
	
	如果执行失败，这两个函数返回的都是没能完成拷贝的数据的字节数。如果成功，返回0。
	当出现上述错误时，系统调用返回标准-EFAULT。
	
	让我们以一个既用了copy_from_user()又用了copy_to_user()的系统调用作例子进行考察。
	这个系统调用silly_copy()毫无实际用处，它从第一个参数里拷贝数据到第二个参数。这种用
	途让人无法理解，它毫无必要地让内核空间作为中转站，把用户空间的数据从一个位置复制
	到另外一个位置。但它却能演示出上述函数的用法。
	
	asmlinkage long sys_silly_copy(unsigned long *src, unsigned long *dst, unsigned long len)
	{
		unsigned long buf;
		/* 如果内核字长与用户字长不匹配，则失败 */
		if (len != sizeof(buf))
			return -EINVAL;
			
		/* 将用户地址空间中的src拷贝进buf */
		if (copy_from_user(&buf, src, len))
			return -EFAULT;
			
		/* 将buf拷贝进用户地址空间中的dst */
		if (copy_to_user(dst, &buf, len))
			return -EFAULT;
		
		/* 返回拷贝的数据量 */
		return len;
	}
	
	注意，copy_to_user()和copy_from_user()都有可能引起阻塞。当包含用户数据的页被换出
	到硬盘上而不是物理内存上的时候，这种情况就会发生。此时，进程就会休眠，直到缺页
	处理程序将该页从硬盘重新换回物理内存。
	
	最后一项检查针对是否有合法权限。在老版本的Linux内核中，需要超级用户权限的系统
	调用可以通过调用suser()函数这个标准动作来完成检查。这个函数只能检查用户是否是超级
	用户；现在它已经被一个更细粒度的“权能”机制代替。新的系统允许检查针对特定资源的
	特殊权限。调用者可以使用capable()函数来检查是否有权能对指定的资源进行操作，如果它
	返回非零值，调用者就有权进行操作，返回零则无权操作。举个例子，capable(CAP_
	SYS_NICE)可以检查调用者是否有权改变其他进程的nice值。默认情况下，属于超级用户的
	进程拥有所有权利而非超级用户没有任何权利。
	
	
4.5 系统调用上下文

	在第2章中曾经讨论过，内核在执行系统调用的时候处于进程上下文。current指针指向当
	前任务，即引发系统调用的那个进程。
	
	在进程上下文中，内核可以休眠（比如在系统调用阻塞或显式调用schedule()的时候）并
	且可以被抢占。这两点都很重要。首先，能够休眠说明系统调用可以使用内核提供的绝大部
	分功能。在第5章我们会看到，休眠的能力会给内核编程带来极大的便利。在进程上下文中能
	够被抢占其实表明，像用户空间内的进程一样，当前的进程同样可以被其他进程抢占。因为
	新的进程可以使用相同的系统调用，所以必须小心，保证该系统调用是可重入的。当然，这
	也是在对称多处理中必须同样关心的问题。关于可重入的保护涵盖在第7章和第8章中。
	
	当系统调用返回的时候，控制权仍然在sydtem_call()中，它最终会负责切换到用户空间并
	让用户进程继续执行下去。
	
4.5.1 绑定一个系统调用的最后步骤

	当编写完一个系统调用后，把它注册成一个正式的系统调用是件琐碎的工作：
	* 在系统调用表的最后加入一个表项。每种支持该系统调用的硬件体系都必须做这样的工
	  作（大部分的系统调用都针对所有的体系结构）。从0开始，系统调用在该表中的位
	  置就是它的系统调用号。如第10个系统调用分配到的系统调用号为9。

	* 对于所支持的各种体系结构，系统调用号都必须定义于include/asm/unistd.h中。
	
	* 系统调用必须被编译进内核映像（不能被编译成模块）。这只要把它放进kernel/下的一
	  个相关文件中就可以了。
	  
	让我们通过一个虚构的系统调用foo()来仔细观察一下这些步骤。首先，我们要把sys_foo
	加入到系统调用表中去。对于大多数体系机构来说，该表位于entry.s文件中，形式如下：
	
		ENTRY(sys_call_table)
			.long sys_restart syscall        /* 0 */
			.long sys_exit
			.long sys_fork
			.long sys_read
			.long sys_write
			.long sys_open                   /* 5 */
			
			...
			
			.long sys_timer_delete
			.long sys_clock_settime
			.long sys_clock_gettime          /* 265 */
			.long sys_clock_getres
			.long sys_clock_nanosleep
		
		我们把新的系统调用加到这个表的末尾：
			.long sys_foo
			
	虽然没有明确指定编号，但我们加入的这个系统调用被按照次序分配给了268这个系统
	调用号。对于每种需要支持的体系结构，我们都必须将自己的系统调用加入到其系统调用表
	中去（每种体系结构不需要对应相同的系统调用号）。通常，你都会需要让所有的体系结构均
	支持这个系统调用。你可以注意一下每隔5个表项就加入一个调用号注释的习惯，这可以在查
	找系统调用对应的调用号时提供方便。
	
	接下来，我们把系统调用号加入到/asm/unistd.h中，它的格式如下：
		/* 本文包含系统调用号 */
		#define _NR_restart_sys_call		0
		#define _NR_exit					1
		#define _NR_fork					2
		#define _NR_read					3
		#define _NR_write					4
		#define _NR_open					5
		...
		#define _NR_timer_delete			263
		#define _NR_clock_settime			264
		#define _NR_clock_gettime			265
		#define _NR_clock_getres			266
		#define _NR_clock_nanosleep			267

	然后，我们在该列表中加入下面这行：
	#define _NR_foo							268
	
	最后，我们来实现foo()系统调用。无论何种配置，该系统调用都必须编译到核心的内核
	映像中去，所以我们把它放进kernel/sys.c文件中。你也可以将其放到与其功能联系最紧密的
	代码中去，假如它的功能与调度相关，那么你也可以把它放到sched.c中去。
		/* sys_foo - 每个人喜欢的系统调用.
		 * 一个完全无用的函数
		 * 它在每次调用时只返回"out of memory".
    	 */
		 asmlinkage long sys_foo(void)
		 {
			return -ENOMEM;
		 }

		 
4.5.2 从用户空间访问系统调用

	通常，系统调用靠C库支持。用户程序通过包含标准头文件并和C库链接，就可以使用系
	统调用（或者调用库函数，再由库函数实际调用）。但如果你仅仅写出系统调用，glibc库恐怕
	并不提供支持。
	
	值得庆幸的是，Linux本身提供了一组宏，用于直接对系统调用进行访问。它会设置好寄
	存器并调用int $0x80指令。这些宏是_syscalln()，其中n的范围从0到6。代表需要传递给系统
	调用的参数个数，这是由于该宏必须了解到底有多少参数按照什么次序压入寄存器。举个例
	子，open()系统调用的定义是：
		long open(const char *filename, int flags, int mode)
	而不靠库支持，直接调用此系统调用的宏的形式为：
		#define NR_open 	5
		_syscall3(long, open, const char *, filename, int, flags, int, mode)
	这样，应用程序就可以直接使用open()。
	
	对于每个宏来说，都有2+2*n个参数。第一个参数对应着系统调用的返回值类型。第二个
	参数是系统调用的名称。再以后的按照系统调用参数的顺序排列的每个参数的类型和名称。
	_NR_open在<asm/unistd.h>中定义，是系统调用号。该宏会被扩展成为内嵌汇编的C函数；由
	汇编语言执行前一节所讨论的步骤，将系统调用号和参数压入寄存器并触发软中断来陷入内
	核。调用open()系统调用直接把上面的宏放置在应用程序中就可以了。
	
4.5.3 为什么不通过系统调用的方式实现

	建立一个新的系统调用非常容易，但却绝不提倡这么做。通常都会有更好的办法用来代
	替新建一个系统调用以作实现。让我们看看采用系统调用作为实现方式的利弊和替代的方法。
	
	建立一个新的系统调用的好处：
	* 系统调用创建容易且使用方便。
	* Linux系统调用的高性能显而易见。
	问题是：
	* 你需要一个系统调用号，而这需要在一个内核在处于开发版本的时候由官方分配给你。
	* 系统调用被加入稳定内核后就被固化了，为了避免应用程序的崩溃，它的界面不允许做
	  改动。
	* 需要将系统调用分别注册到每个需要支持的体系结构中去。
	* 如果仅仅进行简单的信息交换，系统调用就大材小用了。
	
	替代方法：
	* 创建一个设备节点，通过read()和write()访问它。用ioctl()进行特别的设置操作和获取特
	  别信息。
	* 一些界面如信号量，可以用文件描述符表示以进行操作。像信号量这样的某些接口，可
	  以用文件描述符来表示，因此也就可以按上述方式对其进行操作。
	* 目前倾向于实现一个简单的基于RAM的文件系统，其中的文件表示特殊的界面。应用
	  程序对这些文件进行普通的文件I/O操作来访问这些接口。第11章有相关的细节。
	
	对于许多接口来说，系统调用都被视为正确的解决之道。但Linux系统尽量避免每出现一
	种新的抽象就简单的加入一个新的系统调用。这使得它的系统调用界面简洁得令人叹为观止，
	也就避免了许多后悔和反对意见（系统调用再也不被使用或支持）。
	
	新系统调用增添频率很低也反映出Linux是一个相对较为稳定并且功能已经较为完善的操
	作系统。在2.3和2.5内核开发版中，仅仅加入了少数几个系统调用。而这些新加入的系统
	调用中大部分都是为了提高内核性能。
	
	
*****************************************************************************************
	
								第5章 中断和中断处理程序

*****************************************************************************************
	
5.1 中断
	
	中断使得硬件得以与处理器进行通信。举个例子，在你敲打键盘的时候，键盘控制器
	（控制键盘的硬件设备）会发送一个中断，通知操作系统有按键按下。中断本质上是一种特殊的
	电信号，由硬件设备发向处理器。处理器接收到中断后，会马上向操作系统反映此信号的到
	来，然后就由OS负责处理这些新到来的数据。硬件设备生成中断的时候并不考虑与处理器的
	时钟同步――――换句话说就是中断随时可以产生。因此，内核随时可能因为新到来的中断而被
	打断。
	
	从物理学的角度来看，中断是一种电信号，由硬件设备生成，并直接送入中断控制器的输
	入引脚上。然后再由中断控制器向处理器发送相应的信号。处理器一经检测到此信号，便中
	断自己的当前工作转而处理中断。此后，处理器会通知操作系统已经产生了中断，这样，操作
	系统就可以对这个中断进行适当的处理了。
	
	不同的设备对应的中断不同，而每个中断都通过一个唯一的数字标识。因此，来自键盘
	的中断就有别于来自硬盘的中断，从而使得操作系统能够对中断进行区分，并知道哪个硬件
	设备产生了哪个中断。这样，操作系统才能给不同的中断提供不同的中断处理程序。
	
	这些中断值通常称为中断请求(IRQ)线。通常IRQ都是一些数值量――――例如，在PC上，
	IRQ0是时钟中断，而IRQ1是键盘中断。但并非所有的中断号都是这样严格定义的。例如，对
	于连接在PCI总线上的设备而言，中断是动态分配的。而且其他非PC的体系结构也具有动态
	分配可用中断的特性。重点在于特定的中断总是与特定的设备相关联，并且内核要知道这些
	信息。
	
	异常
	
	讨论中断就不能不提及异常。异常与中断不同，它在产生时必须考虑与处理器
	时钟同步。实际上，异常也常常称为同步中断。在处理器执行到由于编程失误而导
	致的错误指令（例如被0除）的时候，或者是在执行期间出现特殊情况（例如缺页），
	必须靠内核来处理的时候，处理器就会产生一个异常。因为许多处理器体系结构处
	理异常与处理中断方式类似，因此，内核对它们的处理也很类似。
	
	
5.2 中断处理程序

	在响应一个特定中断的时候，内核会执行一个函数，该函数叫做中断处理程序（interrupt
	handler）或中断服务例程（interrupt service routine, ISR）。产生中断的每个设备都有一个
	（中断处理程序通常不是和特定设备关联，而是和特定中断关联的，也就是说，如果一个设备
	可以产生多种不同的中断，那么该设备就可以对应多个中断处理程序，相应的，该设备的驱
	动程序也就需要准备多个这样的函数。――――译者注）相应的中断处理程序。例如，由一个函
	数专门处理来自系统时钟的中断，而另外一个函数专门处理由键盘产生的中断。一个设备的
	中断处理程序是它设备驱动程序（driver）的一部分――――设备驱动程序是用于对设备进行管理
	的内核代码。
	
	在Linux中，中断处理程序看起来就是普普通通的C函数。只不过这些函数必须按照特定
	的类型声明，以便内核能够以标准的方式传递处理程序的信息，在其他方面，它们与一般的
	函数看起来别无二致。中断处理程序与其他内核函数的真正区别在于：中断处理程序是被内
	核调用来响应中断的，而它们运行于我们称之为中断上下文的特殊上下文中。关于中断上下
	文，我们将在后面讨论。
	
	中断可能随时发生，因此中断处理程序也就随时可能执行。所以必须保证中断处理程序
	能够快速执行，这样才能保证尽可能快地恢复中断代码的执行。因此，尽管对硬件而言，迅
	速对其中断进行服务非常重要，但对系统的其他部分而言，让中断处理程序在尽可能短的时
	间内完成运行也同样重要。
	
	即使是最精简版的中断服务程序，它也要与硬件进行交互，告诉该设备中断已被接收。
	但通常我们不能像这样给中断服务程序随意减负，相反，我们要靠它完成大量的其他工作。
	举一个例子，我们可以考虑一下网络设备的中断处理程序面临的挑战。该处理程序除了要对
	硬件应答，还要把来自硬件的网络数据包拷贝到内存，对其进行处理后再交给合适的协议栈
	或应用程序。显而易见，这种工作量不会太小。
	
	上半部与下半部的对比
	
	又想程序运行的快，又想程序完成的工作量多，这两个目的显然有所抵触。鉴于两个目
	的之间存在不可调和的矛盾，所以我们一般把中断处理切为两个部分或两半。中断处理程序
	是上半部（top half）――――接收到一个中断，它就立即开始执行，但只做有严格时限的工作，
	例如对接收的中断进行应答或复位硬件，这些工作都是在所有中断被禁止的情况下完成的。
	能够被允许稍后完成的工作会推迟到下半部（bottom half）去。此后，在合适的时机，下半
	部会被执行。通常情况下，下半部会在中断处理程序返回时立即执行。Linux提供了实现下半
	部的各种机制，第6章会讨论这些机制。
	
	
5.3 注册中断处理程序

	驱动程序可以通过下面的函数注册并激活一个中断处理程序，以便处理中断：
	
	int request_irq(unsigned int irq,
		irqreturn_t (*handler)(int, void *, struct pt_regs *),
		unsigned long irqflags,
		const char *devname,
		void *dev_id)
		
	第一个参数irq表示要分配的中断号。对某些设备，如传统PC设备上的系统时钟或键盘，
	这个值通常是预先定死的。而对于大多数其他设备来说，这个值要么是可以通过探测获取，
	要么可以动态确定。
	
	第二个参数handler是一个指针，指向处理这个中断的实际中断处理程序。只要操作系统
	一接收到中断，该函数就被调用。要注意，handler函数的原型是特定的――――它接受三个参数，
	并有一个类型为irqreturn_t的返回值。我们将在本章随后的部分讨论这个函数。
	
	第三个参数irqflags可以为0，也可能是下列一个或多个标志的位掩码：
	* SA_INTERRUPT: 此标志表明给定的中断处理程序是一个快速中断处理处理程序（fast
	  interrupt handler）。过去，Linux将中断处理程序分为快速和慢速两种。那些可以迅速执
	  行但调用频率可能会很高的中断服务程序，会被贴上这样的标签。通常这样做需要修改
	  中断处理程序的行为，使它们能够尽可能快地执行。现在，加不加此标志的区别只剩下
	  一条了：在本地处理器上，快速中断处理程序在禁止所有中断的情况下运行。这使得快
	  速中断处理程序能够不受其他中断干扰，得以迅速执行。而缺省情况下（没有这个标志），
	  除了正运行的中断处理程序对应的那条中断线被屏蔽外，其他所有的中断都是激活的。除
	  了时钟中断外，绝大多数中断都不使用该标志。
	  
	* SA_SAMPLE_RANDOM:此标志表明这个设备产生的中断对内核熵池（entropy pool）
	  有贡献。内核熵池负责提供从各种随机事件导出的真正随机数。如果指定了该标志，
	  那么来自该设备的中断间隔时间就会作为熵填充到熵池。如果你的设备以预知的速率产
	  生中断（比如系统定时器），或者可能受外部攻击者（例如连网设备）的影响，那么就
	  不要设置这个标志。相反，有其他很多硬件产生中断的速率是不可预知的，所以都能成
	  为一种较好的熵源。关于内核熵池更多的信息，请参考附录C。
	  
	* SA_SHIRQ:此标志表明可以在多个中断处理程序之间共享中断线。在同一个给定线上
	  注册的每个处理程序必须指定这个标志；否则，在每条线上只能有一个处理程序。有关
	  共享中断处理程序的更多信息将在下面的小节中提供。
	  
	第四个参数devname是与中断相关的设备的ASCII文本表示法。例如，PC机上键盘中断
	对应的这个值为"keyboard"。这些名字会被/proc/irq/和/proc/interrupt文件使用，以便与用户
	通信，稍后我们将对此进行简短讨论。
	
	第五个参数dev_id主要用于共享中断线。当一个中断处理程序需要释放时（稍后讨论），
	dev_id将提供唯一的标志信息（cookie），以便从共享中断线的诸多中断处理程序中删除指定
	的那一个。如果没有这个参数，那么内核不可能知道在给定的中断线上到底要删除哪了一个处
	理程序。如果无需共享中断，那么将该参数赋为空值（NULL）就可以了，但是，如果中断
	线是被共享的，那么就必须传递唯一的信息（除非设备位于ISA总线上，那么就必须支持共享
	中断）。另外，内核每次调用中断处理程序时，都会把这个指针传递给它（中断处理程序都是
	预先在内核进行注册的回调函数（callback function），而不同的函数位于不同的驱动程序中，
	所以在这些函数共享同一个中断线时，内核必须准确的为它们创造执行环境，此时就可以通
	过这个指针将有用的环境信息传递给它们了――――译者注）。实践中往往会通过它传递驱动程
	序的设备结构：这个指针是唯一的，而且有可能在中断处理程序内被用到。
	
	request_irq()成功执行会返回0。如果返回非0值，就表示有错误发生，在这种情况下，指
	定的中断处理程序不会被注册。最常见的错误是-EBUSH，它表示给定的中断线已经在使用
	（或者当前用户或者你没有指定SA_SHIRQ）。
	
	注意，request_irq()函数可能会睡眠，因此，不能在中断上下文或其他不允许阻塞的代码
	中调用该函数。天真地认为在睡眠不安全的上下文中可以安全地调用request_irq()函数，是
	一种常见的错误。造成这种错误的部分原因是为什么request_irq()会引起睡眠――――这确实让人费
	解。在注册的过程中，内核需要在/proc/irq文件创建一个与中断对应的项。函数proc_
	mkdir()就是用来创建这个新的procfs项的。proc_makedir()通过调用函数proc_create()对这个新
	的profs项进行设置，而proc_create()会调用函数kmalloc()来请求分配内存。我们在第10章会看
	到，函数kmalloc()是可以睡眠的。看清楚了，你的程序就是跑到那里小憩去了！
	
	请求一个中断线，并在驱动程序中安装中断处理程序：
	if (request_irq(irqn, my_interrupt, SA_SHIRQ, "my_device", dev)) {
		printk(KERN_ERR "my_device:cannot register IRQ %d\n", irqn);
		return -EIO;
	}
	
	在这个例子中，irqn是请求的中断线，my_interrupt是中断处理程序，中断线可以共享，
	设备名为"my_device"，而且我们通过dev_id传递dev结构体。如果请求失败，那么这段代码
	将打印出一个错误并返回。如果调用返回0，则说明处理程序已经成功安装。此后，处理程序
	就会在响应该中断时被调用。有一点很重要，初始化硬件和注册中断处理程序的顺序必须正
	确，以防止中断处理程序在设备初始化完成之前就开始执行。
	
	释放中断处理程序
	
		释放中断线，可以调用
		void free_irq(unsigned int irq, void *dev_id)
		
	如果指定的中断线不是共享的，那么，该函数删除处理程序的同时将禁用这条中断线。
	如果中断线是共享的，则仅删除dev_id所对应的处理程序，而这条中断线本身只有在删除了
	最后一个处理程序时才会被禁用。由此可以看出为什么唯一的dev_id如此重要。对于共享的
	中断线，需要一个唯一的信息来区分其上面的多个处理程序，并通过它来保证函数free_irq()
	能够正确的删除指定的处理程序。不管在哪种情况下（共享或不共享），如果dev_id非空，它
	都必须与需要删除的处理程序相匹配。
	
	必须从进程上下文中调用free_irq()。
	

5.4 编写中断处理程序

	以下是一个典型的中断处理程序声明：
	static irqreturn_t intr_handler(int irq, void *dev_id, struct pt_regs *regs)
	
	注意，它的类型与request_irq()所要求的参数类型是相匹配的。第一个参数irq就是这个处
	理程序要响应的中断的中断线号。如今，这个参数已经没有太大用处了，可能只是在打印日
	志信息时会用到。而在2.0以前的内核中，由于没有dev_id这个参数，必须通过irq才能区分使
	用相同驱动程序因而也使用相同的中断处理程序的多个设备（例如，考虑具有多个硬盘驱动
	控制器的计算机）。
	
	第二个参数dev_id是一个通用指针，它与在中断处理程序注册时传递给request_irq()的参
	数dev_id必须一致。如果该值有唯一确定性（建议采用这样的值，以便支持共享），那么它就
	相当于一个cookie，可以用来区分共享同一中断处理程序的多个设备。另外dev_id也可能指向
	中断处理程序使用的一个数据结构。因为对每个设备而言，设备结构都是唯一的，而且可能
	在中断处理程序中也用得到，因此，也通常会把设备结构传递给dev_id。
	
	最后一个参数regs是一个指向结构的指针，该结构包含处理中断之前处理器的寄存器和状
	态。除了调试的时候，它们很少使用到。
	
	中断处理程序的返回值是一个特殊类型：irqreturn_t。中断处理程序可能返回两个特殊的
	值：IRQ_NONE和IRQ_HANDLED。当中断处理程序检测到一个中断，但该中断对应的设备
	并不是在注册处理函数期间指定的产生源时，返回IRQ_NONE；当中断处理程序被正确调用，
	且确实是它所对应的设备产生了中断时，返回IRQ_HANDLED。另外，也可以使用宏
	IRQ_RETVAL(x)。如果x为非0值，那么该宏返回IRQ_HANDLED；否则，返回IRQ_NONE，
	利用这些特殊的值，内核可以知道设备发出的是否是一种虚假的（未请求）中断。如果给定
	中断线上所有中断处理程序返回的都是IRQ_NONE，那么，内核就可以检测到出了问题。注
	意，irqreturn_t这个返回类型实际上就是一个int型。之所以使用这些特殊值是为了与早期的内
	核保持兼容――――2.6版之前的内核并不支持这种特性，中断处理程序只需返回void就行了。如
	果要在2.4或更早的内核上使用这样的驱动程序，只需简单地将typedef irqreturn_t改为void,
	屏蔽掉此特性就可以了，用不着做什么大的修改。
	
	中断处理程序通常会标记为static，因为它从来不会被别的文件中的代码直接调用。
	
	中断处理程序扮演什么样的角色要取决于产生中断的设备和该设备为什么要发送中断。
	即使其他什么工作也不做，绝大部分的中断处理程序至少需要知道产生中断的设备，告诉它
	已经收到中断了。对于复杂一些的设备，可能还需要在中断处理程序中发送和接收数据，以
	及执行一些扩充的工作。如前所述，应尽可能将扩充的工作推给下半部处理程序，这点将在
	下一章中进行讨论。
	
	
5.4.1 共享的中断处理程序

	共享的处理程序与非共享的处理程序在注册和运行方式上比较相似，但差异主要有以
	下三处：
		* request_irq()的参数flags必须设置SA_SHIRQ标志。
		* 对每个注册的中断处理程序来说，dev_id参数必须唯一。指向任一设备结构的指针就可
		  以满足这一要求；通常会选择设备结构，因为它是唯一的，而且中断处理程序可能会用
		  到它。不能给共享的处理程序传递NULL值。
		* 中断处理程序必须能够区分它的设备是否真的产生了中断。这既需要硬件的支持，也需要
		  处理程序中有相关的处理逻辑。如果硬件不支持这一功能，那中断处理程序肯定会束
		  手无策，它根本没法知道到底是与它对应的设备发出了这个中断，还是共享这条中断线
		  的其他设备发出了这个中断。
	
	所有共享中断线的驱动程序必须满足以上要求。只要有任何一个设备没有按规则进行
	共享，那么中断线就无法共享了。指定SA_SHIRQ标志以调用request_irq()时，只有在以下两
	种情况下才可能成功：中断线当前未被注册，或者在该线上的所有已注册处理程序都指定了
	SA_SHIRQ。注意，在这一点上2.6与以前的内核是不同的，共享的处理程序可以混用SA_
	INTERRUPT。
	
	内核接收一个中断后，它将依次调用在该中断线上注册的每一个处理程序。因此，一个
	处理程序必须知道它是否应该为这个中断负责。如果与它相关的设备并没有产生中断，那么
	处理程序应该立即退出。这需要硬件设备提供状态寄存器（或类似机制），以便中断处理程序
	进行检查。毫无疑问，大多数硬件都提供这种功能。
	
	
5.4.2 中断处理程序实例

	让我们考察一个实际的中断处理程序，它来自RTC（real_time clock）驱动程序，可以在
	driver/char/rtc.c中找到。很多机器（包括PC）都可以找到RTC。它是一个从系统定时器中独
	立出来的设备，用于设置系统时钟，提供报警器（alarm）或周期性的定时器。对系统时钟的
	设置，通常只需要向某个特定的寄存器或某个I/O地址上写入一些数据就可以完成。然而报警
	器或周期性定时器通常就得靠中断来实现。这种中断与生活中的闹铃差不多：中断发出时，
	报警器或定时器就会启动。
	
	RTC驱动程序装载时，rtc_init()函数会被调用，对这个驱动程序进行初始化。它的职责之
	一就是注册中断处理程序:
		if (request_irq(RTC_IRQ, rtc_interrupt, SA_INTERRUPT, "rtc", NULL) {
			printk(KERN_ERR "rtc:cannot register IRQ %d\n", rtc_irq);
			return -EIO；
		}
		
	从中我们可以看到，中断线号由RTC_IRQ指定。这是一个预处理定义，为给定体系结构指定
	RTC中断。例如，在PC上，RTC总是位于IRQ8。第二个参数是我们的中断处理程序
	rtc_interrupt，在它执行时要禁止所有的中断，这要归功于SA_INTERRUPT标志。从第四个参
	数我们看到，驱动程序的名称为"rtc"。因为这个设备不允许共享中断线，且处理程序没有用
	到什么特殊的值，因此给dev_id的值是NULL。
	
	最后要展示的是处理程序本身：
	
	static irqreturn_t rtc_interrupt(int irq, void *dev_id, struct pt_regs *regs)
	{
		/* 
		 * 可以是报警器中断、更新近完成的中断或周期性中断.
		 * 我们把状态保存在rtc_irq_data的低字节中.
		 * 而把从最后一次读之后所接收的中断号保存在其余字节中
		 */
		 
		spin_lock (&rtc_lock);
		...
		rtc_irq_data += 0x100;
		rtc_irq_data &= ~0xff;
		rtc_irq_data |= (CMOS_READ(RTC_INTR_FLAGS) & 0xF0);
		 
		if (rtc_status & RTC_TIMER_ON)
			mod_timer(&rtc_irq_timer, jiffies + HZ/rtc_freq + 2*HZ/100);
			
		spin_unlock (&rtc_lock);
		
		/*
		 * 现在执行其余的操作
		 */
		 
		spin_lock(&rtc_task_lock);
		if (rtc_callback)
			rtc_callback->func(rtc_callback->private_data);
		spin_unlock(&rtc_task_lock);
		wake_up_interruptible(&rtc_wait);
		
		kill_fasync (&rtc_async_queue, SIGIO, POLL_IN);
		
		return IRQ_HANDLED;
	}
	
	只要计算机一接收到RTC中断，就会调用这个函数。首先要注意的是使用了自旋锁――――
	第一次调用是为了保证rtc_irq_data不被SMP机器上的其他处理器同时访问，第二次调用避免
	rtc_callback出现相同的情况。锁机制在第8章中进行讨论。
	
	rtc_irq_data存放有关RTC的信息，每次中断时都会更新以反映中断的状态。
	
	接下来，如果设置了RTC周期性定时器，就要通过函数mod_timer()对其更新。定时器在
	第9章进行讨论。
	
	代码的最后一部分要通过设置自旋锁进行保护，它会执行一个可能被预先设置好的回调
	函数。RTC驱动程序允许注册一个回调函数，并在每个RTC中断到来时执行。
	
	最后，这个函数会返回IRQ_HANDLED，表明已经正确地完成了对此设备的操作。因为
	这个中断处理程序不支持共享，而且RTC也没有什么用来测试虚假中断的机制，所以该处理
	程序总是返回IQR_HANDLED。
	
	
5.4.3 中断上下文

	当执行一个中断处理程序或下半部时，内核处于中断上下文（interrupt context）中。让
	我们回忆一下进程上下文。进程上下文是一种内核所处的操作模式，此时内核代表进程执
	行――――例如，执行系统调用或运行内核线程。在进程上下文中，可以通过current宏关联当前
	进程。此外，因为进程是以进程上下文的形式连接到内核中的，因此，进程上下文可以睡
	眠，也可以调用调度程序。

	与之正反，中断上下文和进程并没有什么瓜葛。与current宏也是不想干的（尽管它会指
	向被中断的进程）。因为没有进程的背景，所以中断上下文不可以睡眠――――否则又怎能再对它
	重新调度呢？因此，不能从中断上下文中调用某些函数。如果一个函数睡眠，就不能在你的
	中断处理程序中使用它――――这是对什么样的函数可以在中断处理程序中使用的限制。
	
	中断上下文具有较为严格的时间限制，因为它打断了其他代码。中断上下文中的代码应
	当迅速简洁，尽量不要使用循环去处理繁重的工作。有一点非常重要，请永远牢记：中断处
	理程序打断了其他代码（甚至可能是打断了在其他中断线上的另一中断处理程序）。正是因
	为这种异步执行的特性，所以所有的中断处理程序必须尽可能的迅速、简洁。尽量把工作从
	中断处理程序中分离出来，放在下半部来执行，因为下半部可以在更合适的时间运行。
	
	最后，中断处理程序并不具有自己的栈。相反，它共享被中断进程的内核栈。如果没有
	正在运行的进程，它就使用idle进程的栈。因为中断处理程序共享别人的堆栈，所以它们在栈
	中获取空间时必须非常节省。当然，内核栈本来就很有限，因此，所有的内核代码都应该
	谨慎利用它。
	
	
5.5 中断处理机制的实现

	中断处理系统在Linux中的实现是非常依赖于体系结构的，想必你对此不会感到特别惊讶。
	实现依赖于处理器、所使用的中断控制器的类型、体系结构的设计及机器本身。

	图5-1是中断从硬件到内核的路由。设备产生中断，通过总线把电信号发送给中断控制器。
	如果中断线是激活的（它们是允许被屏蔽的），那么中断控制器就会把中断发往处理器。在大
	多数体系结构中，这个工作就是给处理器的特定管脚发送一个信号。除非在处理器上禁止该
	中断，否则，处理器会立即停止它正在做的事，关闭中断系统，然后跳到内存中预定义的位
	置开始执行那里的代码。这个预定义的位置是由内核设置的，是中断处理程序的入口点。
	
	在内核中，中断的旅程开始于预定义入口点，这类似于系统调用。对于每条中断线，处
	理器都会跳到对应的一个唯一的位置。这样，内核就可知道所接收中断的IRQ号了。初始入口
	点只是在栈中保存这个号，并存放当前寄存器的值（这些值属于被中断的任务）；然后，内
	核调用函数do_IRQ()。从这里开始，大多数中断处理代码是用C写的――――但它们依然与体系结
	构相关。
	
	do_IRQ()的声明如下：
		unsigned int do_IRQ(struct pt_regs regs)
	因为C的调用惯例是要把函数参数放在栈的顶部，因此pt_regs结构包含原始寄存器的值，
	这些值是以前在汇编入口例程中保存在栈中的。中断的值也会得以保存，所以，do_IRQ()可
	以将它提取出来。X86的代码为：
		int irq=regs.orig_eax & 0xff;
		
	计算出中断号后，do_IRQ()对所接收的中断进行应答，禁止这条线上的中断传递。在普
	通的PC机器上，这些操作是由mask_and_ack_8259A()来完成，该函数由do_IRQ()调用。
	
	接下来，do_IRQ()需要确保在这条中断线上有一个有效的处理程序，而且这个程序已经
	启动但是当前并没有执行。如果这样的话，do_IRQ()就调用handle_IRQ_event()来运行为这条
	中断线所安装的中断处理程序。在X86上，handle_IRQ_event()为：
		int handle_IRQ_event(unsigned int irq, struct pt_regs *regs, struct irqaction
		*action)
		{
			int status = 1;
			
			if (!(action->flags & SA_INTERRUPT))
				local_irq_enable();
			
			do {
				status |= action->flags;
				action->handler(irq, action->dev_id, regs);
				action = action->next;
			} while (action);
			
			if (status & SA_SAMPLE_RANDOM)
				add_interrupt_randomness(irq);
			local_irq_disable();
			
			return status;
		}
		
	首先，因为处理器禁止中断，这里要把它们打开，就必须在处理程序注册期间指定SA_
	INTERRUPT标志。回想一下，SA_INTERRUPT表示处理程序必须在中断禁止的情况下运行。
	接下来，每个潜在的处理程序在循环中依次执行。如果这条线不是共享的，第一次执行后就
	退出循环。否则，所有的处理程序都要被执行。之后，如果在注册期间指定了SA_SAMPLE_
	RANDOM标志，则还要调用函数add_interrupt_randomness()。这个函数使用中断间隔时间为
	随机数产生器产生熵。在附录C中有更多关于内核随机数产生器的信息。最后，再将中断禁止
	（do_IRQ()期望中断一直是禁止的），函数返回。回到do_IRQ(),该函数做清理工作并返回到
	初始入口点，然后再从这个入口点跳到函数ret_from_intr()。
		
	ret_from_intr()例程类似于初始入口代码，以汇编编写。这个例程检查重新调度是否正在
	挂起（回想一下第3章，这意味着设置了need_resched）。如果重新调度正在挂起，而且内核正
	在返回用户空间（也就是说，中断了用户进程），那么schedule()被调用。如果内核正在返
	回内核空间（也就是说，中断了内核本身），只有在preempt_count为0时，schedule()才会被调
	用（否则，抢占内核便是不安全的）。在schedule()返回之后，或者如果没有挂起的工作，那
	么，原来的寄存器被恢复，内核恢复到曾经中断的点。
		
	在X86上，初始的汇编例程位于arch/i386/kernel/entry.S，C方法位于arch/i386/
	kernel/irq.c。其他所支持的结构与此类似。
		
	/proc/interrupts
		
	procfs是一个虚拟文件系统，它只存在于内核内存，一般安装于/proc目录。在procfs中读
	写文件都要调用内核函数，这些函数模拟从真实文件中读或写。与此相关的例子是
	/proc/interrupts文件，该文件存放的是系统中与中断相关的统计信息。下面是从单处理器PC上
	输出的信息：
				CUP0
			0:		3602371		XT-PIC		timer
			1:		3048		XT-PIC		i8042
			2:		0			XT_PIC		cascade
			4:		2689466		XT_PIC		uhci-hcd,eth0
			5:		0			XT-PIC		EMU10K1
			12:		85077		XT-PIC		uhci-hcd
			15:		24571		XT-PIC		aic7xxx
			NMI:	0
			LOC:	3602236
			ERR:	0
		
	第一列是中断线。在这个系统中，现有的中断号为0~2、4、5、12及15。这里没有显示没
	有安装处理程序的中断线。第2列是一个接收中断数目的计数器。事实上，系统中的每个处理
	器都存在这样的列，但是，这个机器只有一个处理器。我们看到，时钟中断已接收3602371次
	中断，这里，声卡（EMU10K1）没有接收一次中断（这表示机器启动以来还没有使用它）。
	第3列是处理这个中断的中断控制器。XT-PIC对应于标准的可编程中断控制器，在具有I/O
	APIC的系统上，大多数中断会列出IO-APIC-level或IO-APIC-edge，作为自己的中断控制器。
	最后一列是与这个中断相关的设备名字。这个名字是通过参数devname提供给函数request_irq()
	的，前面已讨论过了。如果中断是共享的（例子中的4号中断就是这种情况），则这条中断线
	上注册的所有设备都会列出来 。
		
	对于想深入探究profs内部的人来说，profs代码位于fs/proc中。不必惊讶，提供/proc/
	interrupts的函数是与体系结构相关的，叫做show_interrupts()。
	
	
5.6 中断控制

	Linux内核提供了一组接口用于操作机器上的中断状态。这些接口为我们提供了能够禁止
	当前处理器的中断系统，或屏蔽掉整个机器的一条中断线的能力，这些例程都是与体系结构
	相关的，可以在<asm/system.h>和<asm/irq.h>中找到。表5-1是接口的完整列表。
	
	一般来说，控制中断系统的原因归根结底是需要提供同步。通过禁止中断，可以确保某
	个中断处理程序不会抢占当前的代码。此外，禁止中断还可以禁止内核抢占。然而，不管是
	禁止中断还是禁止内核抢占，都没有提供任何保护机制来防止来自其他处理器的并发访问。
	Linux支持多处理器，因此，内核代码一般都需要获取某种锁，防止来自其他处理器对共享数
	据的并发访问。获取这些锁的同时也伴随着禁止本地中断。锁提供保护机制，防止来自其他
	处理器的并发访问，而禁止中断提供保护机制，则是防止来自其他中断处理程序的并发访问。
	第7章和第8章着重讨论同步的各种问题及其对策。
	
	因此 ，必须理解内核中断的控制接口。


5.6.1 禁止和激活中断

	用于禁止当前处理器（仅仅是当前处理器）上的本地中断，随后又激活它们的语句为：
	local_irq_disable();
	local_irq_enable();
	
	这两个函数通常以单个汇编指令来实现（当然，这依赖于体系结构）。实际上，在X86中，
	local_irq_discable()仅仅是cli指令，而local_irq_enable()只不过是sti指令。对于非X86的黑客而
	言，cli和sti分别是对clear和set允许中断（allow interrupts）标志的汇编调用。换句话说，在发
	出中断的处理器上，它们将禁止和激活中断传递。
	
	如果在调用local_irq_disable()例程之前已经禁止了中断，那么该例程往往会带来潜在的
	危险；同样相应的local_irq_enable()例程也存在潜在危险，因为它将无条件地激活中断，尽管
	这些中断可能在开始时就是关闭的。所以我们需要一种机制把中断恢复到以前的状态而不是
	简单地禁止或激活。内核普遍关心这点，是因为内核中一个给定的代码路径既可以在中断激
	活的情况下达到，也可以在中断禁止的情况下达到，这取决于具体的调用链。例如，想象一
	下前面的代码片段是一个大函数的组成部分。这个函数被另外两个函数调用：其中一个函数
	禁止中断，而另一个函数不禁止中断。因为随着内核的不断增长，要想知道到达这个函数的
	所有代码路径将变得越来越困难，因此，在禁止中断之前保存中断系统的状态会更加安全一
	些。相反，在准备激活中断时，只需把中断恢复到它们原来的状态。

		unsigned long flags;
		local_irq_save(flags);
		/* 禁止中断 */
		local_irq_restore(flags);  /* 中断被恢复到它们原来的状态 */

	这些方法至少部分要以宏的形式实现，因此表面上flags参数是以值传递的。该参数包含具体
	体系结构的数据，也就是包含中断系统的状态。至少有一种体系结构把栈信息与值相结合
	(SPARC)，因此flags不能传递给另一个函数（换句话说，它必须驻留在同一栈帧中）。基于这
	个原因，对local_irq_save()的调用和对local_irq_restore()的调用必须在同一个函数中进行。
	
	前面的所有函数既可以在中断中调用，也可以在进程上下文中调用。


5.6.2 禁止指定中断线

	在前一节中，我们看到了禁止整个处理器上所有中断的函数。在某些情况下，只禁止整个
	系统中一条特定的中断线就够了。这就是所谓的屏蔽掉(masking out)一条中断线。作为例
	子，你可能想在对中断的状态操作之前禁止设备中断的传递。为此，Linux提供了四个接口：
	
		void disable_irq(unsigned int irq);
		void disable_irq_nosync(unsigned int irq);
		void enable_irq(unsigned int irq);
		void synchronize_irq(unsigned int irq);
		
	前面两个函数禁止中断控制器上指定的中断线，即禁止给定中断向系统中所有处理器的传
	递。另外，函数只有在当前正在执行的所有处理程序完成后，disable_irq()才能返回。因此，
	调用者不仅要确保不在指定线上传递新的中断，同时还要确保所有已经开始执行的处理程序
	已全部退出。函数disable_irq_nosync()不具有这种特性。
	
	函数synchronize_irq()等待一个特定的中断处理程序的退出。如果该处理程序正在执行，
	那么该函数必须退出后才能返回。
	
	对这些函数的调用可以嵌套。但要记住在一条指定的中断线上，对disable_irq()或disable_
	irq_nosync()的每次调用，都需要相应地调用一次enable_irq()。只有在对enable_irq()完成最后
	一次调用后，才真正重新激活了中断线。例如，如果disable_irq()被调用了再次，那么直到第
	二次调用enable_irq()后，才能真正地激活中断线。

	其中有三个函数可以从中断或进程上下文中调用，而且不会睡眠。但如果从中断上下文
	中调用，就要特别小心！例如，当你正在处理一条中断线时，并不想激活它（回想某个处
	理程序的中断线正在被处理时，它被屏蔽掉）。
	
	禁止多个中断处理程序共享的中断线是不合适的。禁止中断线也就禁止了这条线上所有
	设备的中断传递。因此，用于新设备的驱动程序应该倾向于不使用这些接口。根据规范，
	PCI设备必须支持中断线共享，因此，它们根本不应该使用这些接口。所以，disable_irq()及
	其相关函数在老式的传统设备（如PC并口）中更容易被找到。
	
	
5.6.3 中断系统的状态
	
	通常有必要了解中断系统的状态（例如中断是禁止的还是激活的），或者你当前是否处
	于中断上下文的执行状态中。中断控制方法请见表5-1。
	
	宏irqs_disable()定义在<asm/system.h>中。如果本地处理器上的中断系统被禁止，则它返
	回非0，否则，返回0。
	
	在<asm/hardirq.h>中定义的两个宏提供一个用来检查内核的当前上下文的接口，它们是：
		in_interrupt()
		in_irq()
	第一个宏最有用：如果内核处于中断上下文中，它返回非0。说明内核此刻正在执行中断
	处理程序，或者正在执行下半部处理程序。宏in_irq()只有在内核确实正在执行中断处理程序
	时才返回非0；
	
	通常情况下，你要检查自己是否处于进程上下文中。也就是说，你希望确保自己不在中
	断上下文中。这种情况很常见，因为代码要做一些像睡眠这样只能从进程上下文中做的事。
	如果in_interrupt()返回0，则此刻内核处于进程上下文。
	
								表5-1  中断控制方法列表
								
		local_irq_disable()							禁止本地中断传递
		local_irq_enable()							激活本地中断传递
		local_irq_save(unsigned long flags)			保存本地中断传递的当前状态，然后禁止
												本地中断传递
		local_irq_restore(unsigned long flags)		恢复本地中断传递到给定的状态
		disable_irq(unsigned int irq)				禁止给定中断线，并确保该函数返回之前在该中
												断线上没有处理程序在运行
		disable_irq_nosync(unsigned int irq)		禁止给定中断线	
		enable_irq(unsigned int irq)				激活给定中断线
		irqs_disabled()								如果本地中断传递被禁止，则返回非0；否则
												返回0
		in_interrupt()								如果在中断上下文中，则返回非0，如果在
												进程上下文中，则返回0
		in_irq()									如果当前正在执行中断处理程序，则返回非0，
												否则，返回0


												
												
*****************************************************************************************										

								第6章 下半部和推后执行的工作

*****************************************************************************************


	在上一章中，我们研究了内核为处理中断而提供的中断处理程序机制。中断处理程序是
	内核中很有用的――――实际上也是必不可少的一部分。但是，由于本身存在一些局限，所以
	它只能完成整个中断处理流程的上半部分。这些局限包括：
	
		* 中断处理程序以异步方式执行并且它有可能会打断其他重要代码（甚至包括其他中断处
		  理程序）的执行。因此，它们应该执行得越快越好。
		* 如果当前有一个中断处理程序正在执行，在最好的情况下，与该中断同级的其他中断会
		  被屏蔽，在最坏的情况下，所有其他中断都会被屏蔽。因此，仍应该让它们执行的越快
		  越好。
		* 由于中断处理程序往往需要对硬件进行操作，所以它们通常有很高的时限要求。
		* 中断处理程序不在进程上下文中运行，所以它们不能阻塞。
	
	现在，为什么中断处理程序只能作为整个硬件中断处理流程一部分的原因就很明显了。
	我们必须有一个快速、异步、简单的处理程序负责对硬件做出迅速响应并完成那些时间要求
	很严格的操作。中断处理程序很适合于实现这些功能，可是，对于那些其他的、对时间要求
	相对宽松的任务，就应该推后到中断被激活以后再去运行。
	
	这样，整个中断处理流程就被分为了两个部分，或叫两半。第一个部分是中断处理程序
	（上半部），就像我们在上一章讨论的那样，内核通过对它的异步执行完成对硬件中断的即时
	响应。在本章中，我们要研究的是中断处理流程中的另外那一部分，下半部(bottom halves)。
	
	
6.1 下半部

	下半部的任务就是执行与中断处理密切相关但中断处理程序本身不执行的工作。在理想
	的情况下，最好是中断处理程序将所有工作都交给下半部分执行，因为我们希望在中断处理
	程序中完成的工作越少越好（也就是越快越好）。我们期望中断处理程序能够尽可能快地返回。
	
	但是，中断处理程序注定要完成一部分工作。例如，中断处理程序几乎都需要通过操作
	硬件对中断的到达进行确认。有时它还会从硬件拷贝数据。因为这些工作对时间非常敏感，
	所以只能靠中断处理程序自己去完成。
	
	剩下的几乎所有其他工作都是下半部执行的目标。例如，如果你在上半部中把数据从硬
	件拷贝到了内存，那么当然应该在下半部中处理它们。遗憾的是，并不存在严格明确的规定
	来说明到底什么任务应该在哪个部分中完成――――如何决定完全取决于驱动程序开发者自己
	的判断。尽管在理论上不存在什么错误，但轻率的实现效果往往不很理想。记住，中断处理
	程序会异步执行，并且在最好的情况下它也会锁定当前的中断线。因此将中断处理程序持续
	执行的时间缩短到最小程度显得非常重要。对于在上半部和下半部之间划分工作，尽管不存
	在某种严格的规则，但还是有一些提示可供借鉴：
		* 如果一个任务对时间非常敏感，将其放在中断处理程序中执行。
		* 如果一个任务和硬件相关，将其放在中断处理程序中执行。
		* 如果一个任务要保证不被其他中断（特别是相同的中断）打断，将其放在中断处理程序
		  中执行
		* 其他所有任务，考虑放置在下半部执行。
		
	当你开始尝试写自己的驱动程序的时候，读一下别人的的中断处理程序和相应的下半部可
	能会让你受益匪浅。在决定怎样把你的中断处理流程中的工作划分到上半部和下半部中去的时
	候，问问自己什么必须放进上半部而什么可以放进下半部。通常，中断处理程序要执行得越
	快越好。
	
6.1.1 为什么要用下半部

	理解为什么要让工作推后执行以及在什么时候推后执行非常关键。你希望尽量减少中断
	处理程序中需要完成的工作量，因为在它运行的时候当前的中断线会被屏。更糟糕的是如
	果一个处理程序是SA_INTERRUPT类型，它执行的时候会禁止所有本地中断（而且把本地中
	断线全局地屏蔽掉）。而缩短中断被屏蔽的时间对系统的响应能力和性能都至关重要。再加上
	中断处理程序要与其他程序――――甚至是其他的中断处理程序――――异步执行，所以很明显，我
	们必须尽力缩短中断处理程序的执行。解决的方法就是把一些工作放到以后去做。
	
	但具体放到以后的什么时候去做呢？在这里，以后仅仅用来强调不是马上而已，理解这
	一点相当重要。下半部并不需要指明一个确切时间，只要把这些任务推迟一点，让它们在系
	统不太繁忙并且中断恢复后执行就可以了。通常下半部在中断处理程序一返回就会马上运行。
	下半部执行的关键在于当它们运行的时候，允许响应所有的中断。
	
	不仅仅是Linux，许多操作系统也把处理硬件中断的过程分为两个部分。上半部分简单快
	速，执行的时候禁止一些或者全部中断。下半部分（无论具体如何实现）稍后执行，而且执
	行期间可以响应所有的中断。这种设计可使系统处于中断屏蔽状态的时间尽可能的短，以此
	来提高系统的响应能力。
	
	
6.1.2 下半部的环境

	和上半部分只能通过中断处理程序实现不同，下半部可以通过多种机制实现。这些用来
	实现下半部的机制分别由不同的接口和子系统组成。上一章，我们了解到实现中断处理程序
	的方法只有一种（在Linux中，由于上半部从来都只能通过中断处理程序实现，所以它和中断
	处理程序可以说是等价的。――――译者注）但在本章中你会发现，实现一个下半部会有许多不
	同的方法。实际上，在Linux发展的过程中曾经出现过多种下半部机制。让人倍受困扰的是，
	其中不少机制名字起得很相像，甚至还有一些机制名字起得辞不达意。

	在本章中，我们将要讨论2.6版本的内核中的下半部机制是如何设计和实现的。同时我们
	也会讨论怎么在你自己编写的内核代码中使用它们。而那些过去使用的，已经废除了有一段
	时间的机制，由于曾经闻名遐尔，所以在相关的时候我们还是会有所提及。
	
	最早的Linux只提供"bottom half"这种机制用于实现下半部。这个名字在那个时候毫无
	异议，因为当时它是将工作推后的唯一方法。这种机制也被称为"BH"，我们现在也这么叫
	它，以避免和“下半部”这个通用词汇混淆。像过往的那段美好岁月中的许多东西一样，BH
	接口也非常简单。它提供了一个静态创建、由32个bottom half组成的链表。上半部通过一个
	32位整数中的一位来标识出哪个bottom half可以执行，每个BH都在全局范围内进行同步。即
	使分属于不同的处理器，也不允许任何两个bottom half同时执行。这种机制使用方便却不够
	灵活，简单却有性能瓶颈。
	
	不久，内核开发者们就引入了任务队列（task queues）机制来实现工作的推后执行，并用
	它来代替BH机制。内核为此定义了一组队列。其中每个队列都包含一个由等待调用的函数组
	成链表。根据其所处队列的位置，这些函数会在某个时刻被执行。驱动程序可以把它们自己
	的下半部注册到合适的队列上去。这种机制表现的还不错，但仍不够灵活，没法代替整个BH
	接口。对于一些性能要求较高的子系统，像网络部分，它也不能胜任。
	
	在2.3这个开发版本中，内核开发者引入了软中断（softirqs）（这里的软中断与第4章实现
	系统调用所提到的软中断（准确说该叫它软件中断）指的是不同的概念――――译者注）和tasklet。
	
	......
	
	在开发2.5版本的内核时，BH接口最终被弃置了，所有的BH使用者必须转而使用其他下
	半部接口。此外，任务队列接口也被工作队列接口取代了。工作队列是一种简单但很有用的
	方法，它们先对要推后执行的工作排队，稍后在进程上下文中执行它们。

	综上所述，在2.6这个当前版本中，内核提供了三种不同形式的下半部实现机制：软中断、
	tasklets和工作队列。内核过去曾经用过的BH和任务队列接口，现在已经被湮没在记忆中了。
	
	混乱的下半部概念
	
	这些东西确实把人搅得很混乱，但它们其实只不过是一些起名的问题，我们再来梳理一遍。
	
	“下半部（bottom half）”是一个操作系统通用词汇，用于指代中断处理流程中推后执行的
	那一部分。在Linux中，这个词目前确实就是这个含意。所有用于实现将工作推后执行的内核
	机制都被称为“下半部机制”。一些人错误地把所有的下半部机制都叫做“软中断”，真是在
	自寻烦恼。
	
	“下半部”这个词也指代Linux最早提供的那种将工作推后执行的实现机制。由于该机制
	也被叫做“BH”，所以，我们就使用它的这个名称，而“下半部”这个词仍然保持它通常
	的含义。BH机制很早以前就被反对使用了，在2.5版内核中，它被完全去除了。
	
	当前，有三种机制可以用来实现将工作推后执行：软中断、tasklet和工作队列。Tasklets
	通过软中断实现，而工作队列与它们完全不同。
	
	在搞清楚了这些混乱的命名之后，让我们开始具体研究各个机制。
	
	
6.2 软中断

	我们的讨论从实际的下半部实现――――软中断――――方法开始。软中断使用得比较少；而
	tasklets是下半部更常用的一种形式。但是，由于tasklets是通过软中断实现的，所以我们先来
	研究软中断。软中断的代码位于kernel/softirq.c文件中。

6.2.1 软中断的实现

	软中断是在编译期间静态分配的。它不像tasklets那样能被动态地注册或去除。软中断由
	softirq_action结构表示，它定义在<linux/interrupt.h>中：
		/* 本结构代表一个软中断项 */
		struct softirq_action
		{
			void (*action)(struct softirq_action *);    /* 待执行的函数 */
			void *data;      /* 传给函数的参数 */
		};

	kernel/softirq.c中定义了一个包含有32个该结构体的数组。
	static struct softirq_action softirq_vec[32];
	
	每个被注册的软中断都占据该数组的一项。因此最多可能有32个软中断。注意，这是一
	个定值――――注册的软中断数目的最大值没法动态改变。在当前版本的内核中，这32个项中只
	用到6个。
	
	1.软中断处理程序
	
	软中断处理程序action的函数原型如下：
	
	void softirq_handler(struct softirq_action *)
	
	当内核运行一个软中断处理程序的时候，它就会执行这个action函数，其唯一的参数为指
	向相应softirq_action结构体的指针。例如，如果my_softirq指向softirq_vec数组的某项，那么
	内核会用如下的方式调用软中断处理程序中的函数：
	
	my_softirq->action(my_softirq)
	
	当你看到内核把整个结构体都传递给软中断处理程序而不是仅仅传data的时候，你可能会
	很吃惊。这个小技巧可以保证将来在结构体中加入新的域时，无须对所有的软中断处理程序
	都进行变动。如果需要，软中断处理程序可以方便地解析它的参数，从数据成员中提取数值。
	
	一个软中断不会抢占另外一个软中断。实际上，唯一可以抢占软中断的是中断处理程序。
	不过，其他的软中断――――甚至是相同类型的软中断――――可以在其他处理器上同时执行。
	
	2. 执行软中断
	
	一个注册的软中断必须在被标记后才会执行。这被称作触发软中断（raising the softirq），
	通常，中断处理程序会在返回前标记它的软中断，使其在稍后被执行。于是，在合适的时刻，
	该软中断就会运行。在下列地方，待处理的软中断会被检查和执行：
	
		* 在处理完一个硬件中断以后。
		* 在ksoftirqd内核线程中。
		* 在那些显式检查和执行待处理的软中断的代码中，如网络子系统中。
		
	不管是用什么办法唤起，软中断都要在do_softirq()中执行。该函数很简单。如果有待处
	理的软中断，do_softirq()会循环遍历每一个，调用它们的处理程序。让我们观察一下
	do_softirq()经过简化后的核心部分：
		
		u32 pending = softirq_pending(cpu);
		if (pending) {
			struct softirq_action *h = softirq_vec;
			softirq_pending(cpu) = 0;
			do {
				if (pending & 1)
					h->action(h);
				h++;
				pending >> = 1;
			} while(pending);
		}

	以上摘录的是软中断处理的核心部分。它检查并执行所有待处理的软中断，具体要做的
	包括:
		* 用局部变量pending保存softirq_pending()宏的返回值。它是待处理的软中断的32位位
		  图――――如果第n位被设置为1，那么第n位对应类型的软中断等待处理。
		* 现在待处理的软中断位图已经被保存，可以将实际的软中断位图清零了。
		* 将指针h指向softirq_vec的第一项。
		* 如果pending的第一位被置为1，h->action(h)被调用。
		* 指针加１，所以现在它指向softirq_vec数组的第二项。
		* bitmask_pending右移一位。这样会丢弃第一位，然后让其他各位依次向右移动一个位置。
		  于是，原来的第二位现在就在第一位的位置上了（依次类推）。
		* 现在指针h指向数组的第二项，pending_mask的第二位现在也到了第一位上。重复执行
		  上面的步骤。
		* 一直重复下去，直到pending变为0，这表明已经没有待处理的软中断了，我们的任务也
		  就完成了。注意，这种检查足以保证h总指向softirq_vec的有效项了，因为pending最多只
		  可能设置32位，循环最多也只能执行32次。
		  

6.2.2 使用软中断

	软中断保留给系统中对时间要求最严格以及最重要的下半部使用。目前，只有两个子系
	统――――网络和SCSI――――直接使用软中断。此外，内核定时器和tasklets都是建立在软中断上的。
	如果你想加入一个新的软中断，首先应该问问自己为什么用tasklet实现不了。Tasklets可以动
	态生成，由于它们对加锁的要求不高，所以使用起来也很方便，而且它们的性能也非常不错。
	当然，对于时间要求严格并能自己高效地完成加锁工作的应用，软中断会是正确的选择。
	
	1. 分配索引
	
	在编译期间，你通过<linux/interrupt.h>中定义的一个枚举类型来静态地声明软中断。内
	核用这些从0开始的索引来表示一种相对优先级。索引号小的软中断在索引号大的软中断之前
	执行。
	
	建立一个新的软中断必须在此枚举类型中加入新的项。而加入时，你不能像在其他地方
	一样，简单地把新项加到列表的末尾。相反，你必须根据你希望赋予它的优先级来决定加入
	的位置。习惯上，HI_SOFTIRQ通常作为第一项，而TASKLET_SOFTIRQ作为最后一项。新
	项可能插在网络相关的那些项之后、TASKLET_SOFTIRQ之前。

	2. 注册你的处理程序
	
	接着，在运行时通过调用open_softirq()注册软中断处理程序，该函数有三个参数：软中
	断的索引号、处理函数和data域存放的数值。例如网络子系统，通过以下方式注册自己的软中断：
		open_softirq(NET_TX_SOFTIRQ, net_tx_action, NULL);
		open_softirq(NET_RX_SOFTIRQ, net_rx_action, NULL);
	软中断处理程序执行的时候，允许响应中断，但它自己不能休眠。在一个处理程序运行
	的时候，当前处理器上的软中断被禁止。但其他的处理器仍可以执行别的软中断。实际上，
	如果一个软中断在它被执行的同时再次被触发了，那么另外一个处理器可以同时运行其处理
	程序。这意味着任何共享数据――――甚至是仅在软中断处理程序内部使用的全局变量――――都需
	要严格的锁保护（下面两个章节会讨论）。这点很重要，它也是为什么tasklets更受青睐的原因。
	单纯地禁止你的软中断处理程序同时执行不是很理想。如果仅仅通过互斥的加锁方式来防止
	它自身的并发执行，那么使用软中断就没有任何意义。因此，大部分软中断处理程序都通过
	采取单处理器（仅属于某一个处理器的数据，因此根本不需要加锁）或其他一些技巧来
	避免显式地加锁，从而提供更出色的性能。

	3. 触发你的软中断
		
	通过在枚举类型的列表中添加新项以及调用open_softirq()进行注册以后，新的软中断处理
	程序就能够运行。raise_softirq()函数可以将一个软中断设置为挂起状态，让它在下次调用
	do_softirq()函数时投入运行。举个列子，网络子系统可能会调用
	
		raise_softirq(NET_TX_SOFTIRQ);
		
	这会触发NET_TX_SOFTIRQ软中断。它的处理程序net_tx_action()就会在内核下一次执行软
	中断时投入运行。该函数在触发一个软中断之前先要禁止中断，触发后再恢复回原来的状态。
	如果中断本来就已经被禁止了，那么可以调用另一函数raise_softirq_irqoff()，这会带来一些优
	化效果。如：
		/* 中断已经被禁止 */
		raise_softirq_irqoff(NET_TX_SOFTIRQ);
		
	在中断处理程序中触发软中断是最常见的形式。在这种情况下，中断处理程序执行硬件设备
	的相关操作，然后触发相应的软中断，最后退出。内核在执行完中断处理程序以后，马上就
	会调用do_softirq()函数。于是软中断开始执行中断处理程序留给它去完成的剩余任务。在这
	个例子中，“上半部”和“下半部”名字中的含义一目了然。
	
	
6.3 Tasklets

	Tasklets是利用软中断实现的一种下半部机制。我们之前提到过，它和进程没有任何关系。
	Tasklets和软中断在本质上很相似，行为表现也相近，但是，它的接口更简单，锁保护也要求
	较低。
	
	选择到底是用软中断还是tasklets其实很简单：通常你应该用tasllet。就像我们在前面看
	到的。软中断的使用者屈指可数。它只在那些执行频率很高和连续性要求很高的情况下才需
	要。而tasklet却有更广泛的用途。大多数情况下用tasklets效果都不错，而且它们还非常容易 
	使用。
	
6.3.1 Tasklets的实现

	因为tasklet是通过软中断实现的，所以它们本身也是软中断。前面讨论过了，tasklets由
	两类软中断代表：HI_SOFTIRQ和TASKLET_SOFTIRQ。这两者之间唯一的实际区别在于
	HI_SOFTIRQ类型的软中断先于TASKLET_SOFTIRQ类型的软中断执行。
	
	1. Tasklet结构体
	
	Tasklets由tasklet_struct结构表示。每个结构体单独代表一个tasklet，它在
	<linux/interrupt.h>中定义：
		
		struct tasklet_struct {
			struct tasklet_struct *next;		/*  */
			unsigned long state;				/*  */
			atomic_t count;						/*  */
			void (*func) (unsigned long);		/*  */
			unsigned long data;					/*  */
		};
	
	结构体中的func成员是tasklet的处理程序（像软中断的action一样），data是它唯一的参数。
	
	state成员只能在0、TASKLET_STATE_SCHED和TASKLET_STATE_RUN之间取值。
	TASKLET_STATE_SCHED表明tasklet已被调度，正准备投入运行，TASKLET_STATE_RUN
	表明该tasklet正在运行。TASKLET_STATE_RUN只有在多处理器的系统上才会作为一种优化
	来使用，单处理器系统任何时候都清楚单个tasklet是不是正在运行（它要么就是当前正在执行
	的代码，要么不是）。
	
	count成员是tasklet引用计数器。如果它不为0，则tasklet被禁止，不允许执行；只有当
	它为0时，tasklet才被激活，并且在被设置为挂起状态时，该tasklet才能够执行。
	
	2. 调度tasklets
	
	已调度的tasklet(等同于被触发的软中断)存放在两个单处理器数据数据结构：tasklet_vec
	(普通tasklet)和tasklet_hi_vec(高优先级的tasklet)中。这两个数据结构都是由tasklet_struct
	结构体构成的链表。链表中的每个tasklet_struct代表一个不同的tasklet。
	
	Tasklets由tasklet_schedule()和tasklet_hi_schedule()函数进行调度，它们接受一个指向
	tasklet_struct结构的指针作为参数。两个函数非常类似（区别在于一个使用TASKLET_
	SOFTIRQ而另一个用HI_SOFTIRQ）。在下一节我们将仔细研究怎么编写和使用tasklet。现在，
	让我们先考察一下tasklet_schedule()的细节：
	
		* 检查tasklet的状态是否为TASKLET_STATE_SCHED。如果是，说明tasklet已经被调度
		  过了（有可能是一个tasklet已经被调度过但还没来得及执行，而该tasklet又被唤起了一
		  次。――――译者注），函数返回。
		* 保存中断状态，然后禁止本地中断。在我们执行tasklet代码时，这么做能够保证处理器
		  上的数据不会弄乱。
		* 把需要调度的tasklet加到每个处理器一个的tasklet_vec链表或tasklet_hi_vec链表的
		  表头上去。
		* 唤起TASKLET_SOFTIRQ或HI_SOFTIRQ软中断，这样在下一次调用do_softirq()时就会
		  执行该tasklet。
		* 恢复中断到原状态并返回。
	
	在前面的小节中我们曾经提到过挂起，do_softirq()会尽可能早地在下一个合适的时机执行。
	由于大部分tasklets和软中断都是在中断处理程序中被设置成待处理状态，所以最近一个中断
	返回的时候看起来就是执行do_softirq()的最佳时机。因为TASKLET_SOFTIRQ和HI_SOFTIRQ
	已经被触发了，所以do_softirq()会执行相应的软中断处理程序。而这两个处理程序，
	tasklet_action()和tasklet_hi_action()，就是tasklet处理的核心。让我们观察它们做了什么：
		
		* 禁止中断，并为当前处理器检索tasklet_vec或tasklet_hig_vec链表。
		* 将当前处理器上的该链表设置为NULL，达到清空的效果。
		* 允许响应中断（没有必要再恢复它们回原状态，因为这段程序本身就是作为软中断处理
		  程序被调用的，所以中断是应该被允许的）。
		* 循环遍历获得链表上的每一个待处理的tasklet。
		* 如果是多处理器系统，通过检查TASKLET_STATE_RUN来判断这个tasklet是否正在其
		  他处理器上运行。如果它正在运行，那么现在就不要执行，跳到下一个待处理的tasklet
		  去（回忆一下，同一时间里，相同类型的taskelet只能有一个执行）。
		* 如果当前这个tasklet没有执行，将其状态设置为TASKLET_STATE_RUN，这样别的处
		  理器就不会再去执行它了。
		* 检查count值是否为0，确保tasklet没有被禁止。如果tasklet被禁止了，则跳到下一个挂
		  起的tasklet去。
		* 我们已经清楚的知道这个tasklet没有在其他地方执行，并且被我们设置成执行状态，这
		  样它在其他部分就不会被执行，并且引用计数为0，现在可以执行tasklet的处理程序了。
		* tasklet运行完毕，清除tasklet的state域的TASKLET_STATE_RUN状态标志。
		* 重复执行下一个tasklet，直到没有剩余的等待处理的tasklets。
		
	Tasklets的实现很简单，但非常巧妙。我们可以看到，所有的tasklets都通过重复运用
	HI_SOFTIRQ和TASKLET_SOFTIRQ这两个软中断实现。当一个tasklet被调度时，内核就会唤
	起这两个软中断中的一个。随后，该软中断会被特定的函数处理，执行所有已调度的tasklet。
	这个函数保证同一时间里只有一个给定类别的tasklet会被执行（但其他不同类型的tasklet可以
	同时执行）。所有这些复杂性都被一个简洁的接口隐藏起来了。
	
	
6.3.2 使用Tasklets

	大多数情况下，为了控制一个寻常的硬件设备，tasklet机制都是实现你自己的下半部的最
	佳选择。Tasklet可以动态创建，使用方便，执行起来也还算快。
	
	1. 声明你自己的Tasklet
	
	你既可以静态地创建tasklet，也可以动态地创建它。选择哪种方式取决于你到底是有（或
	者是想要）一个对tasklet的直接引用还是间接引用。如果你准备静态的创建一个tasklet（也就
	是有一个它的直接引用），使用下面<linux/interrupt.h>中定义的两个宏中的一个：
	
		DECLARE_TASKLET(name, func, data)
		DECLARE_TASKLET_DISABLED(name, func, data);
		
	这两个宏都能根据给定的名称静态地创建一个tasklet_struct结构。当该tasklet被调度以后，
	给定的函数func会被执行，它的参数由data给出。这两个宏之间的区别在于引用计数器的初始
	值设置不同。前面一个宏把创建的tasklet的引用计数器设置为0，该tasklet处于激活状态。另
	一个把引用计数器设置为1，所以该tasklet处于禁止状态。下面是一个例子：
		DECLARE_TASKLET(my_tasklet, my_tasklet_handler, dev);
	这行代码其实等价于
		struct tasklet_struct my_tasklet = { NULL, 0, ATOMIC_INIT(0),
											tasklet_handler, dev};
	
	这样就创建了一个名为my_tasklet，处理程序为tasklet_handler并且已被激活的tasklet。当
	处理程序被调用的时候，dev就会被传递给它。
	
	还可以通过将一个间接引用（一个指针）赋给一个动态创建的tasklet_struct结构的方式来
	初始化一个tasklet:
	
		tasklet_init(t, tasklet_handler, dev); /* 动态而不是静态创建 */
	
	2. 编写你自己的tasklet处理程序
	
	tasklet处理程序必须符合规定的函数类型：
	void tasklet_handler(unsigned long data)
	
	因为是靠软中断实现，所以tasklet不能睡眠。这意味着你不能在tasklet中使用信号量或者
	其他什么阻塞式的函数。由于tasklet运行时允许响应中断，所以你必须做好预防工作（比如屏
	蔽中断然后获取一个锁），如果你的tasklet和中断处理程序之间共享了某些数据的话。两个相
	同的tasklet决不会同时执行，这点和软中断不同――――尽管两个不同的tasklet可以在两个处理器
	上同时执行。如果你的tasklet和其他的tasklet或者是软中断共享了数据，你必须进行适当地锁
	保护。（参看第7章和第8章）。
	
	3. 调度你自己的tasklet
	
	通过调用tasklet_schedule()函数并传递给它相应的tasklet_struct的指针，该tasklet就会被调
	度以便执行：
		tasklet_schedule(&my_tasklet);  /* 把my_tasklet 标记为挂起 */

	在tasklet被调度以后，只要有机会它就会尽可能早地运行。在它还没有得到运行机会之前，
	如果有一个相同的tasklet又被调度了（译者注：这里应该是唤起的意思，在前面讲述调度流程
	的小节里可以看到，调度tasklet的第一个步骤就是检查是否重复，所以这里根本不会完成调
	度。）那么它仍然只会运行一次。而如果这时它已经开始运行了，比如说在另外一个处理器上，
	那么这个新的tasklet会被重新调度并再次运行。作为一种优化措施，一个tasklet总在调度它的
	处理器上执行――――这是希望能更好地利用处理器的高速缓存。
	
	你可以调用tasklet_disable()函数来禁止某个指定的tasklet。如果该tasklet当前正在执行，
	这个函数会等到它执行完毕再返回。你也可以调用tasklet_disable_nosync()函数，它也用来禁
	止指定的tasklet，不过它无须在返回前等待tasklet执行完毕。这么做往往不太安全，因为你无
	法估计该tasklet是否仍在执行。调用tasklet_enable()函数可以激活一个tasklet，如果希望激活
	DECLARE_TASKLET_DISABLED()创建的tasklet，你也得调用这个函数，如：
	
		tasklet_disable(&my_tasklet);   /* tasklet现在被禁止 */
		/* 我们现在毫无疑问地知道tasklet不能运行 .. */
		tasklet_enable(&my_tasklet);    /* tasklet现在被激活 */

	你可以通过调用tasklet_kill()函数从挂起的队列中去掉一个tasklet。该函数的参数是一个指向
	某个tasklet的tasklet_struct的长指针。在处理一个经常重新调度它自身的tasklet的时候，从挂
	起的队列中移去已调度的tasklet会很有用。这个函数首先等待该tasklet执行完毕，然后再将它
	移去。当然，没有什么可以阻止其他地方的代码重新调度该tasklet。由于该函数可能会引起休
	眠，所以禁止在中断上文中使用它。


6.3.3 ksoftirqd

	每个处理器都有一组辅助处理软中断（和tasklet）的内核线程。当内核中出现大量软中断
	的时候，这些内核进程就会辅助处理它们。
	
	我们前面曾经讨论过，对于软中断，内核会选择在几个特殊时机进行处理。而在中断处
	理程序返回时处理是最常见的。软中断被触发的频率有时可能很高（像在进行大流量的网络
	通信期间）。更不利的是，处理函数有时还会自行重复触发。也就是说，当一个软中断执行的
	时候，它可以重新触发自己以便再次得到执行（事实上，网络子系统就会这么做）。如果软中
	断本身出现的频率就高，再加上它们又有将自己重新设置为可执行状态的能力，那么就会导
	致用户空间进程无法获得足够的处理器时间，因而处于饥饿状态。而且，单纯的对重新触发
	的软中断采取不立即处理的策略，也无法让人接受。这是一个让人进退维谷的问题，亟待解
	决，而直观的解决方案都不理想。首先，就让我们看看两种最容易想到的直观的方案。
	
	第一种方案是只要还有被触发并等待处理的软中断，本次执行就要负责处理，重新触发
	的软中断也在本次执行返回前被处理。这样做可以保证对内核的软中断采取即时处理的方式，
	关键在于，对重新触发的软中断也会立即处理。当负载很高的时候这样做就会出问题，此时
	会有大量被触发的软中断，而它们本身又会重复触发。系统可能会一直处理软中断，根本不
	能完成其他任务。用户空间的任务被忽略了――――实际上，只有软中断和中断处理程序轮流执
	行，而系统的用户只能等待。只有在系统永远处于低负载的情况下，这种方案才会有理想的
	运行效果；只要系统有哪怕是中等程度的负载量，这种方案就无法让人满意。用户空间根本
	不能容忍有明显的停顿出现。
	
	第二种方案选择不处理重新触发的软中断。在从中断返回的时候，内核和平常一样，也
	会检查所有挂起的软中断并处理它们。但是，任何自行重新触发的软中断都不会马上处理，
	它们被放到下一个软中断执行时机去处理，而这个时机通常也就是下一次中断返回的时候，
	这等于是说，一定得等一段时间，新的（或者重新触发的）软中断才能被执行。可是，在比
	较空闲的系统中，立即处理软中断才是比较好的做法。很不幸，这个方案显然又是一个时好
	时坏的选择。尽管它能保证用户空间不处于饥饿状态，但它却让软中断忍受饥饿的痛苦，而
	根本没有好好利用闲置的系统资源。
	
	这里需要一些折中。内核实际选中的方案是不会立即处理重新触发的软中断。而作为改
	进，当大量软中出现的时候，内核会唤醒一组内核线程来处理这些负载。这些线程在最低
	的优先级上运行（nice值是19），这能避免它们跟其他重要的任务抢夺资源。但它们最终肯定
	会被执行，所以，这个折中方案能够保证在软中断负担很重的时候用户程序不会因为得不到
	处理时间而处于饥饿状态。相应的，也能保证“过量”的软中断终究会得到处理。最后，在
	空闲系统上，这个方案同样表现良好，软中断处理得非常迅速（因为仅存的内核线程肯定会
	马上调度）。
	
	每个处理器都有一个这样的线程。所有线程的名字都叫做ksoftirad/n，区别在于n，它对
	应的是处理器的编号。在一个双CPU的机器上就有两个这样的线程，分别叫ksoftirad/0和
	ksoftirad/1。为了保证只要有空闲的处理器，它们就会处理软中断，所以给每个处理器都分配
	一个这样的线程。一旦该线程被初始化，它就会执行类似下面这样的死循环：
	
		for (;;) {
			if (!softirq_pending(cpu))
				schedule();
			
			set_current_state(TASK_RUNNING);
			
			while (softirq_pending(cpu)) {
				do_softirq();
				if (need_resched())
					schedule();
			}
			
			set_current_state(TASK_INTERRUPTIBLE);
		}
		
	只要有待处理的软中断（由softirq_pending()函数负责发现），ksoftirq就会调用do_softirq
	去处理它们。通过重复执行这样的操作，重新触发的软中断也会被执行。如果有必要的话，
	每次迭代后都会调用schedule()以便让更重要的进程得到处理机会。当所有需要执行的操作都
	完成以后，该内核线程将自己设置为TASK_INTERRUPTIBLE状态，唤起调度程序选择其他
	可执行进程投入运行。
	
	只要do_softirq()函数发现已经执行过的内核线程重新触发了它自己，软中断内核线程就
	会被唤醒。
	
	
6.4 工作队列

	工作队列（work queue）是另外一种将工作推后执行的形式，它和我们前面讨论的所有
	其他形式都不同。工作队列可以把工作推后，交由一个内核线程去执行――――该工作总是会
	在进程上下文执行。这样，通过工作队列执行的代码能占尽进程上下文的所有优势。最重要
	的就是工作队列允许重新调度甚至是睡眠。
	
	通常，在工作队列和软中断/tasklet中作出选择非常容易。如果推后执行的任务需要睡眠，
	那么就选择工作队列。如果推后执行的任务不需要睡眠，那么就选择软中断或tasklet。实际上
	工作队列通常可以用内核线程替换。但是由于内核开发者们非常反对创建新的内核线程（在
	有些场合，使用这种冒失的方法可能会吃到苦头），所以我们也推荐使用工作队列。当然，这
	种接口也的确很容易使用。
	
	如果你需要用一个可以重新调度的实体来执行你的下半部处理，你应该使用工作队列。
	它是唯一能在进程上下文运行的下半部实现的机制，也只有它才可以睡眠。这意味着在你需
	要获得大量的内存时、在你需要获取信号量时、在你需要执行阻塞式的I/O操作时，它都会非
	常有用。如果你不需要用一个内核线程来推后执行工作，那么就考虑使用tasklet吧。
	
6.4.1 工作队列的实现

	工作队列子系统是一个用于创建内核线程的接口，通过它创建的进程负责执行由内核其
	他部分排到队列里的任务。它创建的这些内核线程被称作工作者线程（worker threads）。工作
	队列可以让你的驱动程序创建一个专门的工作者线程来处理需要推后的工作。不过，工作队
	列子系统提供了一个缺省的工作者线程来处理这些工作。因此，工作队列最基本的表现形式
	就转变成了一个把需要推后执行的任务交给特定的通用线程这样一种接口。
	
	缺省的工作者线程叫做events/n，这里n是处理器的编号；每个处理器对应一个线程。比
	如，单处理器的系统只有events/0这样一个线程。而双处理器的系统就会多一个events/1线程。
	缺省的工作者线程会从多个地方得到被推后的工作。许多内核驱动程序都把它们的下半部交
	给缺省的工作者线程去做。除非一个驱动程序或者子系统必须建立一个属于它自己的内核线
	程，否则最好使用缺省线程。
	
	不过并不存在什么东西能够阻止代码创建属于自己的工作者线程。如果你需要在工作者
	线程中执行大量的处理操作，这样做或许会带来好处。处理器密集型和性能要求严格的任务
	会因为拥有自己的工作者线程而获得好处。此时这么做也有助于减轻缺省线程的负担，避免
	工作队列中其他需要完成的工作处于饥饿状态。
	
	1. 表示线程的数据结构
		
	工作者线程用workqueue_struct结构表示：
		
		/* 
		 * 外部可见的工作队列抽象是由每个CPU的工作队列组成的数组
		 */
		struct workqueue_struct {
			struct cpu_workqueue_struct cpu_wq(NR_CPUS);
		};

	该结构内是一个由cpu_workqueue_struct结构组成的数组，数组的每一项对应系统中的一
	个处理器。由于系统中每个处理器对应一个工作者线程。所以对于给定的某台计算机来说，
	就是每个处理器，每个工作者线程对应一个这样的cpu_workqueue_struct结构体。cpu_work-
	queue_struct是kernel/workqueue.c中的核心数据结构：
	
		/*
		 * 单CPU对应的工作队列
		 */
		struct cpu_workqueue_struct {
			spinlock_t lock;
			
			atomic_t nr_queued;
			struct list_head worklist;
			wait_queue_head_t more_work;
			wait_queue_head_t work_done;
			
			struct workqueue_struct *wq;
			task *thread;
			struct completion exit;
		}

	注意，每个工作者线程类型关联一个自己的workqueue_struct。在该结构体里面，给每个
	线程分配一个cpu_workqueue_struct，因而也就是给每个处理器分配一个，因为每个处理器都
	有一个该类型的工作者线程。
	
	2. 表示工作的数据结构
		
	所有的工作者线程都是用普通的内核线程实现的，它们都要执行worker_thread()函数。在
	它初始化完以后，这个函数执行一个死循环并开始休眠。当有操作被插入到队列里的时候，
	线程就会被唤醒，以便执行这些操作。当没有剩余的操作时，它又会继续休眠。
	
	工作用<linux/workqueue.h>中定义的work_struct结构体表示：
	
	struct work_struct {
		unsigned long pending;        /* 这个工作正在等待处理吗？ */
		struct list_head entry;       /* 连接所有工作的链表 */
		void (*func) (void *);        /* 处理函数 */
		void *data;                   /* 传递给处理函数的参数 */
		void *wq_data;                /* 内部使用 */
		struct timer_list timer;      /* 延迟的工作队列所用到的定时器 */
	}

	这些结构体被连接成链表，在每个处理器上的每种类型的队列都对应这样一个链表。比
	如，每个处理器上用于执行被推后的工作的那个通用线程就有一个这样的链表。当一个工作者
	线程被唤醒时，它会执行它的链表上的所有工作。工作被执行完毕，它就将相应的
	work_struct对象从链表上移去。当链表上不再有对象的时候，它就会继续休眠。
	
	我们可以看一下worker_thread()函数的核心流程，简化如下：
		for (;;) {
			set_task_state(current,TASK_INTERRUPTIBLE);
			add_wait_queue(&cwq->more_work, &wait);
			
			if (list_empty(&cwq->worklist))
				schedule();
			else
				set_task_state(current, TASK_RUNNIGN);
			remove_wait_queue(&cwq->more_work, &wait);
			
			if (!list_empty(&cwq->worklist))
				run_workqueue(cwq);
		}

	该函数在死循环中完成了以下功能：
	* 线程将自己设置为休眠状态（state被设成TASK_INTERRUPTIBLE）并把自己加入到等
	  待队列上。
	* 如果工作链表是空的，线程调用schedule()函数进入睡眠状态。
	* 如果链表中有对象，线程不会睡眠。相反，它将自己设置成TASK_RUNNIGN，脱离等
	  待队列。
	* 如果链表非空，调用run_workqueue()函数执行被推后的工作。
	
	3. run_workqueue()
	
	下一步，由run_workqueue()函数来实际完成推后到此的工作：
	while (!list_empty(&cwq->worklist)) {
		struct work_struct *work = list_entry(cwq->worklist.next,
					struct work_struct, entry);
		void (*f) (void *) = work->func;
		void *data = work->data;
		
		list_del_init(cwq->worklist.next);
		
		clear_bit(0, &work->pending);
		f(data);
	}

	该函数循环遍历链表上每个待处理的工作，执行链表每个节点上的workqueue_struct（译者
	注：应该是work_struct。）中的func成员函数：
		* 当链表不为空时，选取下一个节点对象。
		* 获取我们希望执行的函数func及其参数data。
		* 把该节点从链表上解下来，将待处理标志位pending清0
		* 调用函数
		* 重复执行
		
	4. 对不起，你到底都说了些什么？
	
	这些数据结构之间的关系确实让人觉得混乱，难以摸清头绪。图6-1给出了示意图，把所
	有这些关系放在一起进行解释。
	
	位于最高一层的是工作者线程。系统允许有多种类型的工作者线程存在。对于指定的一
	个类型，系统的每个CPU上都有一个该类的工作者线程。内核中有些部分可以根据需要来创
	建工作者线程。而在默认情况下内核只有events这一种类型的工作者线程。每个工作者线程都
	由一个cpu_workqueue_struct结构体表示。而workqueue_struct结构体则表示给定类型的所有
	工作者线程。
	
	举个例子，在系统默认的通用events工作者类型之外，我自己加入了一种falcon工作者类
	型。并且我使用的是一个拥有四个处理器的计算机。那么，系统中现在有四个events类型的线
	程（因而也就有四个cpu_workqueue_struct结构体）和四个falcon类型的线程（因而会有另外 
	四个cpu_workqueue_struct结构体）。同时，有一个对应events类型的workqueue_struct和一个
	对应falcon类型的workqueue_struct。
	
	工作处于在最底一层。你的驱动程序创建这些需要推后执行的工作（译者注：这其实可以
	理解成用“工作”这种接口封装我们实际需要推后的工作。以便后续的工作者线程处理）。它
	们用work_struct结构来表示。这个结构体中最重要的部分是一个指针，它指向一个函数，而
	正是该函数负责处理需要推后执行的具体任务。工作会被提交给某个具体的工作者线程。然
	后这个工作者线程会被唤醒并执行这些排好的工作。
	
	大部分驱动程序都使用的是现存的默认工作者线程。它们使用起来简单、方便。可是，
	在有些要求更严格的情况下，驱动程序需要自己的工作者线程。比如说XFS文件系统就为自
	己创建了两种新的工作者线程。
	
	
6.4.2 使用工作队列
	
	工作队列的使用非常简单。我们先来看一下缺省的events任务队列，然后再看看创建新的
	工作者线程。
	
	1. 创建推后的工作
	
	首先要做的是实际创建一些需要推后完成的工作。可以在编译时静态地建该结构体：
	DECLARE_WORK(name, void (*func) (void *), void *data);
	
	这样就会静态地创建一个名为name，处理函数为func，参数为data的work_struct结构体。
	
	同样，也可以在运行时通过指针创建一个工作：
	INIT_WORK(struct work_struct *work, void(*func) (void *), void *data);
	
	这会动态地初始华一个由work指向的工作，处理函数为func，参数为data。
	
	2. 工作队列处理函数
	
	工作队列处理函数的原型是：
	void work_handler(void *data)
	
	这个函数会由一个工作者线程执行，因此，函数会运行在进程上下文中。默认情况下，
	允许响应中断，并且不持有任何锁。如果需要，函数可以睡眠。需要注意的是，尽管操作处
	理函数运行在进程上下文中，但它不能访问用户空间，因为内核线程在用户空间没有相关的
	内存映射。通常在系统调用发生时，内核会代表用户空间的进程运行，此时它才能访问用户
	空间，也只有在此时它才会映射用户空间的内存。

	在工作队列和内核其他部分之间使用锁机制就像在其他的进程上下文中使用锁机制一样
	方便。这使编写处理函数变得相对容易。接着的两章会讨论到锁机制。
	
	3. 对工作进行调度
	
	现在工作已经被创建，我们可以调度它了。想要把给定工作的处理函数提交给缺省的
	events，只需调用
		schedule_work(&work);
		
	work马上就会被调度，一旦其所在的处理器上的工作者线程被唤醒，它就会被执行。
	
	有时候你并不希望工作马上就被执行，而是希望它经过一段延迟以后再执行。在这种情
	况下，你可以调度它在指定的时间执行：
		schedule_delayed_work(&work, delay);
		
	这时，&work指向的work_struct直到delay指定的时钟节拍用完以后才会执行。在第9章将介绍
	这种使用时钟节拍作为时间单位的方法。
	
	4.刷新操作
	
	排入队列的工作会在工作者线程下一次被唤醒的时候执行。有时，在继续下一步工作之
	前，你必须保证一些操作已经执行完毕了。这一点对模块来说就很重要，在卸载之前，它就
	有可能需要调用下面的函数。而在内核的其他部分，为了防止竞争条件的出现，也可能需要
	确保不再有待处理的工作。
	
	出于以上的目的，内核准备了一个用于刷新指定工作队列的函数：
	void flush_scheduled_work(void);
	
	函数会一直等待，直到队列中所有对象都被执行以后才返回。在等待所有待处理的工作执行
	的时候，该函数会进入休眠状态，所以只能在进程上下文中使用它。
	
	注意，该函数并不取消任何延迟执行的工作。就是说，任何通过schedule_delayed_work()
	调度的工作，如果其延迟时间未结束，它并不会因为调用flush_scheduled_work()而被刷新掉。
	取消延迟执行的工作应该调用：
		int cancel_delayed_work(struct work_struct *work);
	这个函数可以取消任何与work_struct相关的挂起工作。
	
	5. 创建新的工作队列
	
	如果缺省的队列不能满足你的需要，你应该创建一个新的工作队列和与之相应的工作者
	线程。由于这么做会在每个处理器上都创建一个工作者线程，所以只有在你明确了必须要靠
	自己的一套线程来提高性能的情况下，再创建自己的工作队列。
	
	创建一个新的任务队列和与之相关的工作者线程，你只需调用一个简单的函数：
		struct workqueue_struct *create_workqueue(const char *name);
	
	name参数用于该内核线程的命名。比如，缺省的events队列的创建就调用的是
		struct workqueue_struct *keventd_wq = create_workqueue("events");
		
	这样就会创建所有的工作者线程（系统中的每个处理器都有一个）并且做好所有开始处理工
	作之前的准备工作。
	
	创建一个工作的时候无须考虑工作队列的类型。在创建之后，可以调用下面列举的函数。
	这些函数与schedule_work()以及schedule_delayed_work()相近，唯一的区别就在于它们针对给
	定的工作队列而不是缺省的event队列进行操作。
		int queue_work(struct workqueue_struct *wq, struct work_struct *work);
		int queue_delayed_work(struct workqueue_struct *wq,
					struct work_struct *work, unsigned long delay);
					
	最后，你可以调用下面的函数刷新指定的工作队列：
		flush_workqueue(struct workqueue_struct *wq);
		
	该函数和前面讨论过的flush_schedule_work()作用相同，只是它在返回前等待清空的是给定
	的队列。


6.5 下半部机制的选择

	在各种不同的下半部实现机制之间做出选择是很重要的。在当前的2.6版内核中，有三种
	可能的选择：软中断、tasklets和工作队列。Tasklets基于软中断实现，所以两者很相近。工作
	队列机制与它们完全不同，它靠内核线程实现。
	
	从设计的角度考虑，软中断提供的执行序列化的保障最少。这就要求软中断处理函数必
	须格外小心地采取一些步骤确保共享数据的安全，两个甚至更多相同类别的软中断有可能在
	不同的处理器上同时执行。如果被考察的代码本身多线索化的工作就做得非常好，比如像网
	络子系统，它完全使用单处理器变量，那么软中断就是非常好的选择。对于时间要求严格和
	执行频率很高的应用来说，它执行得也最快。如果代码多线索化考虑得并不充分，那么选择
	tasklets意义更大。它的接口非常简单，而且，由于两个同种类型的tasklet不能同时执行，所
	以实现起来也会简单一些。
	
	如果你需要把任务推后到进程上下文中完成，那么在这三者中就只能选择工作队列了。
	如果进程上下文并不是必须的条件――――明确点说，就是如果并不需要睡眠――――那么软中断和
	tasklets可能更合适。工作队列造成的开销最大，因为它要牵扯到内核线程甚至是上下文切换。
	这并不是说工作队列的效率低，如果每秒钟有几千次中断，就像网络子系统时常经历的那样，
	那么采用其他的机制可能更合适一些。尽管如此，针对大部分情况，工作队列都能提供足够
	的支持。
	
	如果讲到易于使用，工作队列就当仁不让了。使用缺省的events队列简直不费吹灰之力。
	接下来是tasklets，它的接口也很简单。最后才是软中断，它必须静态创建。
	
	表6-3是对三种下半部接口的比较。
	
								表6-3 对下半部的比较
		下半部					上下文						顺序执行保障
		
		软中断					中断						没有
		Tasklet					中断						同类型不能同时执行
		工作队列				进程						没有（和进程上下文一样被调度）
	
	简单地说，一般的驱动程序的编写者需要做两个选择。首先，你是不是需要一个可调度
	的实体来执行需要推后完成的工作――――你有休眠的需要吗？要是有，工作队列就是你的唯一
	选择。否则最好用tasklet。要是必须专注于性能的提高，那么就考虑软中断吧。
	
	
	6.6 在下半部之间加锁
	
	到现在为止，我们还没讨论过锁机制，这是一个非常有趣的话题，我将在接下来的两章
	里仔细讨论它。不过，在这里还是应该对它的重要性有所了解，在使用下半部机制时，即使
	是在一个单处理器的系统上，避免共享数据被同时访问也是至关重要的。记住，一个下半部
	实际上可能在任何时候执行。如果你对锁机制一无所知的话，你也可以在读完后面两章以后
	再回过头来看这部分。
	
	使用tasklets的一个好处在于它自己负责执行的序列化保障：两个相同类型的tasklet不允许
	同时执行，即使在不同的处理器上也不行。这意味着你无须为tasklet内部的同步问题操心了。
	Tasklet之间的同步（就是当两个不同类型的tasklet共享同一数据时）需要正确使用锁机制。
	
	因为软中断根本不保障执行序列化，（即使相同类型的软中断也有可能有两个实例在同时
	执行）所以所有的共享数据都需要合适的锁。
	
	如果进程上下文和一个下半部共享数据，在访问这些数据之前，你需要禁止下半部的处
	理并得到锁的使用权。所做的这些是为了本地和SMP的保护并且防止死锁的出现。
	
	如果中断上下文和一个下半部共享数据，在访问数据之前，你需要禁止中断并得到锁的
	使用权。所做的这些也是为了本地和SMP的保护并且防止死锁的出现。
	
	任何在工作队列中被共享的数据也需要使用锁机制。其中有关锁的要点和在一般内核代
	码中没什么区别，因为工作队列本来就是在进程上下文中执行的。
	
	在第7章里，我们会揭示锁的奥妙。而在第8章中，我们将讲述内核的加锁原语。这两个
	章节会描述如何保护下半部使用的数据。
	
	禁止下半部

	一般单纯禁止下半部的处理是不够的。为了保证共享数据的安全，更常见的做法是先得
	到一个锁然后再禁止下半部的处理。驱动程序中通常使用的都是这种方法，在第8章会详细介
	绍。然而，如果你编写的是内核的核心代码，你也可能仅需要禁止下半部就可以了。
	
	如果需要禁止所有的下半部处理（明确点说，就是所有的软中断和所有的tasklets），可以
	调用local_bh_disable()函数。允许下半部进行处理，可以调用local_bh_enable()函数。没错，
	这些函数的命名也有问题；可是既然BH接口早就让位给软中断了，那么谁又会去改这些名称
	呢。表6-4是这些函数的一份摘要。
							表6-4 下半部机制控制函数的清单
		函数									描述
		void local_bh_disable()		禁止本地处理器的软中断和tasklet的处理
		void local_bh_enable()		激活本地处理器的软中断和tasklet的处理
		
	这些函数有可能被嵌套使用――――最后被调用的local_bh_enable()最终激活下半部。函数通
	过preempt_count（很有意思，还是这个计数器，内核抢占的时候用的也是它）为每个进程
	维护一个计数器。当计数器变为0时，下半部才能够被处理。因为下半部的处理已经被禁止，
	所以local_bh_enable()还需要检查所有现在的待处理的下半部并执行它。
	
	这些函数与硬件体系结构相关，它们位于<asm/softirq.h>中，通常由一些复杂的宏实现，
	下面是为那些好奇的人准备的C语言的近似描述：
		/* 通过增加preempt_count禁止本地下半部 */
		void local_bh_disable(void)
		{
			struct thread_info *t = current_thread_info();
			t->preempt_count += SOFTIRQ_OFFSET;
		}
		
		/* 减少preempt_count - 如果该返回值为0，将导致自动激活下半部 */
		/* 执行挂起的下半部 */
		void local_bh_enable(void)
		{
			struct thread_info *t = current_thread_info();
			t->preempt_count -= SOFTIRQ_OFFSET;
			/* preempt_count是否为0，另外是否有挂起的下半部，如果都满足，则执行
			 * 待执行的下半部
			 */
			if (unlikely(!t->preempt_count && softirq_pending(smp_processor_id())))
				do_softirq();
		}

	这些函数并不能禁止工作队列的执行。因为工作队列是在进程上下文中运行的，不会涉
	及异步执行的问题，所以也就没有必要禁止它们执行。由于软中断和tasklet是异步发生的（就
	是说，在中断处理返回的时候），所以，内核代码必须禁止它们。另一方面，对工作队列来说，
	它保护共享数据所做的工作和其他任何进程上下文中所做的都差不多。第7和第8章将揭示其
	中的细节。


*****************************************************************************************
	
								第7章  内核同步介绍

*****************************************************************************************

	共享内存的应用程序必须特别留意保护共享资源，防止共享资源被并发访问。内核也不
	例外。共享资源之所以要防止并发访问，是因为如果多个执行线程同时访问和操作数据，
	就有可能发生各线程之间相互覆盖共享数据的情况，造成被访问数据处于不一致状态。并发
	访问共享数据是造成系统不稳定的一类隐患，而且这种错误一般难以跟踪和调试――――所以首
	先应该认识到这个问题的重要性。
	
	要做到对共享资源的恰当保护往往很困难。多年之前，在Linux还未支持对称多处理器的
	时候，避免并发访问数据的方法相对来说比较简单。在单一处理器的时候，只有在中断发生
	的时候，或在内核代码显式地请求重新调度，执行另一个任务的时候，数据才可能被并发访
	问。回想起来，那时的生活何其简单啊！
	
	那只是过去，从2.0开始，内核就开始支持对称多处理器了，而且从那以后对它的支持不
	断地加强和完善。支持多处理意味着内核代码可以同时运行在两个或更多的处理器上。因此，
	如果不加保护，运行在两个不同处理器上的内核代码完全可能在同一时刻里并发访问共享数
	据。随着2.6内核的出现，Linux内核已发展成为了抢占式内核，这意味着（当然，还是指不加
	保护的情况下）调度程序可以在任何时候抢占正在运行的内核代码，重新调度其他的进程执
	行。现在，内核代码中有不少部分都能够同步执行，而它们都必须被妥善地保护起来。
	
	这一章重点讨论操作系统内核中的并发和同步问题，下一章将详细介绍Linux内核为解决
	同步问题和防止产生竞争条件而提供的机制及接口。
	
	
7.1 临界区和竞争条件

	所谓临界区（critical regions）就是访问和操作共享数据的代码段。多个执行线程并发
	访问同一个资源通常是不安全的，为了避免在临界区中并发访问，编程者（也就是你）必须
	保证这些代码原子地执行――――也就是说，代码在执行结束前不可被打断，就如同整个临界
	区是一个不可分割的指令一样。如果两个执行线程有可能处于同一个临界区中，那么这就是
	程序包含的一个bug。如果这种情况确实发生了，我们就称它是竞争条件（race conditions），
	这样命名是因为这里会存在线程竞争。这种情况出现的机会非常小――――就是因为竞争引起
	的错误非常不易重现，所以调试这种错误才会非常困难。避免并发和防止竞争条件被称为同
	步（synchronization）。
	
	为什么我们需要保护
	
	为了认清竞争条件的本来面目，我们首先要明白临界区无处不在。作为第一个例子，考
	虑一个非常简单的共享资源：一个全局整型变量和一个简单的临界区，其中的操作仅仅是将
	整型变量的值增加1。
		i++;
	
		该操作可以转化成类似于下面动作的机器指令序列：
		* 得到当前变量i的值并且拷贝到一个寄存器中
		* 将寄存器中的值加1
		* 把i的新值写回到内存中
		
	现在假定有两个执行线程同时进入这个临界区，如果i是初始值是7，那么，我们所期望的
	结果应该像下面这样（每一行代表一个时间单元）：
		
			线程1					线程2
			
			获得i(7)				-
			增加i(7->8)				-
			写回i(8)				-
									获得i(8)
									增加i(8->9)
									写回i(9)
									
	正如所期望的，7被两个线程分别加1变为9。但是，实际上的执行序列却可能如下：
			线程1					线程2
			
			获得i(7)				-
			-						获得i(7)
			增加i(7->8)				-
			-						增加i(7->8)
			写回i(8)				-
			-						写回i(8)
	
	如果两个执行线程都在变量i值增加前读取它的初值，进而又分别增加变量i的值，最后再
	保存该值，那么变量i的值就变成了8，而变量i的值本该是9的。这是最简单的临界区例子，幸
	好对这种简单竞争条件的解决方法也同样简单――――我们仅仅需要将这些指令作为一个不可分
	割的整体来执行就万事大吉了。多数处理器都提供了指令来原子地读变量、增加变量然后再
	写回变量，使用这样的指令就能解决一些问题。内核也提供了一组实现这些原子操作的接口，
	我们将在下一章讨论它们。
	

7.2 加锁

	现在我们来讨论一个更为复杂的竞争条件，相应的解决方法也更为复杂。假设需要处理
	一个队列上的所有服务请求，我们可以任意选一种方法实现这个队列，这里我们假定该队列
	是一个链表，链表中的每个结点就代表一个请求。有两个函数可以用来操作此队列：一个函
	数将新请求添加到队列尾部，另一个函数从队列头删除请求，然后处理它。内核各个部分都
	会调用这两个函数，所以内核会频繁地将在队列中加入请求，从队列中删除和处理请求。对
	请求队列的操作无疑要用到多条指令。如果一个线程试图读取队列，而这时正好另一个线程
	正在处理该队列，那么读取线程就会发现队列此刻正处于不一致状态。很明显，如果允许并
	发访问队列，就会产生危害。当共享资源是一个复杂的数据结构时，竞争条件往往会使该数
	据结构遭到破坏。
	
	表面上看，这种情况好像没有一个好的方法来解决，一个处理器读取队列的时候，我们
	怎么能禁止另一个处理器更新队列呢？虽然有些体系结构提供简单的原子指令，实现算术运
	算和比较之类的原子操作，但让体系结构提供专门的指令，对像上例中那样的不定长度的临
	界区进行保护，就强人所难了。我们需要一种方法，可以标记临界区起始和结束位置，而且，
	只要有一个执行线程处于临界区时，就禁止（或者说锁定）其他访问。
	
	锁提供的就是这种机制：它就如同一把门锁，门后的房间可想象成一个临界区。在一个
	指定的时间内，房间里只能有个一个执行线程存在，当一个线进入房间后，它会锁住身后的
	房门；当它结束对共享数据的操作后，就会走出房间，打开门锁。如果另一个线程在房门上
	锁时来了，那么它就必须等待房间内的线程出来并打开门锁后，才能进入房间。
	
	上面例子中讲到的请求队列，可以使用一个单独的锁进行保护。每当有一个新请求要加
	入队列，线程会首先要占住锁，然后就可以安全地将请求加入到队列中，结束操作后再释放
	该锁；同样当一个线程想从请求队列中删除一个请求时，也需要先占住锁，然后才能从队列
	中读取和删除请求，而且在完成操作后也必须释放锁。任何要访问队列的其他线程也类似，
	必须占住锁后才能进行操作。因为在一个时刻只能有一个线程持有锁，所以在一个时刻只有
	一个线程可以操作队列。由此可见锁机制可以防止并发执行，并且保护队列不受竞争条件的
	影响。
	
	任何要访问队列的代码首先都需要占住相应的锁，这样该锁就能阻止来自其他执行线程
	的并发访问：
	
			线程1					线程2
			
			试图锁定队列			试图锁定队列
			成功：获得锁			失败：等待...
			访问队列...				等待...
			为队列解除锁			等待...
			...						成功：获得锁
									访问队列...
									为队列解除锁
									
	请注意锁的使用的自愿的、非强制的，它完全属于一种编程者自选的编程手段。没有什
	么可以强制编程者在操作我们虚构的队列时必须使用锁。当然，如果不这么做，无疑会造成
	竞争条件而破坏队列。
	
	锁有多种多样的形式，而且加锁的粒度范围也各不相同――――Linux自身实现了几种不同
	的锁机制。各种锁机制之间的区别主要在于当锁被争用时（已经被使用）的行为表现――――一
	些锁被争用时会简单地执行忙等待，而有些锁会使当前任务睡眠直到锁可用为止。下一章
	我们将讨论Linux中不同锁之间的行为差别及它们的接口。
	
	
7.2.1 到底是什么造成了并发执行

	用户空间之所以需要同步，是因为用户程序会被调度程序抢占和重新调度。由于用户进
	程可能在任何时刻被抢占，而调度程序完全可能选择另一个高优先级的进程到处理器上执行，
	所以就有可能在一个程序正处于临界区时，就被非自愿地抢占了，如果新调度的进程随后
	也进入同一个临界区（比如说，这两个进程是同一个进程的两个可执行线程，它们两个要访
	问共享的内存），前后两个进程相互之间就会产生竞争。另外，因为信号处理是异步发生的，
	所以即使是单线程的程序，如果需要处理信号，也有可能产生竞争条件。这种类型的并发操
	作――――这里其实两者并不真是同时发生的，但它们相互交叉进行，所以也可称作伪并发执行。
	
	如果你有一台支持对称多处理器的机器，那么两个进程就可以真正的在临界区中同时执
	行了，这种类型被称为真并发。虽然真并发和伪并发的原因和含义不同，但它们都同样会造
	成竞争条件，而且也需要同样的保护。
	
	内核中有类似可能造成并发执行的原因。它们是：
	* 中断――――中断几乎可以在任何时刻异步发生，也就可能随时打断当前正在执行的代码。
	* 内核抢占――――因为内核具有抢占性，所以内核中的任务可能会被另一任务抢占。
	* 睡眠及用户空间的同步――――在内核执行的进程可能会睡眠，这就会唤醒调度程序，从
	  而导致调度一个新的用户进程执行。
	* 对称多处理――――两个或多个处理器可以同时执行代码。
	
	对内核开发者来说，必须理解上述这些并发执行的原因，并且为它们事先做足准备工作。
	如果在一段内核代码操作某资源的时候系统产生了一个中断，而且该中断的处理程序还要访
	问这一资源，这就是一个bug；类似地，如果一段内核代码在访问一个共享资源期间可以被抢
	占，这也是一个bug；还有，如果内核代码在临界区里睡眠，那简直就是鼓掌欢迎竞争条件的
	到来。最后还要注意，两个处理器绝对不能同时访问同一共享数据。当我们清楚什么样的数
	据需要保护时，提供锁来保护代码安全也就不难做到了。然而，真正困难的就是发现上述的
	潜在并发执行的可能，并有意识的采取某些措施来防止并发执行。我们要重申这点，因为它
	实在是很重要。其实，真正用锁来保护共享资源并不困难，在设计代码的早期就这么做了，
	事情就更简单了。辩认出真正需要共享的数据和相应的临界区，才是真正有挑战性的地方。
	要记住，最开始设计代码的时候就要考虑加入锁，而不是事后才想到。如果代码已经写好了，
	再在其中找到需要上锁的部分并向其中追加锁，是非常困难的，结果也往往不尽人意。所以，
	这里的基本原则是：在编写代码的开始阶段就要设计恰当的锁。
	
	在中断处理程序中能避免并发访问的安全代码称作中断安全代码（interrupt_saft），在对
	称多处理的机器中能避免并发访问的安全代码称为SMP安全代码（SMP-safe），在内核抢占时
	能避免并发访问的安全代码称为抢占安全代码（preempt-saft）。在下一章会重点讲述为了提
	供同步和避免所有上述竞争条件，内核所使用的实际方法。
	

7.2.2 需要保护什么

	找出哪些数据需要保护是关键所在。由于任何可能被并发访问的代码都可能需要保护，
	所以寻找哪些代码不需要保护反而相对更容易些，我们也就从这里入手。执行线程的局部数
	据仅仅被它本身访问，显然不需要保护，比如，局部自动变量不需要任何形式的锁，因为它们
	独立存在于执行线程的栈中。类似的， 如果数据只会被特定的进程访问，那么也不需要加锁。
	
	到底什么数据需要加锁呢？大多数内核数据结构都需要加锁！有一条很好的经验可以帮
	助我们判断：如果有其他执行线程可以访问这些数据，那么就给这些数据加上某种形式的
	锁；如果任何其他什么东西能看到它，那么就要锁住它。
	
	在编写内核代码时，你要问自己下面这些问题：
	* 这个数据是不是全局的？除了当前线程外，其他线程能不能访问它？
	* 这个数据会不会在进程上下文和中断上下文中共享？它是不是要在两个不同的中断处理
	  程序中共享？
	* 进程在访问数据时可不可能被抢占？被调度的新程序会不会访问同一数据？
	* 当前进程是不是会睡眠（阻塞）在某些资源上，如果是，它会让共享数据处于何种状
	  态？
	* 怎样防止数据失控？
	* 如果这个函数又在另一个处理器上被调度将会发生什么呢？
	* 你要对这些代码做什么？
	
	简而言之，几乎访问所有的内核全局变量和共享数据都需要某种形式的同步方法，具体
	加锁方法将在下章进行讨论。
	
	
7.3 死锁

	死锁的产生需要一定条件：要有一个或多个执行线程和一个或多个资源，每个线程都在
	等待其中的一个资源，但所有的资源都已经被占用了。所有线程都在相互等待，但它们永远
	不会释放已经占有的资源。于是任何线程都无法继续，这便意味着死锁的发生。
	
	一个很好的死锁例子是四路交通堵塞问题。如果每一个停止的车都决心等待其他的车开
	动后自己再启动，那么就没有任何一辆车能启动，于是交通死锁发生。
	
	最简单的死锁例子是自死锁：如果一个执行线程试图去获得一个自己已经持有的锁，
	它将不得不等待锁被释放，但因为它正在忙着等待这个锁，所以自己永远也不会有机会释放
	锁，最终结果就是死锁：
		获得锁
		再次试图获得锁
		等待锁重新可用
		...
		
	同样道理，考虑有n个线程和n个锁，如果每个线程都持有一把其他进程需要等到的锁，
	那么所有的线程都将阻塞地等待它们希望得到的锁重新可用。最常见的例子是有两个线程和
	两把锁，它们通常被叫做ABBA死锁。
	
		线程1				线程2
		获得锁A				获得锁B
		试图获得锁B			试图获得锁A
		等待锁B				等待锁A
		
	每个线程都在等待其他线程持有的锁，但是绝没有一个线程会释放它们一开始就持有的锁，
	所以没有任何锁会释放后被其他线程使用。这种类型的死锁也叫做致使拥抱（deadly embrace）。
	
	预防死锁的发生非常重要，虽然很难证明代码不会发生死锁，但是可以写出避免死锁的
	代码，一些简单的规则对避免死锁大有帮助：
		* 加锁的顺序是关键。使用嵌套的锁时必须保证以相同的顺序获取锁，这样可以阻止致使
		  拥抱类型的死锁。最好能记录下锁的顺序，以便其他人也能照此顺序使用。
		* 防止发生饥饿，试问，这个代码的执行是否一定会结束？如果“张”不发生？“王”要
		  一直等待下去吗？
		* 不要重复请求同一个锁。
		* 越复杂的加锁方案越有可能造成死锁――――设计应力求简单。
		
	最值得强调的是第一点，它最为重要。如果有两个或多个锁曾在同一时间里被请求，那
	么以后其他函数请求它们也必须按照前次的加锁顺序进行。假设有cat、dog和fox这几个锁来
	保护某同名的多个数据结构，也假设有一个函数同时对这三个锁保护的数据结构进行操
	作――――可能在它们之间进行拷贝。不管哪种情况，这些数据结构都需要保护才能被安全访
	问。如果有一个函数以cat，dog，然后是fox的顺序获得了锁，那么其他任何函数都必须以同样
	的顺序来获得这些锁（或是它们的子集）。如果其他函数首先获得锁fox，然后获得锁dog（因
	为锁dog总是应该先于锁fox被获得)，就有发生死锁的可能（所以是个bug）。为更直观地说明，
	下面给出一个造成死锁的例子。
			
			线程1					线程2
			获得锁cat				获得锁fox
			获得锁dog				试图获得锁dog
			试图获得锁fox			等待锁dog
			等待锁fox				...
			
	线程1在等待锁fox，而该锁此刻被线程2持有；同样线程2正在等待锁dog，而该锁此刻又
	被线程1持有。任何一方都不会放弃自己已持有的锁，于是双方都会永远的等待下去――――也就
	是死锁。但是，只要线程都按照相同的顺序去获取这些锁，就可以避免上述的死锁情况。
	
	只要嵌套地使用多个锁，就必须按照相同的顺序去获取它们。在代码中使用锁的地方，
	对锁的获取顺序加上注释是个良好的习惯。下面的例子就做得很不错：
		/*
		 * cat_lock ――――总是要在获得锁dog前先获得
		 * （并且锁dog总是应该在获得锁fox前获得）
		 */
	
	注意，尽管释放锁的顺序和死锁是无关的，但最好还是以获得锁的相反顺序来释放锁。
	
	防止死锁非常重要，所以Linux内核提供了一些简单易用的调试工具，可以在运行时检测
	死锁，我们将在下一章讨论它们。
	
	
7.4 争用和扩展性

	锁的争用（lock connection），或简称争用，是指当锁正在被占用时，有其他线程试图获得
	该锁。说一个锁处于高度争用状态，就是指有多个其他线程在等待获得该锁。由于锁的作用
	是使程序以串行方式对资源进行访问，所以使用锁无疑会降低系统的性能。被高度争用的锁
	会成为系统的瓶颈，严重降低系统性能。即使是这样，相对于被几个相互抢夺共享资源的线
	程撕成碎片，搞得内核崩溃，还是这种同步保护来得好一点。当然，如果有办法能解决高度
	争用问题，就更好不过了。
	
	扩展性（Scalability）是对系统可拓展程序的一个量度。对于操作系统，我们在谈及可拓
	展性时就会和大量进程、大量处理器、或是大量内存等联系起来。其实任何可以被计量的计
	算机组件都可以涉及到可扩展性。理想情况下，处理器的数量加倍应该会使系统处理性能翻
	倍。而实际上，这是不可能达到的。
	
	自从2.0内核引入多处理支持后，Linux对集群处理器的可扩展性大大提高了。在Linux刚
	加入对多处理器支持的时候，一个时刻只能有一个任务在内核中执行；在2.2版本中，当加锁
	机制发展到细粒度（fine-grained）加锁后，便取消了这种限制，而在2.4和后续版本中，内核加
	锁的粒度变得越来越精细。
	
	加锁粒度用来描述加锁保护的数据规模。一个过粗的锁保护大块数据――――比如，一个子
	系统用到的所有的数据结构；相反，一个过于精细的锁保护很小的一块数据――――比如，一个
	大数据结构中的一个元素。在实际使用中，绝大多数锁的加锁范围都处于上述两种极端之间，
	保护的既不是一个完整的子系统也不是一个独立元素，而可能是一个单独的数据结构。许多
	锁的设计在开始阶段都很粗，但是当锁的争用问题变得严重时，设计就向更加精细的加锁方
	向进化。
	
	在第3章中讨论过的运行队列，就是一个锁从粗到细精化的实例。在2.4版和更早的内核
	中，调度程序有一个单独的调度队列（回忆一下，调度队列是一个由可调度进程组成的链表），
	在2.6版中，O(1)调度程序为每个处理器单独配备一个运行队列，每个队列拥有自己的锁，于
	是加锁由一个全局锁精化到了每个处理器拥有各自的锁。
	
	一般来说，提高可扩展性是件好事，因为它可以提高Linux在更大型的、处理能力更强大
	的系统上的性能。但是一味的“提高”可扩展性，却会导致Linux在小型SMP和UP机器上的
	性能降低，这是因为小型机器可能用不到特别精细的锁，锁得过细只会增加复杂度，并加大
	开销。考虑一个链表，最初的加锁方案可能就是用一个锁来保护链表，后来发现，在拥有集
	群处理器机器上，当各个处理器需要频繁访问该链表的时候，只用单独一个锁却成了扩展性
	的瓶颈。为解决这个瓶颈，我们将原来加锁的整个链表变成为链表中的每一个结点都加入自
	已的锁，这样一来，如果要对结点进行读写，必须先得到这个结点对应的锁。将加锁粒度变
	细后，多处理器访问同一个结点时，只会争用一个锁。可是这时锁的争用仍然没有完全避免，
	那么，能不能为每个结点中的每个元素都提供一个锁呢？（答案是：不能）。严格地讲，即使
	这么细的锁可以在大规模SMP机器上执行得很好，但它在双处理器机器上的表现又会怎样
	呢？如果在双处理器机器锁争用表现得并不明显，那么多余的锁会加大系统开销，造成很
	大浪费。
	
	不管怎么说，可扩展性都是很重要的，需要慎重考虑。关键在于，在设计锁的开始阶段
	就应该考虑到要保证良好的扩展性。因为即使在小型机器上，如果对重要资源锁得太粗，也
	很容易造成系统性能瓶颈。锁加得过粗或过细，差别往往在一线之间，当锁争用严重时，加
	锁太粗会降低可扩展性；而锁争用不明显时，加锁过细会加大系统开销，带来浪费，这两种
	情况都会造成系统性能下降。但要记住：设计初期加锁方案应该力求简单，仅当需要时再进
	一步细化加锁方案。精髓在于力求简单。
	

7.5 小结

	要编写SMP安全代码，不能等到编码完成后才考虑如何加锁。恰当的同步（也就是加
	锁）――――既要满足不死锁、可扩展，而且还要清晰、简洁――――需要从头到尾，在整个编码过
	程中不断考虑与完善。无论你在编写哪种内核代码，是新的系统调用也好，还是重写驱动程
	序也好，首先应该考虑的就是保护数据不被并发访问，记住，加锁你的代码。
	
	下一章将讨论如何为SMP、内核抢占和其他各种情况提供充分的同步保护，确保数据在
	任何机器和配置中的安全。
	
	
	
*****************************************************************************************
	
									第8章 内核同步方法

*****************************************************************************************


	上一章讨论了竞争条件为何会产生以及怎么去解决。幸运的是，Linux内核提供了一组相
	当完备的同步方法。这一章讨论的就是这些方法，包括它们的接口、行为和用途。
	
	
8.1 原子操作

	原子操作可以保证指令以原子的方式执行――――执行过程不被打断。众所周知，原子原本
	指的是不可分割的微粒，所以原子操作也就是不能够被分割的指令。例如，前一章曾提到过
	的原子方式的加操作，它通过把读取和增加变量的行为包含在一个单步中执行，从而防止了
	竞争的发生，保证了操作结果总是一致的（假定i初值是7）：
	
		线程1						线程2
		increment i(7->8)			-
		-							increment i(i->9)
		
	最后得到的9，毫无疑问是正确结果。两个原子操作绝对不可能并发地访问同一个变量，
	这样加操作也就绝不可能引起竞争
	
	内核提供了两组原子操作接口――――一组针对整数进行操作，另一组针对单独的位进行操
	作。在Linux支持的所有体系结构上都实现了这两组接口。大多数体系结构要么本来就支持简
	单的原子操作，要么就为单步执行提供了锁内存总线的指令（这就确保其他操作不能同时发
	生）。而有些体系结构本身就不太支持原子操作，比如SPARC，但还是有一些办法可想，而且
	也确实做到了。
	
	
8.1.1 原子整数操作

	针对整数的原子操作只能对atomic_t类型的数据进行处理。在这里之所以引入了一个特殊
	数据类型，而没有直接使用C语言的int类型，主要是出于两个原因：首先，让原子函数只接
	受atomic_t类型的操作搂，可以确保原子操作只与这种特殊类型数据一起使用。同时，这也保
	证了该类型的数据不会被传递给其他任何非原子函数。实际上，对一个数据一会儿要采用原
	子操作，一会儿又不用原子操作了，这又能有什么好处？其次，使用atomic_t类型确保编译
	器不对（不能说完美地完成了任务但不乏自知之明）相应的值进行访问优化――――这点使得原
	子操作最终接收到正确的内存地址，而不只是一个别名。最后，在不同体系结构上实现原子
	操作的时候，使用atomic_t可以屏蔽其间的差异。
	
	尽管Linux支持的所有机器上的整型数据都是32位的，但是使用atomic_t的代码只能将该
	类型的数据当作24位来用。这个限制完全是因为在SPARC体系结构上，原子操作的实现不同
	于其他体系结构：32位int类型的低8位被嵌入了一个锁中（如图8-1所示），因为SPARC体系结
	构对原子操作缺乏指令级的支持，所以只能利用该锁来避免对原子类型数据的并发访问。所
	以在SPARC机器上就只能使用24位了。虽然其他机器上的代码完全可以使用这全部的32位，
	但在SPARC机上却可能造成一些奇怪和微妙的错误――――这简直太不和谐了。
	
	使用原子整型操作需要的声明都在<asm/atomic.h>文件中。有些体系结构会提供一些只能
	在该体系结构上使用的额外原子操作方法，但所有的体系结构都能保证内核使用到的所有操作
	的最小集。在你写内核代码时，可以肯定，这个最小操作集在所有体系结构上都已实现好了。
	
	定义一个atomic_t类型的数据方法很平常，你还可以在定义时给它设定初值：
		atomic_t u;						// 定义 u
		atomic_t v = ATOMIC_INIT(0);	// 定义 v 并把它初始化为0
		
	操作也都非常简单：
		atomic_set(&v,4);		// v = 4 (原子地)
		atomic_add(2,&v);		// v = v + 2 = 6 (原子地)
		atomic_inc(&v);			// v = v + 1 = 7 (原子地)
		
	如果需要将atomic_t转换成int型，可以使用atomic_read()来完成：
		printk("%d\n", atomic_read(&v));  // 会打印"7"
		
	原子整数操作最常见的用途就是实现计数器。使用复杂的锁机制来保护一个单纯的计数
	器是很笨拙的，所以，开发者最好使用atomic_inc()和atomit_dec()这两个相对来说轻便一点的
	操作。还可以用原子整数操作原子地执行一个操作并检查结果。一个常见的例子就是原子的
	减操作和检查。
	
		int atomic_dec_and_test(atomic_t *v)
		
	这个函数将给定的原子变量减1，如果结果为0，就返回真；否则返回假。表8-1列出了所
	有的标准原子整数操作（所有体系结构都包含这些操作）。某种特定的体系结构上实现的所有
	操作可以在文件<asm/atomic>中找到。
	
								表8-1 原子整数操作列表
				原子整数操作								描述
		ATOMIC_INIT(int i)							在声明一个atomic_t变量时，将它初始化为i
		int atomic_read(atomic_t *v)				原子地读取整数变量v
		void atomic_set(atomic_t *v, int i)			原子地设置v值为1
		void atomic_add(int i,atomic_t *v)			原子地给v加i
		void atomic_sub(int i,atomic_t *v)			原子地从v减i
		void atomic_inc(atomic_t *v)				原子地给v加1
		void atomic_dec(atomic_t *v)				原子地给v减1
		int atomic_sub_and_test(int i,atomic_t *v)	原子地从v减i，如果结果等于0返回真，否则返回假
		int atomic_add_negative(int i,atomic_t *v)	原子地给v加i，如果结果是负数返回真，否则返回假
		int atomic_dec_and_test(atomic_t *v)		原子地给v减1，如果结果为0返回真，否则返回假
		int atomic_inc_and_test(atomic_t *v)		原子地给v加1，如果结果是0返回真，否则返回假
		
	原子操作通常是内联函数，往往是通过内嵌汇编指令来实现的（内核开发者喜欢使用内
	联函数）。如果某个函数本来就是原子的，那么它往往会被定义成一个宏。例如在大部分健
	全的体系结构上，读取一个字本身就是一种原子操作，也就是说，在对一个字进行写入操作
	的期间不可能完成对该字的读取。这样，把atomic_read()定义成一个宏，只须返回atomic_t类
	型的整数值就可以了。
	
	在你编写代码的时候，能使用原子操作的时候，就尽量不要使用复杂的加锁机制。对多
	数体系结构来讲，原子操作与更复杂的同步方法相比较，给系统带来的开销小，对高速缓存
	行(cache-line)的影响也小。但是，对于那些有高性能要求的代码，对多种同步方法进行测
	试比较，不失为一种明智的作法。
	
	
8.1.2 原子位操作

	除了原子整数操作外，内核也提供了一组针对位这一级数据进行操作的函数。没什么好
	奇怪的，它们是与体系结构相关的操作，定义在文件<asm/bitops.h>中。
	
	令人感到奇怪的是位操作函数是对普通的内存地址进行操作的。它的参数是一个指针和
	一个位号，第0位是给定地址的最低有效位。在32位机上，第31位是给定地址的最高有效位而
	第32位是下一个字的最低有效位。虽然使用原子位操作在多数情况下是对一个字长的内存进
	行访问，因而位号应该位于0~31之间（在64位机器中是0~63之间），但是，对位号的范围并
	没有限制。
	
	由于原子位操作是对普通的指针进行的操作，所以不像原子整型对应atomic_t，这里没有
	特殊的数据类型。相反，只要指针指向了任何你希望的数据，你就可以对它进行操作。来看
	一个例子：
		unsigned long word = 0;
		set_bit(0,&word);			// 第0位被设置（原子地）
		set_bit(1,&word);			// 第1位被设置（原子地）
		printk("%ul\n",word);		// 打印3
		clear_bit(1,&word);			// 清空第1位
		change_bit(0,&word);		// 翻转第0位的值，这里它被清空
		
		/* 原子地设置第0位并且返回设置前的值(0) */
		if (test_and_set_bit(0, &word)) {
									// 永远不为真...
		}
	
	在表8-2中给出了标准原子位操作列表：
	
							表8-2 原子位操作的列表
				原子位操作									描述
		void set_bit(int nr, void *addr)			原子地设置addr所指对象的第nr位
		void clear_bit(int nr, void *addr)			原子地清空addr所指对象的第nr位
		void change_bit(int nr, void *addr)			原子地翻转addr所指对象的第nr位
		int test_and_set_bit(int nr, void *addr)	原子地设置addr所指对象的第nr位，并
													返回原先的值
		int test_and_clear_bit(int nr, void *addr)	原子地清空addr所指对象的第nr位，并
													返回原先的值
		int test_and_change_bit(int nr, void *addr)	原子地翻转addr所指对象的第nr位，并
													返回原先的值
		int test_bit(int nr, void *addr)			原子地返回addr所指对象的第nr位
		
	为方便起见，内核还提供了一组与上述操作对应的非原子位函数。非原子位函数与原子
	位函数的操作完全相同，但是，前者不保证原子性，且其名字前缀多两个下划线。例如，与
	test_bit()对应的非原子形式是__test_bit()。如果你不需要原子性操作（比如说，如果你已经用
	锁保护了自己的数据），那么这些非原子的位函数相比原子的位函数可能会执行得更快些。
	
	内核还提供了两个例程用来从指定的地址开始搜索第一个被设置（或未被设置）的位。
		int find_first_bit(unsigned long *addr, unsigned int size)
		int find_first_zero_bit(unsigned long *addr, unsigned int size)
		
	这两个函数中第一个参数是一个指针，第二个参数是要搜索的总位数，返回值分别是第
	一个被设置的（或没被设置的）位的位号。如果你的搜索范围仅限于一个字，使用__ffs()
	和__ffz()这两个函数更好，它们只需给定一个要搜索的地址做参数。
	
	与原子整数操作不同，代码一般无法选择是否使用位操作――――它们是唯一的、具有可移植
	性的设置特定位方法，需要选择的是使用原子位操作还是非原子位操作。如果你的代码本身
	已经避免了竞争条件，你可以使用非原子位操作，通常这样执行得更快，当然，这还要取决
	于具体的体系结构。
	
	
8.8 自旋锁

	如果每个临界区都能像增加变量这样简单就好了，可惜现实总是残酷的。现实世界里，
	临界区甚至可以跨越多个函数。举个例子，我们经常会碰到这种情况：先得从一个数据结构
	中移出数据，对其进行格式转换和解析，最后再把它加入到另一个数据结构中。整个执行过
	程必须是原子的，在数据被更新完毕前，不能有其他代码读取这些数据。显然，简单的原子
	操作对此无能为力，这就需要使用更为复杂的同步方法――――锁来提供保护。
	
	Linux内核中最常见的锁是自旋锁(spin lock)。自旋锁最多只能被一个可执行线程持有。
	如果一个执行线程试图获得一个被争用（已经被持有）的自旋锁，那么该线程就会一直进行
	忙循环――――旋转――――等待锁重新可用。要是锁未被争用，请求锁的执行线程便能立刻得到它，
	继续执行。在任意时间，自旋锁都可以防止多于一个的执行线程同时进入临界区。注意同一
	个锁可以用在多个位置――――例如，对于给定数据的所有访问都可以得到保护和同步。
	
	一个被争用的自旋锁使得请求它的线程在等待锁重新可用时自旋（特别浪费处理器时间），
	所以自旋锁不应该被长时间持有。事实上，这点正是使用自旋锁的初衷：在短期间内进行轻
	量级加锁。还可以采取另外的方式来处理对锁的争用：让请求线程睡眠，直到锁重新可用时
	再唤醒它。这样处理器就不必循环等待，可以去执行其他代码。这也会带来一定的开销――――
	这里有两次明显的上下文切换，被阻塞的线程要换出和换入。因此，持有自旋锁的时间最好
	小于完成两次上下文切换的耗时。当然我们大多数人都不会无聊到去测量上下文切换的耗时，
	所以我们让持有自旋锁的时间应尽可能的短就可以了。下一节讨论信号量，信号量便提供
	了上述第二种锁机制，它使得在发生争用时，等待的线程能投入睡眠，而不是旋转。

	自旋锁的实现和体系结构密切相关，代码往往通过汇编实现。这些体系结构相关的代
	码定义在文件<asm/spinlock.h>中，实际需要用到的接口定义在文件<linux/spinlock.h>中。自
	旋锁的基本使用形式如下：
		spinlock_t_mr_lock = SPIN_LOCK_UNLOCKED;
		spin_lock(&mr_lock);
		/* 临界区 */
		spin_unlock(&mr_lock);
		
	因为自旋锁在同一时刻至多被一个执行线程持有，所以一个时刻只能有一个线程位于临
	界区内，这就为多处理器机器提供了防止并发访问所需的保护机制。注意在单处理器机器上，
	编译的时候并不会加入自旋锁。它仅仅被当作一个设置内核抢占机制是否被启用的开关。如
	果禁止内核抢占，那么在编译时自旋锁会被完全剔除出内核。

	警告：自旋锁是不可递归的！
	Linux内核实现的自旋锁是不可递归的，这点不同于自旋锁在其他操作系统中的实
	现。所以如果你试图得到一个你正持有的锁，你必须自旋，等待你自己释放这个锁。
	但你牌自旋忙等待中，所以你永远没有机会释放锁，于是你被自己锁死了。千万小
	心自加锁！
	
	自旋锁可以使用在中断处理程序中（此处不能使用信号量，因为它们会导致睡眠）。在中
	断处理程序中使用自旋锁时，一定要在获取锁之前，首先禁止本地中断（在当前处理器上的
	中断请求），否则，中断处理程序就会打断正持有锁的内核代码，有可能会试图去争用这个已
	经被持有的自旋锁。这样一来，中断处理程序就会自旋，等待该锁重新可用，但是锁的持有
	者在这个中断处理程序执行完毕前不可能运行。这正是我们在前一章节中提到的双重请求死
	锁。注意，需要关闭的只是当前处理器上的中断。如果中断发生在不同的处理器上，即使中
	断处理程序在同一锁上自旋，也不会妨碍锁的持有者（在不同的处理器上）最终释放锁。
	
	内核提供的禁止中断同时请求锁的接口，使用起来很方便，方法如下：
		spinlock_t mr_lock=SPIN_LOCK_UNLOCKED;
		unsigned long flags;
		
		spin_lock_irqsave(&mr_lock,flags);
		/* 临界区 */
		spin_unlock_irqrestore(&mr_lock,flags);

	函数spin_lock_irqsave()保存中断的当前状态，并禁止本地中断，然后再去获取指定的锁。
	反过来spin_lock_irqsave()对指定的锁解锁，然后让中断恢复到加锁前的状态。所以即使中断
	最初是被禁止的，你的代码也不会错误地激活它们。注意，flags变量看起来像是由数值传递
	的，这是因为这些锁函数有些部分是通过宏的方式实现的。
	
	在单处理器系统上，虽然在编译时抛弃掉了锁机制，但在上面例子中仍需要关闭中断，
	以禁止中断处理程序访问共享数据。加锁和解锁分别可以禁止和允许内核抢占。
	
	锁什么？
		使用锁的时候一定要对症下药，要有针对性。要知道需要保护的是数据而不是
		代码。尽管本章的例子讲的都是保护临界区的重要性，但是真正保护的其实是临界
		区中的数据，而不是代码。针对代码的加锁会使得程序难以理解，并且容易引发竞争
		条件。正确的做法应该是用特定的锁来保护数据。例如，"struct foo由loo_lock加锁"。
		最好给每个全局数据都关联一个对应的锁。无论你何时需要访问数据，一定要先保
		证数据是安全的。而保证数据安全往往就意味着在对数据进行操作前，首先占用恰
		当的锁，完成操作后再释放它。
		
	如果你能确定中断在加锁前是激活的，那就不需要在解锁后恢复中断以前的状态了。你
	可以无条件地在解锁时激活中断。这时，使用spin_lock_irq()和spin_unlock_irq()会更好一些。
	
		spinlock_t mr_lock = SPIN_LOCK_UNLOCKED;
		spin_lock_irq(&mr_lock);
		/* 临界区 */
		spin_unlock_irq(&mr_lock);
		
	但是，在内核的执行路线上，你很难搞清楚中断在当前调用节点上到底是不是处于激活状态。
	也正因为如此，我们并不提倡使用spin_lock_irq()方法。如果你一定要使用它，那你应该要确
	定中断原来就处于未激活状态，否则当其他人突然发现中断处于激活状态的时候，可能会非
	常不开心。
	
	调试自旋锁
		配置选项CONFIG_DEBUG_SPINLOCK为使用自旋锁的代码加入了许多调试检
		测手段。例如，激活了该选项，内核就会检查是否使用了未初始化的锁，是否在还
		没加锁的时候就要对锁执行开锁操作。在测试代码时，总是应该激活这个选项。
		
		
8.2.1 其他针对自旋锁的操作

	spin_lock_init()用来初始化动态创建的自旋锁（此时你只有一个指向spinlock_t类型的指
	针，没有它的实体）。
	
	spin_try_lock()试图获得某个特定的自旋锁，如果该锁已经被争用，那么该方法会立刻返
	回一个非0值，而不会自旋等待锁被释放；如果成功地获得了这个自旋锁，该函数返回0。同
	理，spin_is_locked()方法用于检查特定的锁当前是否已被占用，如果已被占用，返回非0值；
	否则返回0。该方法只做判断，并不实际占用。
	
	表8-3给出了标准的自旋锁操作的完整列表：
							表8-3 自旋锁方法列表
		
		spin_lock()				获取指定的自旋锁
		spin_lock_irq()			禁止本地中断并获取指定的锁
		spin_lock_irqsave()		保存本地中断的当前状态，禁止本地中断，并获取指定的锁
		spin_unlock()			释放指定的锁
		spin_unlock_irq()		释放指定的锁，并激活本地中断
		spin_unlock_irqrestore()释放指定的锁，并让本地中断恢复到以前状态
		spin_lock_init()		初始化指定的spinlock_t
		spin_trylock()			试图获取指定的锁；如果未获取，则返回非0
		spin_is_locked()		如果指定的锁当前正在被获取，则返回非0，否则，返回0
		
	
8.2.2 自旋锁和下半部

	在第6章中曾经提到，在与下半部配合使用时，必须小心地使用锁机制。函数
	spin_lock_bh()用于获取指定的锁，同时它会禁止所有下半部的执行。相应的spin_unlock_bh()函
	数执行相反的操作。
	
	由于下半部可以抢占进程上下文中的代码，所以当下半部和进程上下文共享数据时，必
	须对进程上下文中的共享数据进行保护，所以需要加锁的同时还要禁止下半部执行。同样，
	由于中断处理程序可以抢占下半部，所以如果中断处理程序和下半部共享数据，那么就必须
	在获取恰当的锁的同时还要禁止中断。
	
	回忆一下，同类的tasklet不可能同时运行，所以对于同类tasklet中的共享数据不需要保护。
	但是当数据被两个不同种类的tasklet共享时，就需要在访问下半部中的数据前先获得一个普通
	的自旋锁。这里不需要禁止下半部，因为在同一个处理器上决不会有tasklet相互抢占的情况。
	
	对于软中断，无论是否同种类型，如果数据被软中断共享，那么它必须得到锁的保护。
	这是因为即使是同种类型的两个软中断也可以同时运行在一个系统的多个处理器上。但是，
	同一处理器上的一个软中断绝不会抢占另一个软中断，因此，根本没必要禁止下半部。
	
	
8.3 读-写自旋锁

	有时，锁的用途可以明确地分为读取和写入。例如，对一个链表可能既要更新又要检索。
	当更新（写入）链表时，不能有其他代码并发地写链表或从链表中读取数据，写操作要求完
	全互斥。另一方面，当对其检索（读取）链表时，只要其他程序不对链表进行写操作就行了。
	只要没有写操作，多个并发的读操作都是安全的。任务链表（在第2章中讨论过）就非常类似
	于这种情况，它就是通过读-写自旋锁获得保护的。
	
	当对某个数据结构的操作可以像这样被划分为读/写两种类别时，类似读/写锁这样机制就
	很有帮助了。为此，Linux提供了专门的读-写自旋锁。这种自旋锁为读和写分别提供了不同
	的锁。一个或多个读任务可以并发的持有读者锁；相反，用于写的锁最多只能被一个写任务
	持有，而且此时不能有并发的读操作。读/写自旋锁的使用方法类似于普通自旋锁，它们通过
	下面的方法初始化：
		rwlock_t mr_rwlock = RW_LOCK_UNLOCKED;
	然后，在读者的代码分支中使用如下函数：
		read_lock(&mr_rwlock);
		/* 临界区（只读） */
		read_unlock(&mr_rwlock);
	最后，在写者的代码分支中使用如下函数：
		write_lock(&mr_rwlock);
		/* 临界区（读写） */
		write_unlock(&mr_rwlock);
		
	通常情况下，读锁和写锁会位于完全分割开的代码分支中，如上例所示。
	注意，不能把一个读锁“升级”为写锁。这种代码：
		read_lock(&mr_rwlock);
		write_lock(&mr_rwlock);
		
	将会带来死锁，因为写锁会不断自旋，等待所有读者释放锁，其中也包括它自己。所以当
	确实需要写操作时，要在一开始就请求写锁。如果写和读不能清晰地分开的话，那么使用一
	般的自旋锁就行了，不要使用读-写自旋锁。
	
	多个读者可以安全地获得同一个锁，事实上，即使一个线程递归地获得同一读锁也是
	安全的。这个特性使得读-写自旋锁真正成为一种有用并且常用的优化手段。如果在中断处
	理程序中只有读操作而没有写操作，那么，就可以混合使用“中断禁止”锁，使用
	read_lock()而不是read_lock_irqsave()对读进行保护。不过，你还是需要用write_lock_irqsave()
	禁止有写操作的中断，否则，中断里的读操作就有可能锁死在写锁上（译者注：假如读者正
	在进行操作，包含写操作的中断发生了，由于读锁还没有全部被释放，所以写操作会自旋，
	而读操作只能在包含写操作的中断返回后才能继续，释放读锁，此时死锁就发生了）。表8-4
	列出了针对读-写自旋锁的所有操作。
	
							表8-4 读-写自旋锁方法列表
			方法										描述
		read_lock()						获得指定的读锁
		read_lock_irq()					禁止本地中断并获得指定读锁
		read_lock_irqsave()				存储本地中断的当前状，禁止本地中断并获得指定读锁
		read_unlock()					释放指定的读锁
		read_unlock_irq()				释放指定的读锁并激活本地中断
		read_unlock_irqrestore()		释放指定的读锁并将本地中断恢复到指定的前状态
		write_lock()					获得指定的写锁
		write_lock_irq()				禁止本地中断并获得指定写锁
		write_lock_irqsave()			存储本地中断的当前状态，禁止本地中断并获得指定写锁
		write_unlock()					释放指定的写锁
		write_unlock_irq()				释放指定的写锁并激活本地中断
		write_unlock_irqrestore()		释放指定的写锁并将本地中断恢复到指定的前状态
		write_trylock()					试图获得指定的写锁；如果写锁不可用，返回非0值
		rw_lock_init()					初始化指定的rwlock_t
		rw_is_locked()					如果指定的锁当前已被持有，该函数返回非0值，否则返回0
		
	在使用Linux读-写自旋锁时，最后要考虑的一点是这种锁机制照顾读比照顾写要多一点。
	当读锁被持有时，写操作为了互斥访问只能等待，但是，读者却可以继续成功地作占用锁。
	而自旋等待的写者在所有读者释放锁之前是无法获得锁的。所以，大量读者必定会使挂起的
	写者处于饥饿状态，在你自己设计锁时一定要记住这一点。
	
	自旋锁提供了一种快速简单的锁实现方法。如果加锁时间不长并且代码不会睡眠（比如
	中断处理程序），利用自旋锁是最佳的选择。如果加锁时间可能很长或者代码在持有锁时有可能
	睡眠，那么最好使用信号量来完成加锁功能。
	
	
8.4 信号量

	Linux中的信号量是一种睡眠锁。如果有一个任务试图获得一个已经被占用的信号量时，
	信号量会将其推进一个等待队列，然后让其睡眠。这时处理器能重获自由，从而去执行其他
	代码。当持有信号量的进程将信号量释放后，处于等待队列中的那个任务将被唤醒，并获
	得该信号量。
	
	我们可以从信号量的睡眠特性得出一些有意思的结论：
		* 由于争用信号量的进程在等待锁重新变为可用时会睡眠，所以信号量适用于锁会被长时
		  间持有的情况。
		* 相反，锁被短时间持有时，使用信号量就不太适宜了。因为睡眠引起的时耗可能比锁被
		  占用的全部时间还要长。
		* 由于执行线程在锁被争用时会睡眠，所以只能在进程上下文中才能获取信号量锁，因为
		  在中断上下文中是不能进行调度的。
		* 你可以在持有信号量时去睡眠（当然你也可能并不需要睡眠），因为当其他进程试图获
		  得同一信号量时不会因此而死锁。（因为该进程也只是去睡眠而已，而你最终会继续执
		  行的。）
		* 在你占用信号量的同时不能占用自旋锁。因为在你等待信号量时可能会睡眠，而在持有
		  自旋锁时是不允许睡眠的。
		  
	以上这些结论阐明了信号量和自旋锁在使用上的差异。在使用信号量的大多数时候，你
	的选择余地并不大。往往在需要和用户空间同步时，你的代码会需要睡眠，此时使用信号量
	是唯一的选择。由于不受睡眠的限制，使用信号量通常来说更加容易一些。如果需要在自旋
	锁和信号量中作选择，应该根据锁被持有的时间长短做判断。理想情况当然是所有的锁定操
	作都应该越短越好。但如果你用的是信号量，那么锁定的时间长一点也能够接受。另外，信
	号量不同于自旋锁，它不会禁止内核抢占，所以持有信号量的代码可以被抢占。这意味着信
	号量不会对调度等待时间带来负面影响。
	
	最后要讨论的是信号量的一个有用特性，它可以同时允许任意数量的锁持有者，而自旋
	锁在一个时刻最多允许一个任务持有它。信号量同时允许的持有者数量可以在声明信号量时
	指定。这个值称为使用者数量（usage count）或简单地叫数量（count）。通常情况下，信号量
	和自旋锁一样，在一个时刻仅允许有一个锁持有者。这时计数等于1，这样的信号量被称为二
	值信号量（因为它或者由一个任务持有，或者根本没有任务持有它）或者称为互斥信号量
	（因为它强制进行互斥）。另一方面，初始化时也可以把数量设置为大于1的非0值。这种情况，
	信号量被称为计数信号量（counting semaphone），它允许在一个时刻至多有count个锁持有者。
	计数信号量不能用来进行强制互斥，因为它允许多个执行线程同时访问临界区。相反，这种
	信号量用来对特定代码加以限制，内核中使用它的机会不多。在使用信号量时，基本上你用
	到的都是互斥信号量（计数等于1的信号量）。
	
	信号量在1968年由Edsger Wybe DijKstra提出，此后它逐渐成为一种常用的锁机制。信
	号量支持两个原子操作P()和V()，这两个名字来自荷兰语Proberen和Vershogen。前者做测试
	操作（字面意思是探查），后者叫做增加操作。后来的系统把两种操作分别叫做down()和up()，
	Linux也遵从这种叫法。down()操作通过对信号量计数减1来请求获得一个信号量。如果结果
	是0或大于0，获得信号量锁，任务就可以进入临界区。如果结果是负数，任务会被放入等待
	队列，处理器执行其他任务。该函数如同一个动词，降低（down）一个信号量就等于获取该
	信号量。相反，当临界区中的操作完成后，up()操作用来释放信号量，该操作也被称作是提升
	（upping）信号量，因为它会增加信号量的计数值。如果在该信号量上的等待队列不为空，那
	么处于队列中等待的任务在被唤醒的同时会获得该信号量。
	
	
8.4.1 创建和初始化信号量

	信号量的实现是与体系结构相关的，具体实现定义在文件<asm/semaphore.h>中。struct
	semaphore类型用来表示信号量。可以通过以下方式静态地声明信号量：
		static DECLARE_SEMAPHORE_GENERIC(name, count);
		
	其中name是信号量变量名，count是信号量的使用者数量。创建更为普通的互斥信号量可
	以使用以下快捷方式：
		static DECLARE_MUTEX(name);
		
	name同样指信号量变量名。更常见的情况是，信号量作为一个大数据结构的一部分被动
	态创建。此时，你只有指向该动态创建的信号量的间接指针，你可以使用如下函数来对它进
	行初始化：
		sema_init(sem, count);
		
	sem是指针，count是信号量的使用者数量。与前面类似，初始化一个动态创建的互斥信
	号量时使用以下函数：
		init_MUTEX(sem);
		
	
8.4.2 使用信号量

	函数down_interruptible()试图获取指定的信号量，如果获取失败，它将以TASK_
	INTERRUPTIBLE状态进入睡眠。回忆第2章的内容，这种进程状态意味着任务可以被信号唤
	醒，一般来说这是件好事。如果进程在等待获取信号量的时候接收到了信号，那么该进程就
	会被唤醒，而函数down_interruptible()会返回-EINTER。另外一个函数down()会让进程在
	TASK_UNINTERRUPTIBLE状态下睡眠。你应该不希望这种情况发生，因为这样一来，进程
	在等待信号量的时候就不再响应信号了。因此，使用down_interruptible()比使用down()更为普
	遍。也许你会觉得这两个函数名字起得有点有恰当，的确，这些命名并不很理想。
	
	使用down_trylock()函数，你可以尝试以非堵塞方式来获取指定的信号量。在信号量已被
	占用时，它立刻返回非0值；否则，它返回0，而且让你成功持有信号量锁。
	
	要释放指定的信号量，需要调用up()函数。请看例子：
		static DECLARE_MUTEX(mr_sem);
		...
		if (down_interruptible(&mr_sem))
			/* 信号被接收，信号量还未获取 */
		
		/* 临界区  */

		up(&mr_sem);
		
	表8-5给出了针对信号量的操作的完整列表。
								表8-5 信号量方法列表
			方法													描述
		sema_init(struct semaphore *, int)			以指定的计数值初始化动态创建的信号量
		init_MUTEX(struct semaphore *)				以计数值1初始化动态创建的信号量
		init_MUTEX_LOCKED(struct semaphore *)		以计数值0初始化动态创建的信号量（初始为加锁
													状态）
		down_interruptible(struct semaphore *)		试图获得指定的信号量，如果信号量已被争用，
													则进入可中断睡眠状态
		down(struct semaphore *)					试图获得指定的信号量，如果信号量已被争用，
													则进入不可中断睡眠状态
		down_trylock(struct semaphore *)			试图获得指定的信号量，如果信号量已被争用，
													则立刻返回非0值
		up(struct semaphore *)						释放指定的信号量，如果睡眠队列不空，则唤醒
													其中一个任务


8.5 读-写信号量

	与自旋锁一样，信号量也有区分读-写访问的可能。与读-写自旋锁和普通自旋锁之间
	的关系差不多，读-写信号量也要比普通信号量更具优势。
	
	读-写信号量在内核中是由rw_semaphore结构表示的，定义在文件<linux/rwsem.h>中。
	通过以下语句可以创建静态声明的读-写信号量：
	
		static DECLARE_RWSEM(name);
		
	其中name是新信号量名。
	
	动态创建的读-写信号量都是互斥信号量（也就是说，它们的引用计数等于1）。只要没有写
	者，并发持有读锁的读者数不限。相反，只有唯一的写者（在没有读者时）可以获得写锁。
	所有读-写锁的睡眠都不会被信号打断，所以它只有一个版本的down()操作。例如：
		static DECLARE_RWSEM(mr_rwsem);
		
		...
		
		down_read(&mr_rwsem);
		
		/* 临界区（只读）... */

		up_read(&mr_rwsem);
		
		...
		
		down_write(&mr_rwsem);
		
		/* 临界区（读和写）... */

		up_write(&mr_rwsem);
		
	与标准信号量一样，读-写信号量也提供了down_read_trylock()和down_write_trylock()方
	法。这两个方法都需要一个指向读-写信号量的指针作为参数。如果成功获得了信号量锁，
	它们返回非0值；如果信号量锁被争用，则返回0。要小心――――不知道为什么要这样――――这与
	普通信号量的情形完全相反。
	
	读-写信号量相比读-写自旋锁多一种特有的操作：downgrade_writer()。这个函数可以
	动态地将获取的写锁转换为读锁。
	
	读-写信号量和读-写自旋锁一样，除非代码中的读和写可以明白无误地分割开来，否
	则最好不使用它。再强调一次，读者-写者机制使用是有条件的，只有在你的代码可以自然
	地界定出读/写时才有价值。
	
	了解何时使用自旋锁，何时使用信号量对编写优良代码很重要，但是多数情况下，并不
	需要太多的考虑，因为在中断上下文中只能使用自旋锁，而在任务睡眠时只能使用信号量。
	
	表8-6回顾了一下各种锁的需求情况。
	
						表8-6 使用什么：自旋锁与信号量的比较
			需求											建议的加锁方法
		低开销加锁										优先使用自旋锁
		短期锁定										优先使用自旋锁
		长期加锁										优先使用信号量
		中断上下文中加锁								使用自旋锁
		持有锁需要睡眠									使用信号量
	
	
8.6 完成变量
	
	如果在内核中一个任务需要发出信号通知另一任务发生了某个特定事件，利用完成变量
	(completion variables)是使用两个任务得以同步的简单方法。如果一个任务要执行一些工作时，
	另一个任务就会在完成变量上等待。当这个任务完成工作后，会使用完成变量去唤醒在等待
	的任务。这听起来很像一个信号量，的确如此――――思想是一样的。事实上，完成变量仅仅提
	供了代替信号量的一个简单的解决方法。
	
	完成变量由结构completion表示，定义在<linux/completion>中。通过以下宏静态地创建
	完成变量并初始化它：
		DECLARE_COMPLETION(mr_comp);
	
	通过init_completion()动态创建并初始化完成变量。
	
	在一个指定的完成变量上，需要等待的任务调用wait_for_completion()来等待特定事件。
	当特定事件发生后，产生事件的任务调用complete()来发送信号唤醒正在等待的任务。表8-7
	列出了完成变量的方法。
	
								表8-7 完成变量方法
			方法											描述
		init_completion(struct completion *)			初始化指定的动态创建的完成变量
		wait_for_completion(struct completion *)		等待指定的完成变量接受信号
		complete(struct completion *)					发信号唤醒任何等待任务
		
	使用完成变量的例子可以参考kernel/sched.c和kernel/fork.c。完成变量的通常用法是，将
	完成变量作为数据结构中的一项动态创建，而完成数据结构初始化工作的内核代码将调用
	wait_for_completion()进行等待。初始化完成后，初始化函数调用completion()唤醒在等待的内
	核任务。
	
	
8.7 BKL

	欢迎来到内核的原始混沌时期。BKL（大内核锁）是一个全局自旋锁，使用它主要是为了
	方便实现从Linux最初的SMP过度到细粒度加锁机制。我们下面来介绍BKL的一些有趣的特性：
	
		* 持有BKL的任务仍然可以睡眠。因为当任务无法被调度时，所加锁会自动被丢弃；当任
		  务被调度时，锁又会被重新获得。当然，这并不是说，当任务持有BKL时，睡眠是安全
		  的，仅仅是可以这样做，因为睡眠不会造成任务死锁。
		* BKL是一种递归锁。一个进程可以多次请求一个锁，并不会像自旋锁那样产生死锁现象。
		* BKL可以用在进程上下文中。
		* BKL是有害的。
		
	这些特性有助于2.0版本的内核向2.2版本过渡。在SMP支持被引入到2.0版本时，内核中
	一个时刻上只能有一个任务运行。（当然，经过长期发展，现在内核已经被很好的线程化了）。
	2.2版本的目标是允许多处理器在内核中并发执行程序。引入BKL是为了使到细粒度加锁机
	制的过度更容易些，虽然当时BKL对内核过度很有帮助，但是目前它已成为内核可扩展性的
	障碍了。
	
	在内核中不鼓励使用BKL。事实上，新代码中不再使用BKL，但是这种锁仍然在部分内核
	代码中得到沿用。所以我们仍然需要理解BKL以及它的接口。除了前面提到的以外，BKL的
	使用方式和自旋锁类似。函数lock_kernel()请求锁，unlock_kernel()释放锁。一个执行线程可
	以递归的请求锁，但是，释放锁时也必须调用同样次数的unlock_kernel()操作，在最后一个解
	锁操作完成后，锁才会被释放。函数kernel_locked()检测锁当前是否被持有，如果被持有，返
	回一个非0值，否则返回0。这些接口被声明在文件<linux/smp_lock.h>中，简单的用法如下：
		Lock_kernel();
		/* 临界区，对所有其他的BKL用户进行同步
		 * 注意，你可以安全地在此睡眠，锁会悄无声息地被释放。
		 * 当你的任务被重新调度时，锁又会被悄无声息地获得。
		 * 这意味着你不会处于死锁状态，但是，如果你需要锁保护这里的数据，
		 * 你还是不需要睡眠
		 */
		unlock_kernel();
		
	BKL在被持有时同样会禁止内核抢占。在单一处理器内核中，BKL并不执行实际的加锁
	操作。表8-8列出了所有BKL函数。
						表8-8 BKL函数列表
			函数								描述
		lock_kernel()				获得BKL
		unlcok_kernel()				释放BKL
		kernel_locked()				如果锁被持有返回非0值，否则返回0（UP总是返回非零）
		
	对于BKL最主要的问题是确定BKL锁保护的到底是什么。多数情况下，BKL更像是保护代
	码（比如“它保护对foo()函数的调用者进行同步”）而不保护数据（比如“保护结构foo”）。
	这个问题给利用自旋锁取代BKL造成了很大困难，因为难以判断BKL到底锁的是什么，更难
	的是发现所有使用BKL的用户之间的关系。
	
	
8.8 Seq锁

	Seq锁是在2.6内核版本中才引入的一种新型锁。这种锁提供了一种简单的机制，用于
	读写共享数据。实现这种锁主要依靠一个序列计数器。当有疑义的数据被写入时，会得到一
	个锁，并且序列值会增加。在读取数据之前和之后，序列号都被读取。如果读取的序列号值
	相同，说明在读操作进行的过程中没有被写操作打断过。此外，如果读取的值是偶数，那么
	就表明写操作没有发生（要明白因为锁的初值是0，所以写锁会使值成奇数，释放的时候变成
	偶数）。定义一个Seq锁的形式为：
		seqlock_t mr_seq_lock = SEQLOCK_UNLOCKED;
	
	然后，写锁的方法如下：
		write_seqlock(&mr_seq_lock);
		/* 写锁被获取... */
		write_sequnlock(&mr_seq_lock);
	
	这和普通的自旋锁类似，不同的情况发生在读时，与自旋锁有很大不同:
		unsigned long seq;
		do {
			seq = read_seqbegin(&mr_seq_lock);
			/* 读这里的数据... */
		} while(read_seqretry(&mr_seq_lock, seq));

	在多个读者和少数写者共享一把锁的时候，seq锁有助于提供一种非常轻量级和具有可扩
	展性的外观。但是Seq锁对写者更有利。只要没有其他写者，写锁总是能够被成功获得，读
	者不会影响写锁，这点和读者-写者自旋锁及信号量一样。另外，挂起的写者会不断地使得
	读操作循环（前一个例子），直到不再有任何写者持有锁为止。


8.9 禁止抢占

	由于内核是抢占性的，内核的进程在任何时刻都可能停下来以便另一个具有更高优先
	权的进程运行。这意味着一个任务与被抢占的任务可能会在同一个临界区内运行。为了避免
	这种情况，内核抢占代码使用自旋锁作为非抢占区域的标记。如果一个自旋锁被持有，内核
	便不能进行抢占。因为内核抢占和SMP面对相同的并发问题，并且内核已经是SMP安全的
	（SMP_safe），因此，这种简单的变化使得内核也是抢占安全的（preempt_safe）。
	
	或许这就是我们希望的。实际中，某些情况并不需要自旋锁，但是仍然需要关闭内核抢
	占。最频繁出现的情况就是每个处理器上的数据。如果数据对每个处理器是唯一的，那么，
	这样的数据可能就不需要使用锁来保护，因为数据只能被一个处理器访问。如果自旋锁没有
	被持有，内核又是抢占式的，那么一个新调度的任务就可能访问同一个变量，如下所示：
		
		任务A操作变量foo
		任务A被抢占
		任务B被调度
		任务B操作变量foo
		任务B完成
		任务A被调度
		任务A操作变量foo
	
	这样，即使这是一个单处理器计算机，变量foo也会被多个进程以伪并发的方式访问。通
	常，这个变量会请求得到一个自旋锁（防止多处理器机器上的真并发）。但是如果这是每个
	处理器上独立的变量，可能就不需要锁。
	
	为了解决这个问题，可以通过preempt_disable()禁止内核抢占。这是一个可以嵌套调用的
	函数，可以调用任意次。每次调用都必须有一个相应的preempt_enable()调用。当最后一次
	preempt_enable()被调用后，内核抢占才重新启用。例如：
	
		preempt_disable();
		/* 抢占被禁止... */
		preempt_enable();
		
	抢占计数存放着被持有锁的数量和preempt_disable()的调用次数，如果计数是0，那么内
	核可以进行抢占，如果为1或更大的值，那么，内核就不会进行抢占。这个计数非常有用――――
	它是一种对原子操作和睡眠很有效的调试方法。函数preempt_count()返回这个值。表8-9列出
	了内核抢占相关的函数。
							表8-9 内核抢占的相关函数
			函数											描述
		preempt_disable()					禁止内核抢占
		preempt_enable()					激活内核抢占并且检查和执行被挂起的需调度的任务
		preempt_enable_no_resched()			激活内核抢占但不再进行调度
		preempt_count()						返回抢占计数
		
	为了用更简洁的方法解决每个处理器上的数据访问问题，你可以通过get_cpu()获得处理
	器编号（假定是用这种编号来对每个处理器的数据进行索引的）。这个函数在返回当前处理器
	号前首先会关闭内核抢占。
	
		int cpu = get_cpu();
		/* 对每个处理器的数据进行操作 */
		put_cpu(); // 完成，重新激活内核抢占
		
		
8.10 屏障

	当处理多处理器之间或硬件设备之间的同步问题时，有时需要在你的程序代码中以指定
	的顺序发出读内存（读入）和写内存（存储）指令。在和硬件交互时，时常需要确保一个给
	定的读操作发生在其他读或写操作之前。另外，在多处理器上，可能需要按写数据的顺序读
	数据（通常确保后来以同样的顺序进行读取）。但是编译器和处理器为了提高效率，可能对读
	和写重新排序，这样无疑使问题复杂化了。幸好，所有可能重新排序和写的处理器提供了
	机器指令来确保顺序要求。同样也可以指示编译器不要对给定点周围的指令序列进行重新排
	序。这些确保顺序的指令称作屏障(barriers)。
	
	基本上说，在某些处理器上，存在以下代码：
		a=1;
		b=2;
		
	有可能会在a中存放新值之前就在b中存放新值。但是处理器和编译器绝不会对下面的代
	码重新排序：
		a = 1;
		b = a;
	
	因为a和b之间有明确的数据依赖关系。但是不管是编译器还是处理器都不知道其他上下文中
	的相关代码。偶然情况下，有必要让写操作被其他代码识别，也让你所期望的指定顺序之外
	的代码识别。这种情况常常发生在硬件设备上，但是在多处理器机器上也很常见。
	
	rmb()方法提供了一个“读”内存屏障，它确保跨越rmb()的载入动作不会发生重排序。也
	就是说，在rmb()之前的载入操作不会被重新排在该调用之后，同理，在rmb()之后的载入操
	作不会被重新排在该调用之前。
	
	wmb()方法提供了一个“写”内存屏障，这个函数的功能和rmb()类似，区别仅仅是它是
	针对存储而非载入-它确保跨越屏障的存储不发生重排序。
	
	mb()方法既提供了读屏障也提供了写屏障。载入和存储动作都不会跨越屏障重新排序。
	这是因为一条单独的指令（通常和rmb()使用同一个指令）既可以提供载入屏障，也可以提供
	存储屏障。
	
	read_barrier_depends()和rmb()的变种，它提供一个读屏障，但是仅仅是针对后续读操作
	所依靠的那些载入。因为屏障后的读操作依赖于屏障前的读操作，因此，该屏障确保屏障前
	的读操作在屏障后的读操作之前完成。明白了吗？基本上说，该函数设置一个读屏障，如
	rmb()，但是只针对特定的读――――也就是那些相互依赖的读操作。在有些体系结构上，
	read_barrier_depends()比rmb()执行得快，因为它仅仅是个空操作，实际并不需要。
	
	看看使用了mb()和rmb()的一个例子，其中a的初始值是1，b的初始值是2。
	
			线程1					线程2
			
			a = 3;					-
			mb();					-
			b = 4;					c = b;
			-						rmb();
			-						d = a;
			
	如果不使用内存屏障，在某些处理器上，c可能接受了b的新值，而d接收了a原来的值。
	比如c可能等于4（正是我们希望的）然而d可能等待1（不是我们希望的）。使用mb()能确保a
	和b按照预定的顺序写入，而rmb()确保c和d按照预定的顺序读取。
	
	这种重排序的发生是因为现代处理器为了优化其传送管道（pipeline），打乱了分派和提
	交指令的顺序。如果上例中读入a、b时的顺序被打乱的话，又会发生什么情况呢？rmb()或
	wmb()函数相当于指令，它们告诉处理器在继续执行前提交所有尚未处理的载入或存储指令。
	
	看一个类似的例子，但是其中一个线程用read_barrier_depends代替了rmb()。例子中a的
	初始值是1，b的初始值是2，p是&b。
	
			线程1					线程2
			
			a = 3;					-
			mb();					-
			p = &a;					pp = p;
			-						read_barrier_depends();
			-						b = *pp;
	
	再一次声明，如果没有内存屏障，有可能在pp被设置成p前，b就被设置为pp了。由于载
	入*pp依靠载入p，所以read_barrier_depends()提供了一个有效的屏障。虽然使用rmb()同样有
	效，但是因为读是数据相关的，所以我们使用read_barrier_depends()可能更快。注意，不管在
	哪种情况下，左边的线程都需要mb()操作来确保预定的载入或存储顺序。
	
	宏smp_rmb()、smp_wmb()、smp_mb()和smp_read_barrier_depends()提供了一个有用的优
	化。在SMP内核中它们被定义成常用的内存屏障，而在单处理机内核中，它们被定义成编译
	器的屏障。对于SMP系统，在有顺序限定要求时，可以使用SMP的变种。
	
	barriers()方法可以防止编译器跨屏障对载入或存储操作进行优化。编译器不会重新组织存
	储或载入操作而防止改变C代码的效果和现有数据的依赖关系。但是，它不知道在当前上下文
	之外会发生什么事。例如，编译器不可能知道有中断发生，这个中断有可能在读取正在被写
	入的数据。这时就要求存储操作发生在读取操作前。前面讨论的内存屏障可以完成编译器屏
	障的功能，但是编译器屏障要比内存屏障轻量（它实际上是轻快）得多。
	
	表8-10给出了内核中所有体系结构提供的完整的内存和编译器屏障方法。
	
							表8-10 内存和编译器屏障方法
			屏障											描述
			
		rmb()							阻止跨越屏障的载入动作发生重排序
		read_barrier_depends()			阻止跨越屏障的具有数据依赖关系的载入动作重排序
		wmb()							阻止跨越屏障的存储动作发生重排序
		mb()							阻止跨越屏障的载入和存储动作重新排序
		smp_rmb()						在SMP上提供rmb()功能，在UP上提供barrier()功能
		smp_read_barrier_depends()		在SMP上提供read_barrier_depends()功能，在UP上提
										供barrier()功能
		smp_wmb()						在SMP上提供wmb()功能，在UP上提供barrier()功能
		smp_mb()						在SMP上提供mb()功能，在UP上提供barrier()功能
		barrier()						阻止编译器跨屏障对载入或存储操作进行优化
		
	注意，对于不同体系结构，屏障的实际效果差别很大。例如，如果一个体系结构不执行
	打乱存储（比如Intel x86芯片就不会），那么wmb()就什么也不做。但应该为最坏的情况（即
	排序能力最弱的处理器）使用恰当的内存屏蔽，这样代码才能在编译时执行针对体系结构的
	优化。
	
	
	
	
	
*****************************************************************************************

								第9章 定时器和时间管理
								
*****************************************************************************************


	时间管理在内核中占有非常重要的地位。相对于事件驱动而言，内核中有大量的函数
	都是基于时间驱动的。其中有些函数是周期执行的，像对调度程序中的运行队列进行平衡调
	整或对屏幕进行刷新这样的函数，都需要定期执行，比如说，每秒执行100次；有些函数，
	比如需要推后执行的磁盘I/O操作等，则需要等待一个相对时间后才运行――――比如说，内核会
	在500毫秒后再执行某个任务。除了上述两种函数需要内核提供时间外，内核还必须管理系统
	的运行时间以及当前日期和时间。
	
	请注意相对时间和绝对时间之间的差别。如果某个事件在5秒后被调度执行，那么系统所
	需要的不是绝对时间――――而是相对时间（比如，相对现在起5秒后）；相反，如果要求管理当
	前日期和当前时间，则内核不但要计算流逝的时间而且还要计算绝对时间。所以这两种时间
	概念对内核时间管理来说都至关重要。
	
	另外还请注意周期性产生的事件与推迟执行的事件之间的差别。周期性产生的事件――――
	比如每10毫秒一次――――都是由系统定时器驱动的。系统定时器是一种可编程硬件芯片，它能
	以固定频率产生中断。该中断就是所谓的定时器中断，它所对应的中断处理程序负责更新系
	统时间，也负责执行需要周期性运行的任务。系统定时器和时钟中断处理程序是Linux系统内
	核管理机制中的中枢，本章将着重讨论它们。
	
	另外一个关注的焦点是动态定时器――――一种用来推迟执行程序的工具。比如说，如果软
	驱马达在一定时间内都未活动，那么软盘驱动程序会使用动态定时器关闭软驱马达。内核可
	以动态创建或销毁动态定时器。本章将介绍动态定时器在内核中的实现，同时给出在内核代码
	中可以供使用的定时器接口。
	
	
9.1 内核中的时间概念

	时间概念对计算机来说有些模糊，事实上内核必须在硬件的帮助下才能和计算和管理时间。
	硬件为内核提供了一个系统定时器用以计算流逝的时间，该时钟在内核中可看成是一个电子
	时间资源，比如数字时钟处理器频率等。系统定时器以某种频率自行触发（经常被称为击
	中(hitting)或射中(poping)）时钟中断，该频率可以通过编程预定，称作节拍率（tick rate）。
	当时钟中断发生时，内核就通过一种特殊的中断处理程序对其进行处理。
	
	因为预编的节拍率对内核来说是可知的，所以内核知道连续两次时钟中断的间隔时间。这
	个间隔时间就称为节拍（tick），它等于节拍率分之一（one-over-the-tick-rate）秒。正如你所
	看到的，内核就是靠这种已知的时钟中断间隔来计算墙上时间和系统运行时间。墙上时间――――
	也就是实际时间――――对用户空间的应用程序来说是最重要的。内核通过控制时钟中断维护实际
	时间，另外内核也为用户空间提供了一组系统调用以获取实际日期和实际时间。系统运行时
	间――――自系统启动开始所经过的时间――――对用户空间和内核都很有用，因为许多程序都必须清
	楚流逝过的时间。通过再次（现在和以后）读取运行时间再计算它们的差，就可以得到相对
	的流逝过的时间了。
	
	时钟中断对于管理操作系统尤为重要，大量内核函数的生命周期都离不开流逝的时间的
	控制。下面给出一些利用时间中断周期执行的工作：
	
		* 更新系统运行时间。
		* 更新实际时间。
		* 在SMP系统上，均衡调度程序中各处理器上的运行队列。如果运行队列负载不均衡的话，
		  尽量使它们均衡（在第3章讨论过）。
		* 检查当前进程是否用尽了自己的时间片。如果用尽，就重新进行调度（在第3章中讨论过）。
		* 运行超时的动态定时器。
		* 更新资源消耗和处理器时间的统计值。
		
	这其中有些工作在每次的时钟中断处理程序中都要被处理――――也就是说这些工作随时钟
	的频率反复运行。另一些也是周期性的执行，但只需要每n次时钟中断运行一次，也就是说，
	这些函数在累计了一定数量的时钟节拍数时才被执行。在9.5节，我们将详细讨论时钟中断处
	理程序。
	

9.2 节拍率：Hz

	系统定时器频率（节拍率）是通过静态预处理定义的，也就是Hz（赫兹），在系统启动时
	按照Hz值对硬件进行设置。体系结构不同，Hz的值也不同，实际上，对于某些体系结构来说，
	甚至是机器不同，它的值都会不一样。
	
	内核在文件<asm/param>中定义了Hz的实际值，节拍率就等于Hz，周期为1/Hz秒。比如，
	i386体系结构在include/asm-i386/param.h中对Hz值定义如下：
		#define Hz 1000    /* 内核时间频率 */
		
	可以看到i386体系结构中系统定时器频率为1000Hz，也就是说每秒钟时钟中断1000次
	（每毫秒产生一次）。但其他体系结构的节拍率绝大多数都等于100。表9-1给出了各种体系结
	构与各自对应节拍率的完整列表。
	
	编写内核代码时，不要认为Hz值是一个固定不变的值。这不是一个常见的错误，因为大
	多数体系结构的节拍率都是可调的。但是在过去，只有Alpha一种机型的节拍率不等于100，
	所以很多本该使用Hz的地方，都错误地在代码中直接硬编码（hard-code）成100这个值。稍
	后，我们会给出内核代码中使用Hz的例子。
	
	正如我们所看到的，时钟中断能处理许多内核任务，所以它对内核来说极为重要。事实上，
	内核中的全部时间概念都来源于周期运行的系统时钟。所以选择一个合适的频率，就如同在
	人际交往中建立和谐关系一样，必须取得各方面的折衷。
	
	理想的Hz值
	
	自Linux问世以来，i386体系结构中时钟频率就设定为100Hz，但是在2.5开发版内核
	中，中断频率被提高到1000Hz。当然，是否应该提高频率（如同其他绝大多数事情一样）是
	饱受争议的。由于内核中众多子系统都必须依赖时钟中断工作，所以改变中断频率必然会对
	整个系统造成很大的冲击。但是，任何事情总是有两面性的，我们接下来就来分析定时
	器使用高频率与使用低频率各有哪些优劣。
	
	提高节拍率意味着时钟中断产生得更加频繁，所以中断处理程序也会更频繁地执行。如
	此一来会给整个系统带来如下好处：
		* 更高的时钟中断解析度（resolution）可提高时间驱动事件的解析度。
		* 提高了时间驱动事件的准确度（accuracy）。
		
	提高节拍率等同于提高中断解析度。比如Hz=100的时钟的执行粒度为10毫秒，即系统中
	的周期事件最快为每10毫秒运行一次，而不可能有更高的精度，但是当Hz=1000时，解析度
	就为1毫秒――――精细了10倍。虽然内核可以提供频度为1毫秒的时钟，但是并没有证据显示对
	系统中所有程序而言，频率为1000Hz的时钟率相比频率为100Hz的时钟都更合适。
	
	另外，提高解析度的同时也提高了准确度。假定内核在某个随机时刻触发定时器，而它
	可能在任何时间超时，但由于只有在时钟中断到来时才可能执行它，所以平均误差大约为半
	个时钟中断周期。比如说，如果时钟周期为Hz=100，那么事件平均在设定时刻的+/-5毫秒内
	发生，所以平均误差为5毫秒。如果Hz=1000，那么平均误差可降低到0.5毫秒――――准确度提
	高了10倍。
	
	更高的时钟中断频度和更高的准确度又会带来如下优点：
	  * 内核定时器能够以更高的频度和更高的准确度（它带来了大量的好处，下一条便是其中
	    之一）运行。
	  * 依赖定时值执行的系统调用，比如poll()和select()，能够以更高的精度运行。
	  * 对诸如资源消耗和系统运行时间等的测量会有更精细的解析度。
	  * 提高进程抢占的准确度。
	  
	对poll()和select()超时精度的提高会给系统性能带来极大的好处。提高精度可以大幅度提
	高系统性能。频繁使用上述两种系统调用的应用程序往往在等待时钟中断上浪费大量的时间，
	而事实上，定时值可能早就超时了。回忆一下，平均误差（也就是，可能浪费的时间）可是
	时钟中断周期的一半。
	
	更高的准确率也使进程抢占更准确，同时还会加快调度响应时间。第3章中提到过，时钟
	中断处理程序负责减少当前进程的时间片计数。当时间片计数跌到0时，而又设置了
	need_resched标志的话，内核便立刻重新运行调度程序。假定有一个正在运行的进程，它的
	时间片只剩下2毫秒了，此时如果调度程序要求抢占该进程，然后去运行另一个新进程，然而
	该抢占行为不会在下一个时钟中断到来前发生，也就是说，在这2毫秒内不可能进行抢占。实
	际上，对于频率为100Hz的时钟来说，最坏要在10毫秒后，当下一个时钟中断到来时才能进行
	抢占，所以新进程也就可能要比要求的晚10毫秒才能执行。当然，进程之间也是平等的，因
	为对所有的进程都是一视同仁的，调度起来都不是很准确――――但关键不在于此。问题在于由
	于耽误了抢占，所以对于类似于填充音频缓冲区这样有严格时间要求的任务来说，结果是无
	法接受的。如果将节拍率提高到1000Hz，在最坏的情况下，也能将调度延误时间降低到1毫秒，
	平均情况下，能降到0.5毫秒左右。
	
	现在该谈谈另一面了，提高节拍率会产生负作用。事实上，提高节拍率即把节拍率提高
	到1000Hz（甚至更高）会带来一个大问题：节拍率越高，意味着时钟中断频率越高，也就意味
	着系统负担越重。因为处理器必须花时间来执行时钟中断处理程序，所以节拍率越高，中断处
	理程序占用的处理器的时间越多。这样不但减少了处理器处理其他工作的时间，而且还会更
	频繁地打乱处理器高速缓存。负载造成的影响值得进一步的探讨。将时钟频率从100Hz提高到
	1000Hz必然会使时钟中断的负载增加10倍。可是增加前的系统负载又是多少呢？如果原来负
	载为0，那么即使增加10倍，不是还为0吗？最后的结论是：至少在现代计算机系统上，时钟
	频率为1000Hz不会导致难以接受的负担。尽管如此，在2.6内核中还是允许在编译内核时选定
	不同的Hz值。
	
	
9.3 jiffies

	全局变量jiffies用来记录自系统启动以来产生的节拍的总数。启动时，内核将该变量初始
	化为0，此后，每次时钟中断处理程序都会增加该变量的值。因为一秒内时钟中断的次数等于
	Hz，所以jiffies一秒内增加的值也就为Hz。系统运行时间以秒为单位计算，就等于jiffies/Hz。
	
	jiffies定义于文件<linux/jiffies.h>中：
		extern unsigned long volatile jiffies;
		
	下一节，我们会看到它的实际定义，它看起来有点特殊。现在我们先来看一些用到jiffies
	的内核代码。将以秒为单位的时间转化为jiffies:
		(seconds * HZ)
		
	相反，将jiffies转换为以秒为单位的时间：
		(jiffies/HZ)
		
	比较而言，内核中将秒转换为jiffies用得多一些，比如代码经常需要设置一些将来的时间：
		unsigned long time time_stamp = jiffies;   // 现在
		unsigned long next_tick = jiffies + 1;     // 从现在开始1个节拍
		unsigned long later = jiffies + 5*HZ;      // 从现在开始5秒
		
	上面这种操作经常会用在内核和用户空间进行交互的时候，而内核本身很少用到绝对时间。
	注意jiffies类型为无符号长整型（unsigned long），用其他任何类型存放它都不正确。
	
	
9.3.1 jiffies的内部表示

	和过去相比，jiffies在内核中的表示比较怪异，2.6内核中它的变量类型从无符号长整型
	（unsigned long）变为了u64，换句话说，即使对32位的机器，也使用无符号的64位整型表示
	jiffies。这个64位的jiffies变量在文件<linux/jiffies.h>中声明：
		extern u64 jiffies_64;
	
	32位的jiffies变量，在时钟频率为100Hz的情况下，497天后会溢出。如果频率为1000Hz，
	49.7天后就会溢出。而使用64位的jiffies变量，任何人都别指望会看到它溢出。下一节我们将讨
	论溢出带来的潜在问题（虽然我们并不希望节拍计数溢出，但这却很常见，好像总是会出现）。
	
	从32位到64位转换会带来一些问题，但是内核开发者们巧妙地处理了这些问题。由于现
	存的代码引用了jiffies，而且大多数代码只涉及它的低32位，所以原来的jiffies变量依然在使
	用。内核开发者们通过一些神奇的连接技术（linker scripts），将原有的jiffies变量和新的
	jiffies_64变量搁在了一起。图9-1给出了jiffies和jiffies_64上的布局。
	
	访问jiffies的代码仅会读取jiffies_64的低32位。通过get_jiffies_64()函数，就可以读取整
	个64位的数值。但是这种需求很少，多数代码仍然只要能通过jiffies变量读取低32位就够了。
	
	在64位体系结构上，jiffies_64和jiffies指的是同一个变量，代码既可以直接读取jiffies也
	可以调用get_jiffies_64()函数，它们的作用相同。
	
	
9.3.2 jiffies的回绕

	和任何C整型一样，当jiffies变量的值超过它的最大存放范围后就会发生溢出。对于32位
	无符号长整型，最大取值为2的32次方减1。所以在溢出前，定时器节拍计数最大为4294967295。如果
	节拍计数达到了最大值后还要继续增加的话，它的值会回绕（wraparound）到0。
	
	请看下面一个回绕的例子：
	
		unsigned long timeout = jiffies + HZ/2;    // 0.5秒后超时
		/* 执行一些任务，然后查看是否花的时间过长 ...*/
		if (timeout > jiffies) {
			/* 没有超时，很好 ... */
		} else {
			/* 超时了，发生错误...*/
		}
		
	上面这一小段代码是希望设置一个准确的超时时间――――从现在开始计时，时间为半秒。
	然后再去处理一些工作，比如探测硬件然后等待它的响应。如果处理这些工作的时间超过了
	设定的超时时间，代码就要做相应的出错处理。
	
	这里有很多种发生溢出的可能，我们只分析其中之一：考虑如果在设置完timeout变量后，
	jiffies重新回绕为0将会发生什么？此时，第一个判断会返回假，因为尽管实际上用去的时间
	可能比timeout值要大，但是由于溢出后回绕为0，所以jiffies这时肯定会小于timeout的值。
	jiffies本该是个非常大的数值――――大于timeout，但是因为超过了它的最大值，所以反而变成了
	一个很小的值――――也许仅仅只有几个节拍计数。由于发生了回绕，所以if判断语句的结果刚好
	相反。
	
	幸好，内核提供了四个宏来帮助比较节拍计数，它们能正确地处理了节拍计数回绕情况。
	这些宏定义在文件<linux/jiffies.h>中：
	
		#define time_after(unknown,known)          ((long)(known) - (long) (unknown)<0)
		#define time_before(unknown,known)         ((long)(unknown) - (long) (known)<0)
		#define time_after_eq(unknown,known)       ((long)(unknown) - (long) (known)>=0)
		#define time_before_eq(unknown,known)      ((long)(known) - (long) (unknown)>=0)
	
	其中unknown参数通常是jiffies，known参数是需要对比的值。
	
	宏time_after(unknown,known)，当时间unknown超过指定的known时，返回真，否则返
	回假；宏time_before(unknown,known)，当时间unknown没超过指定的known时，返回真，否则
	返回假。后面两个宏作用和前面两个宏一样，只有当两个参数相等时，它们才返回真。
	
	所以前面的例子可以改造成时钟-回绕-安全（timer-wraparound-safe）的版本，形式如下：
	
		unsigned long timeout = jiffies + HZ/2;   // 0.5秒后超时
		/* ... */
		if (time_after(jiffies,timeout)) {
				/* 没有超时，很好... */
		} else { 
				/* 超时了，发生错误... */
		}
		
	如果你对这些宏能避免因为回绕而产生的错误而感到好奇，那么，你可以试一试对这两个参
	数取不同的值。然后，设定一个参数回绕到0值，看看会发生什么。
	
	
9.3.3 用户空间和Hz

	在以前如果改变内核中的Hz的值会给用户空间中某些程序造成异常结果。这是因为内核是
	以节拍数/秒的形式给用户空间导出这个值的，在这个接口稳定了很长一段时间后，应用程序
	便逐渐依赖于这个特定的Hz值了。所以如果在内核中更改了Hz的定义值，就打破了用户空间
	的常量关系――――用户空间并不知道新的Hz值。所以用户空间可能认为系统运行时间已经是20
	个小时了，但实际上系统仅仅启动了两个小时。
	
	要想避免上面的错误，内核必须更改所有导出的jiffies值。因而内核定义了USER_HZ来
	代表用户空间看到的Hz值。在0x86体系结构上，由于Hz值原来一直是100，所以USER_HZ就
	定义为100。内核可以使用宏jiffies_to_clock_t()将一个由Hz表示的节拍计数转换成一个由
	USER_HZ表示的节拍计数。该宏的用法取决于USER_HZ是否为Hz的整数倍或相反。当是整
	数倍时，宏的形式相当简单：
		#define jiffies_to_clock_t(x) ((x)/(HZ/USER_HZ))
		
	如果不是整数倍关系，那么该宏就得用到更为复杂的算法了。
	
	最后还要说明，内核使用函数jiffies_64_to_clock_t()将64位的jiffies值的单位从Hz转换
	为USER_HZ。
	
	在需要把以节拍/秒为单位的值导出到用户空间时，需要使用上面这几个函数。比如：
	
		unsigned long start = jiffies;
		unsigned long total_time;
		
		/* 执行一些任务 */
		total_time = jiffies - start;
		printk("That took %lu ticks\n", jiffies_to_clock_t(total_time));
	
	用户空间期望HZ=USER_HZ，但是如果它们不相等，则由宏完成转换，这样的结果自然是皆
	大欢喜。说实话，上面的例子看起来是挺简单的，如果以秒为单位而不是以节拍为单位，打
	印信息会执行得好一些。比如像下面这样：
	
		printk("That took %lu seconds \n", total_time/HZ);
		
		
9.4 硬时钟和定时器

	体系结构提供了两种设备进行计时――――一种是我们前面讨论过的系统定时器；另一种是
	实时时钟。虽然在不同机器上这两种时钟的实现并不相同，但是它们有着相同的作用和设计
	思路。
	

9.4.1 实时时钟
	
	实时时钟(RTC)是用来持久存放系统时间的设备，即使系统关闭后，它也可以靠主板
	上的微型电池提供的电力保持系统的计时。在PC体系结构中，RTC和CMOS集成在一起，而
	且RTC的运行和BIOS的保存设置都是通过同一个电池供电的。
	
	当系统启动时，内核通过读取RTC来初始化墙上时间，该时间存放xtime变量中。虽然
	内核通常不会在系统启动后再读取xtime变量，但是有些体系结构，比如x86，会周期性地将
	当前时间值存回RTC中。尽管如此，实时时钟最主要的作用仍是在启动时初始化xtime变量。
	
	
9.4.2 系统定时器

	系统定时器是内核定时机制中最为重要的角色。尽管不同体系结构中的定时器实现不尽
	相同，但是系统定时器的根本思想并没有区别――――提供一种周期性触发中断机制。有些体系
	结构是通过对电子晶振进行分频来实现系统定时器；还有些体系结构则提供了一个衰减测量
	器（decrementer）――――衰减测量器设置一个初始值，该值以固定频率递减，当减到零时，触
	发一个中断，无论哪种情况，其效果都一样。
	
	在0x86体系结构中，主要采用可编程中断时钟（PIT）。PIT在PC机器中普遍存在，而且
	从DOS时代，就开始以它作时钟中断源了。内核在启动时对PIT进行编程初始化，使其能够以
	Hz/秒的频率产生时钟中断。虽然PIT设备很简单，功能也有限，但它却足以满足我们的需要。
	x86体系结构中的其他的时钟资源还包括本地APIC时钟和时间戳计数（TSC）等。
	
	
9.5 时钟中断处理程序

	现在我们已经理解了Hz、jiffies等概念以及系统定时器的功能。下面将分析时钟中断处理
	程序是如何实现的。时钟中断处理程序可以划分为两个部分：体系结构相关部分和体系结构
	无关部分。
	
	与体系结构相关的例程作为系统定时器的中断处理程序而注册到内核中，以便在产生
	时钟中断时，它能够相应地运行。虽然处理程序的具体工作依赖于特定的体系结构，但是绝
	大多数处理程序最低限度都要执行如下工作：
		* 获得xtime_lock锁，以便对访问jiffies_64和墙上时间xtime进行保护。
		* 需要时应答或重新设置系统时钟。
		* 周期性地使用墙上时间更新实时时钟。
		* 调用体系结构无关的时钟例程：do_timer()。
		
	中断服务程序主要通过调用与体系结构无关的例程do_timer()执行下面的工作：
		* 给jiffies_64变量增加1（这个操作即使是在32位体系结构上也是安全的，因为前面已经
		  获得了xtime_lock锁）。
		* 更新资源消耗的统计值，比如当前进程所消耗的系统时间和用户时间。
		* 执行已经到期的动态定时器（下一节将讨论）。
		* 执行第3章曾讨论的sheduler_tick()函数。
		* 更新墙上时间，该时间存放在xtime变量中。
		* 计算平均负载值。
		
	因为上述工作分别都由单独的函数负责完成，所以实际上do_timer()例程的执行代码看起
	来非常简单。
		void do_timer(struct pt_regs *regs)
		{
			jiffies_64++;
			update_process_times(user_mode(regs));
			update_times();
		}
		
	user_mode()宏查询处理器寄存器regs的状态。如果时钟中断发生在用户空间，它返回1；
	如果发生在内核模式，则返回0。update_process_times()函数根据时钟中断产生的位置，对用
	户或对系统进行相应的时间更新。
	
		void update_process_times(int user_tick)
		{
			struct task_struct *p = current;
			int cpu = smp_processor_id();
			int system = user_tick^1;
			
			update_one_process(p, user_tick, system, cpu);
			run_local_timers();
			scheduler_tick(user_tick, system);
		}

	update_one_process()函数的作用是更新进程时间。它的实现是相当细致的，但是要注意，
	因为使用了XOR操作，所以user_tick和system两个变量只要其中有一个为1，则另外一个就必
	为0。update_one_process()函数可以通过判断分支，将user_tick和system加到进程相应的计数上：
	
		/* 更新恰当的时间计数器，给其加一个jiffy */
		p->utime += user;
		p->stime += system;
		
	上述操作将适当的计数值增加1，而另一个值保持不变。也许你已经发现了，这样做意味
	着内核对进程进行时间计数时，是根据中断发生时处理器所处的模式进行分类统计的，它把
	上一个tick全部算给进程。但是事实上进程在上一个节拍期间可能多次进入和退出内核模式，
	而且在上一个节拍期间，该进程也不一定是唯一一个运行进程。很不幸，没有更加精密的统
	计算法的支持，内核现在只能做到这个程度。这也是内核应该采用更高频率的另一个原因。
	
	接下来的run_lock_times()函数标记了一个软中断（请参考第6章）去处理所有到期的定时
	器，在下一节中将具体讨论定时器。
	
	最后scheduler_tick()函数负责减少当前运行进程的时间片计数值并且在需要时设置
	need_resched标志。在SMP机器中，该函数还要负责平衡每个处理器上的运行队列，这点在第
	3章曾讨论过。
	
	当update_process_times()函数返回后，do_timer()函数接着会调用update_times()函数更
	新墙上时钟。
	
		void update_times(void)
		{
			unsigned long ticks;
			
			ticks = jiffies - wall_jiffies;
			if (ticks) {
				wall_jiffies += ticks;
				update_wall_time(ticks);
			}
			last_time_offset = 0;
			calc_load(ticks);
		}
		
	ticks记录最近一次更新后新产生的节拍数。通常情况下ticks显然应该等于1。但是时钟
	中断也有可能丢失，因而节拍也会丢失。在中断长时间被禁止的情况下，就会出现这种现
	象――――但这种现象并不正常，往往是个bug。wall_jiffies值随后被加上ticks――――所以此刻
	wall_jiffies值就等于最新的墙上时间的更新值jiffies――――接着调用update_wall_time()函数更新
	xtime，最后由calc_load()值，到此，update_times()执行完毕。
	
	do_timer()函数执行完毕后返回与体系结构相关的中断处理程序，继续执行后面的工作，
	释放xtime_lock锁，然后退出。
	
	以上全部工作每1/Hz秒都要发生一次，也就是说在你的PC机上时钟中断处理程序每秒执
	行1000次。
	
	
9.6 实际时间
	
	当前实际时间（墙上时间）定义在文件kernel/timer.c中：
		struct timespec xtime;
	
	timespec数据结构定义在文件<linux/time.h>中，形式如下：
		struct timespec {
			time_t tv_sec;        /* 秒 */
			long tv_nsec;		  /* 纳秒 */
		}
		
	xtime.tv_sec以秒为单位，存放着自1970年1月1日（UTC）以来经过的时间，1970年1月1
	日被称为纪元，多数Unix系统的墙上时间都是基于该纪元而言的。xtime.tv_nsec记录自上一
	秒开始经过的纳秒数。
	
	读写xtime变量需要使用xtime_lock锁，该锁不是普通自旋锁而是一个seqlock锁，在第8章
	中曾讨论过seqlock锁。
	
	更新xtime首先要申请一个seqlock锁：
		write_seqlock(&xtime_lock);
		/* 更新xtime... */
		write_sequnlock(&xtime_lock);
		
	读取xtime时也要使用read_seqbegin()和read_seqretry()函数：
		
		do {
			unsigned long lost;
			seq = read_seqbegin(&xtime_lock);
			
			usec = timer->get_offset();
			lost = jiffies - wall_jiffies;
			if (lost) 
				usec += lost *(1000000/HZ);
			sec = xtime.tv_sec;
			usec += (xtime.tv_nsec/1000);
		} while (read_seqretry(&xtime_lock, seq));
	
	该循环不断重复，直到读者确认读取数据时没有写操作介入。如果发现循环期间有时钟
	中断处理程序更新xtime，那么read_seqretry()函数就返回无效序列号，继续循环等待。
	
	从用户空间取得墙上时间的主要接口是gettimeofday()，在内核中对应系统调用为
	sys_gettimeofday();
	
		asmlinkage long sys_gettimeofday(struct timeval *tv, struct timezone *tz)
		{
			if (likely(tv != NULL)) {
				struct timeval ktv;
				do_gettimeofday(&ktv);
				if (copy_to_user(tv, &ktv, sizeof(ktv)))
					return -EFAULT;
			}
			if (unlikely(tz != NULL)) {
				if (copy_to_user(tz, &sys_tz, sizeof(sys_tz)))
					return -EFAULT;
			}
			return 0;
		}
	
	如果用户提供的tv参数非空，那么与体系结构相关的do_gettimeofday()函数将被调用。
	该函数执行的就是上面提到的循环读取xtime的操作。如果tz参数为空，该函数将把系统时区
	（存放在sys_tz中）返回用户。如果在给用户空间拷贝墙上时间或时区时发生错误，该函数返
	回-EFAULT；如果成功，则返回0。
	
	虽然内核也实现了time()系统调用，但是gettimeofday()几乎完全取代了它。另外C库函
	数也提供了一些墙上时间相关的库调用，比如ftime()和ctime()。
	
	另外，系统调用settimeofday()来设置当前时间，它需要具有CAP_SYS_TIME权能。
	
	除了更新xtime时间以外，内核不会像用户空间程序那样频繁使用xtime。但也有需要注意
	的特殊情况，那就是在文件系统的实现代码中存放访问时间戳时需要使用xtime。
	
	
9.7 定时器
	
	定时器――――有时也被称为动态定时器或内核定时器――――是管理内核流逝的时间的基础。
	内核经常需要推后执行某些代码，比如以前章节提到的下半部机制就是为了将工作放到以后
	执行。但不幸的是，之后这个概念很含糊，下半部的本意并非是放到以后的某个时间去执行
	任务，而仅仅是不在当前时间执行就可以了。我们所需要的是一种工具，能够使工作在指定
	时间点上执行――――不长不短，正好在希望的时间点上。内核定时器正是解决这个问题的理想
	工具。
	
	定时器的使用很简单。你只需要执行一些初始化工作，设置一个超时时间，指定超时发
	生后执行的函数，然后激活定时器就可以了。指定的函数将在定时器到期时自动执行。注意
	定时器并不周期运行，它在超时后就自行销毁，这也正是这种定时器被称为动态定时器的一个
	原因；动态定时器不断地创建和销毁，而且它的运行次数也不受限制。定时器在内核中应
	用得非常普遍。
	
	
9.7.1 使用定时器

	定时器由结构time_list表示，定义在文件<linux/timer.h>中。
		struct timer_list {
			struct list_head entry;              // 包含定时器的链表
			unsigned long expires;               // 以jiffies为单位的定时值
			spinlock_t lock;                     // 保护定时器的锁
			void (*function)(unsigned long);     // 定时器处理函数
			unsigned long data;                  // 传给处理函数的长整型参数
			struct tvec_t_base_s *base;          // 定时器内部值，用户不要使用
		};
		
	幸运的是，使用定时器并不需要深入了解该数据结构。事实上，过深的陷入该结构，反
	而会使你的代码不能保证对可能发生的变化提供支持。内核提供了一组与定时器相关的接口
	用来简化管理定时器的操作。所有这些接口都声明在文件<linux/timer.h>中，大多数接口在文
	件kernel/timer.c中获得实现。
	
	创建定时器时需要先定义它：
		struct timer_list my_timer;
		
	接着需要通过一个辅助函数初始化定时器数据结构的内部值，初始化必须在使用其他定
	时器管理函数对定时器进行操作前完成。
	
		init_timer(&my_timer);
	
	现在你可以填充结构中需要的值了：
		my_timer.expires = jiffies + delay;      // 定时器超时时的节拍数
		my_timer.data = 0;                       // 给定时器处理函数传入0值
		my_timer.function = my_function;         // 定时器超时时调用的函数
	
	my_timer.expires表示超时时间，它是以节拍为单位的绝对计数值。如果当前jiffies计数等
	于或大于my_timer.expires，那么my_timer.function指向的处理函数就会开始执行，另外该函
	数还要使用长整型参数my_timer.data。所以正如我们从timer_list结构看到的形式，处理函数
	必须符合下面的函数原型：
		void my_timer_function(unsigned long data);
		
	data参数使你可以利用同一个处理函数注册多个定时器，只需通过该参数就能区别对待它
	们。如果你不需要这个参数，可以简单地传递0（或任何其他值）给处理函数。
	
	最后，你必须激活定时器：
	
		add_timer(&my_timer);
		
	大功告成，定时器可以工作了！但请注意定时值的重要性。当前节拍计数等于或大于指
	定的超时时，内核就开始执行定时器处理函数。虽然内核可以保证不会在超时时间到期前运
	行定时器处理函数，但是有可能延误定时器的执行。一般来说，定时器都在超时后马上就会
	执行，但是也有可能被推迟到下一次时钟节拍时才能运行，所以不能用定时器来实现任何硬
	实时任务。
	
	有时可能需要更改已经激活的定时器超时时间，所以内核通过函数mod_timer()来实现该
	功能，该函数可以改变指定的定时器超时时间：
		mod_timer(&my_timer, jiffies+new_delay);
		
	mod_timer()函数也可操作那些已经初始化，但还没有被激活的定时器，如果定时器未被
	激活，mod_timer()会激活它。如果调用时定时器未被激活，该函数返回0；否则返回1。但
	不论哪种情况，一旦从mod_timer()函数返回，定时器都将被激活而且设置了新的定时值。
	
	如果需要在定时器超时前停止定时器，可以使用del_timer()函数：
		del_timer(&my_timer);
	
	被激活或未被激活的定时器都可以使用该函数，如果定时器还未被激活，该函数返回
	0；否则返回1。注意，你不需要为已经超时的定时器调用该函数，因为它们会自动被删除。
	
	当删除定时器时，必须小心一个潜在的竞争条件。当del_timer()返回后，可以保证的只是：
	定时器不会再被激活（也就是说，将来不会执行），但是在多处理器机器上定时器中断可能已经
	在其他处理器上运行了，所以删除定时器时需要等待可能在其他处理器上运行的定时器处理
	程序都退出，这时就要使用del_timer_sync()函数执行删除工作：
		del_timer_sync(&my_timer);
		
	和del_timer()函数不同，del_timer_sync()函数不能在中断上下文中使用。
	
	
9.7.2 定时器竞争条件

	因为定时器与当前执行代码是异步的，因此就有可能存在潜在的竞争条件。所以，首先
	绝不能用如下所示的代码替代mod_timer()函数，来改变定时器的超时时间。这样的代码在多
	处理器机器上是不安全的：
		del_timer(my_timer);
		my_timer->expires = jiffies + new_delay;
		add_timer(my_timer);
	
	其次，一般情况下应该使用del_timer_sync()函数取代del_timer()函数，因为无法确定在删
	除定时器时，它是否在其他处理器上运行，为了防止这种情况的发生，应该调用
	del_timer_sync()函数，而不是del_timer()函数。否则，对定时器执行删除操作后，代码会继续
	执行，但它有可能会去操作在其他处理器上运行的定时器正在使用的资源，因而造成并发访
	问。所以请优先使用删除定时器的同步方法。
	
	最后，因为内核异步执行中断处理程序，所以应该重点保护定时器中断处理程序中的共
	享数据。共享数据的保护问题曾在第8章讨论过。
	
	
9.7.3 实现定时器

	内核在时钟中断发生后执行定时器，定时器作为软中断在下半部上下文中执行。具体来
	说，时钟中断处理程序会执行update_process_timers()函数，该函数随即调用
	run_local_timers()函数：
		void run_local_timers(void)
		{
			raise_softirq(TIMER_SOFTIRQ);
		}

	run_timer_softirq()函数处理软中断TIMER_SOFTIRQ，从而在当前处理器上运行所有的
	（如果有的话）超时定时器。
	
	虽然所有定时器都以链表形式存放在一起，但是让内核经常为了寻找超时定时器而遍历
	整个链表是不明智的。同样，将链表以超时时间进行排序也是很不明智的做法，因为这样一
	来在链表中插入和删除定时器都会很费时。为了提高搜索效率，内核将定时器按它们的超时
	时间划分为五组。当定时器超时时间接近时，定时器将随组一起下移。采用分组定时器的方
	法可以在执行软中断的多数情况下，确保内核尽可能减少搜索超时定时器所带来的负担。因
	此定时器管理代码是非常高效的。
	
	
9.8 延迟执行

	内核代码（尤其是驱动程序）除了使用定时器或下半部机制以外还需要其他方法来推迟
	执行任务。这种推迟通常发生在等待硬件完成某些工作时，而且等待的时间往往非常短，比
	如，重新设置网卡的以太模式需要花费2毫秒，所以在设定网卡速度后，驱动程序必须至少等
	待两毫秒才能继续运行。
	
	内核提供了许多延迟方法处理各种延迟要求。不同的方法有不同的处理特点，有些是在
	延迟任务时挂起处理器，防止处理器执行任何实际工作；另一些不会挂起处理器，所以也不
	能确保被延迟的代码能够在指定的延迟时间运行。
	
	
9.8.1 忙等待

	最简单的延迟方法（虽然通常也是最不理想的办法）是忙等待（或者说忙循环）。但要注
	意该方法仅仅在想要延迟的时间是节拍的整数倍，或者精确率要求不高时才可以使用。
	
	忙循环实现起来很简单：在循环中不断旋转直到希望的时钟节拍数耗尽，比如：
		unsigned long delay = jiffies + 10;    // 10个节拍
		while (time_before(jiffies, delay))
			;
			
	循环不断执行，直到jiffies大于dealy为止，总共的循环时间为10个节拍。在HZ值等于
	1000的0x86体系结构上，耗时为10毫秒。
	
	类似地：
		unsigned long delay = jiffies + 2*HZ;    // 2秒
		while (time_before(jiffies, delay))
			;
			
	程序要循环等待2xHZ个时钟节拍，也就是说无论时钟节拍率如何，都将要等待两秒钟。
	
	对于系统的其他部分，忙循环方法算不上一个好办法。因为当代码等待时，处理器只能
	在原地旋转等待――――它不会去处理其他任何任务！事实上，你几乎不会用到这种低效率的办
	法，这里介绍它仅仅是因为它是最简单最直接的延迟方法。当然你也可能在那些蹩脚的代码
	中发现它的身影。
	
	更好的方法应该是在代码等待时，允许内核重新调度执行其他任务：
		unsigned long delay = jiffies + 5*HZ;
		while (time_before(jiffies, delay))
			cond_resched();
			
	cond_resched()函数将调度一个新程序投入运行，但它只有在设置完need_resched标志后，
	才能生效。换句话说，该方法有效的条件是系统中存在更重要的任务需要运行。注意因为该
	方法需要调用调度程序，所以它不能在中断上下文中使用――――只能在进程上下文中使用。事
	实上，所有延迟方法在进程上下文中使用，因为中断处理程序都应该尽可能快地执行（忙循
	环与这种目标绝对是背道而驰）。另外，延迟执行不管在哪种情况下都不应该在持有锁时或禁
	止中断时发生。
	
	C语言的推崇者可能会问什么能保证前面的循环已经执行了。C编译器通常只将变量装载
	一次。一般情况下不能保证循环中的jiffies变量在每次循环中被读取时都重新被载入。但是我
	们要求jiffies在每次循环时都必须重新装载，因为在后台jiffies值会随时钟中断的发生而不断
	增加。为了解决这个问题，<linux/jiffies.h>中jiffies变量被标记为关键字volatile，关键字
	volatile指示编译器在每次访问变量时都重新从主内存中获得，而不是通过寄存器中的变量别
	名来访问，从而确保前面的循环能按预期的方式执行。
	
	
9.8.2 短延迟

	有时内核代码（通常也是驱动程序）不但需要很短的延迟（比时钟节拍还短）而且还要
	求延迟的时间很精确。这种情况多发生在和硬件同步时，也就是说需要短暂等待某个动作的
	完成――――等待时间往往小于1毫秒，所以不可能使用像前面例子中那种基于jiffies的延迟方法。
	对于频率为100Hz的时钟中断，它的节拍间隔甚至会超过10毫秒！即使频率为1000Hz的时钟中
	断，节拍间隔也只能到1毫秒，所以我们必须寻找其他方法满足更短、更精确的延迟要求。
	
	幸运的是，内核提供了两个可以处理微秒和毫秒级别的延迟的函数，它们定义在文件
	<linux/delay.h>中，可以看到它们并不使用jiffies:
	
		void udelay(unsigned long usecs);
		void mdelay(unsigned long msecs);
		
	前一个函数利用忙循环将任务延迟指定的微秒数后运行，后者延迟指定的毫秒数。众所
	周知，1秒等于1000毫秒，等于1000000微秒。用起来很简单：
	
		udelay(150);      // 延迟150微秒
		
	udelay()函数依靠执行数次循环达到延迟效果，而mdelay()函数又是通过udelay()函数实现
	的。因为内核知道处理器在一秒内能执行多少次循环（请看副栏中的BogoMIPS内容），所以
	udelay()函数仅仅需要根据指定的延迟时间在1秒中占的比例，就能决定需要进行多少次循环
	就能达到要求的推迟时间。
	
	udelay()函数仅能在要求的延迟时间很短的情况下执行，而在高速机器中的时间较长的延
	迟会造成溢出。经验证明，不要使用udelay()函数处理超过1毫秒的延迟。延迟超过1毫秒的情
	况下，使用mdelay()函数更为安全。和其他忙等待延迟方法一样，这些函数（特别是mdelay()
	函数，因为它延迟时间较长）如果不是非常必要的情况下，尽量少用。千万注意不要在持有
	锁时或禁止中断时使用忙等待，因为这时忙等待会使系统响应速度和性能大打折扣。但是如
	果你真的需要精确延迟，那么用这些函数冒一下险，可能是你最好的选择。通常这些忙等待
	推迟函数都用在短暂延迟的情况下，往往是微秒级的推迟。
	
	1. Schedule_timeout()
	
	更理想的延迟执行方法是使用schedule_timeout()函数，该方法会让需要延迟执行的任务
	睡眠到指定的延迟时间耗尽后再重新运行。但该方法也不能保证睡眠时间正好等于指定的延
	迟时间――――只能尽量使睡眠时间接近指定的延迟时间。当指定的时间到期后，内核唤醒被延
	迟的任务并将其重新放回运行队列，用法如下：
	
		set_current_state(TASK_INTERRUPTIBLE);
		schedule_timeout(s*HZ);
		
	唯一的参数是延迟的相对时间，单位为jiffies，上例中将相应的任务推入可中断睡眠队列，
	睡眠s秒。因为任务处于可中断状态，所以如果任务收到信号将被唤醒。如果睡眠任务不想接
	收信号，可以将任务状态设置为TASK_UNINTERRUPTIBLE，然后睡眠。注意在调用
	schedule_timeout()函数前必须首先将任务设置成上面两种状态之一，否则任务不会睡眠。
	
	注意，由于schedule_timeout()函数需要调用调度程序，所以调用它的代码必须保证能够睡
	眠（请参考第8章）。简而言之，调用代码必须处于进程上下文中，并且不能持有锁。
	
	schedule_timeout()函数的用法相当简单、直接。其实，它是内核定时器的一个简单应用。
	请看下面的代码：
		
		signed long schedule_timeout(signed long timeout)
		{
			timer_t timer;
			unsigned long expire;
			
			switch (timeout)
			{
				case MAX_SCHEDULE_TIMEOUT:
						schedule();
						goto out;
				default:
						if (timeout < 0)
						{
							printk(KERN_ERR"schedule_timeout: wrong timeout "
								"value %lx from %p\n", timeout,
								__builtin_return_address(0));
							current->state = TASK_RUNNING;
							goto out;
						}
			}
			
			expire = timeout + jiffies;
			
			init_timer(&timer);
			timer.expires = expire;
			timer.data = (unsigned long) current;
			timer.function = process_timeout;
			
			add_timer(&timer);
			
			schedule();
			del_timer_sync(&timer);
			timeout = expire - jiffies;
			
			out:
				return timeout < 0 ? 0 : timeout;
		}
		
	该函数创建一个定时器timer；然后设置它的超时时间timeout；设置超时执行函数
	process_timeout()；然后激活定时器而且调用schedule()。因为任务被标识为TASK_
	INTERRUPTIBLE或TASK_UNINTERRUPTIBLE，所以调度程序不会再选择该任务投入运行，
	而会选择其他新任务运行。
	
	当定时器超时，process_timeout()函数被调用：
		
		void process_timeout(unsigned long data)
		{
			wake_up_process((task_t *)data);
		}

	该函数将任务置为TASK_RUNNING状态，然后将其放入运行队列。
	
	当任务重新被调度时，将返回代码进入睡眠前的位置继续执行（正好在调用schedule()后）。
	如果任务提前被唤醒（比如收到信号），那么定时器被销毁，process_timeout()函数返回剩余
	的时间。
	
	在switch()括号中的代码是为处理特殊情况而写的，正常情况不会用到它们。MAX_
	SCHEDULE_TIMEOUT是用来检查任务是否无限期地睡眠，如果那样的话，函数不会为它设
	置定时器（因为睡眠时间没有期限），这时调度程序会立刻被调用。如果你需要无限期的让任
	务睡眠，最好使用其他方法唤醒任务。
	
	2. 设置超时时间，在等待队列上睡眠
	
	第3章我们已经看到进程上下文中的代码为了等待特定事件发生，可以将自己放入等待队
	列，然后调用调度程序去执行新任务。一旦事件发生后，内核调用wake_up()函数唤醒在睡眠
	队列上的任务，使其重新投入运行。
	
	有时，等待队列上的某个任务可能既在等待一个特定事件到来，又在等待一个特定时间
	到期――――就看谁来得更快。这种情况下，代码可以简单地使用schedule_timeout()函数代替
	schedule()函数，这样一来，当希望指定时间到期，任务都会被唤醒。当然，代码需要检查被
	唤醒的原因――――有可能是被事件唤醒，也有可能是因为延迟的时间到期，还可能是因为接收
	到了信号――――然后执行相应的操作。
	
	















































































































































































































































































































































































































































































































































































































































































































































